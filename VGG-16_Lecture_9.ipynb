{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa3b4d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import  transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cff1a71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: ipbh00ef\n",
      "Sweep URL: https://wandb.ai/fangselection123-happiest-minds-technologies/VGG-16/sweeps/ipbh00ef\n"
     ]
    }
   ],
   "source": [
    "sweep_config = {\n",
    "    'method': 'grid',\n",
    "    'metric': {\n",
    "        'name' : 'val_accuracy',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'learning_rate': {'values': [0.001, 0.0001]},\n",
    "        'batch_size': {'values': [16, 32]},\n",
    "        'num_epochs': {'values': [5, 10]},\n",
    "        'image_size': {'values' : [224]},\n",
    "        'epochs':{'values': [5,10]},\n",
    "        'droput_rate': {'values': [0.2, 0.5]},\n",
    "    }\n",
    "}\n",
    "sweep_id = wandb.sweep(sweep=sweep_config, project='VGG-16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "882d527e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.transform = transform\n",
    "        self.image_paths=[]\n",
    "        self.labels = []\n",
    "        self.class_to_idx ={}\n",
    "        class_names = sorted(os.listdir(root_dir))\n",
    "        self.class_to_idx = { class_name: i for i, class_name in enumerate(class_names)}\n",
    "        for class_name in class_names:\n",
    "            class_dir= os.path.join(root_dir, class_name)\n",
    "            for img_name in os.listdir(class_dir):\n",
    "                img_path = os.path.join(class_dir, img_name)\n",
    "                self.image_paths.append(img_path)\n",
    "                self.labels.append(self.class_to_idx[class_name])\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c42c771b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=\"same\")\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=\"same\")\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=\"same\")\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=\"same\")\n",
    "\n",
    "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=\"same\")\n",
    "        self.conv6 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=\"same\")\n",
    "        self.conv7 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=\"same\")\n",
    "\n",
    "        self.conv8 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=\"same\")\n",
    "        self.conv9 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=\"same\")\n",
    "        self.conv10 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=\"same\")\n",
    "\n",
    "        self.conv11 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=\"same\")\n",
    "        self.conv12 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=\"same\")\n",
    "        self.conv13 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=\"same\")\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(512 * 7 * 7, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.fc3 = nn.Linear(4096, 5)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        x= self.relu(self.conv1(x))\n",
    "        # print(x.shape)\n",
    "        x=self.relu(self.conv2(x))\n",
    "        # print(x.shape)\n",
    "\n",
    "        x=self.pool(x)\n",
    "        # print(x.shape)\n",
    "\n",
    "        x=self.relu(self.conv3(x))\n",
    "        # print(x.shape)\n",
    "\n",
    "        x=self.relu(self.conv4(x))\n",
    "        # print(x.shape)\n",
    "\n",
    "        x=self.pool(x)\n",
    "        # print(x.shape)\n",
    "\n",
    "        x=self.relu(self.conv5(x))\n",
    "        # print(x.shape)\n",
    "\n",
    "        x=self.relu(self.conv6(x))\n",
    "        # print(x.shape)\n",
    "\n",
    "        x=self.relu(self.conv7(x))\n",
    "        # print(x.shape)\n",
    "\n",
    "        x=self.pool(x)\n",
    "        # print(x.shape)\n",
    "\n",
    "        x=self.relu(self.conv8(x))\n",
    "        # print(x.shape)\n",
    "        \n",
    "        x=self.relu(self.conv9(x))\n",
    "        # print(x.shape)\n",
    "\n",
    "        x=self.relu(self.conv10(x))\n",
    "        # print(x.shape)\n",
    "\n",
    "        x=self.pool(x)\n",
    "        # print(x.shape)\n",
    "\n",
    "        x=self.relu(self.conv11(x))\n",
    "        # print(x.shape)\n",
    "\n",
    "        x=self.relu(self.conv12(x))\n",
    "        # print(x.shape)\n",
    "\n",
    "        x=self.relu(self.conv13(x))\n",
    "        # print(x.shape)\n",
    "        x=self.pool(x)\n",
    "        x=self.flatten(x)\n",
    "        # print(x.shape)\n",
    "\n",
    "        x=self.dropout(self.fc1(x))\n",
    "        x=self.relu(x)\n",
    "        x=self.dropout(self.fc2(x))\n",
    "        x=self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51b1c851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
      "              ReLU-2         [-1, 64, 224, 224]               0\n",
      "            Conv2d-3         [-1, 64, 224, 224]          36,928\n",
      "              ReLU-4         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-5         [-1, 64, 112, 112]               0\n",
      "            Conv2d-6        [-1, 128, 112, 112]          73,856\n",
      "              ReLU-7        [-1, 128, 112, 112]               0\n",
      "            Conv2d-8        [-1, 128, 112, 112]         147,584\n",
      "              ReLU-9        [-1, 128, 112, 112]               0\n",
      "        MaxPool2d-10          [-1, 128, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]         295,168\n",
      "             ReLU-12          [-1, 256, 56, 56]               0\n",
      "           Conv2d-13          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-14          [-1, 256, 56, 56]               0\n",
      "           Conv2d-15          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-16          [-1, 256, 56, 56]               0\n",
      "        MaxPool2d-17          [-1, 256, 28, 28]               0\n",
      "           Conv2d-18          [-1, 512, 28, 28]       1,180,160\n",
      "             ReLU-19          [-1, 512, 28, 28]               0\n",
      "           Conv2d-20          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-21          [-1, 512, 28, 28]               0\n",
      "           Conv2d-22          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-23          [-1, 512, 28, 28]               0\n",
      "        MaxPool2d-24          [-1, 512, 14, 14]               0\n",
      "           Conv2d-25          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-26          [-1, 512, 14, 14]               0\n",
      "           Conv2d-27          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-28          [-1, 512, 14, 14]               0\n",
      "           Conv2d-29          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-30          [-1, 512, 14, 14]               0\n",
      "        MaxPool2d-31            [-1, 512, 7, 7]               0\n",
      "          Flatten-32                [-1, 25088]               0\n",
      "           Linear-33                 [-1, 4096]     102,764,544\n",
      "          Dropout-34                 [-1, 4096]               0\n",
      "             ReLU-35                 [-1, 4096]               0\n",
      "           Linear-36                 [-1, 4096]      16,781,312\n",
      "          Dropout-37                 [-1, 4096]               0\n",
      "           Linear-38                    [-1, 5]          20,485\n",
      "================================================================\n",
      "Total params: 134,281,029\n",
      "Trainable params: 134,281,029\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 218.74\n",
      "Params size (MB): 512.24\n",
      "Estimated Total Size (MB): 731.56\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "model = VGG16()\n",
    "summary(model, input_size=(3, 224, 224))  # ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d74949d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchvision.models as models\n",
    "# model = models.vgg16(pretrained=True)\n",
    "# model.classifier[6] = nn.Linear(4096, 5)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01a037a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    with wandb.init() as run:\n",
    "        config= wandb.config\n",
    "        train_dir=\"Data/flowers/train\"\n",
    "        test_dir=\"Data/flowers/val\"\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((config.image_size, config.image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "\n",
    "\n",
    "        train_dataset = CustomDataset(train_dir, transform=transform)\n",
    "        test_dataset = CustomDataset(test_dir, transform=transform)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "        class SimpleModel(nn.Module):\n",
    "            def __init__(self):\n",
    "                super(SimpleModel, self).__init__()\n",
    "                self.flatten = nn.Flatten()\n",
    "                self.fc1 = nn.Linear(config.img_size * config.img_size * 3, config.hidden_nodes)\n",
    "                self.bn1 = nn.BatchNorm1d(config.hidden_nodes)\n",
    "                # Output layer for 5 classes\n",
    "                self.relu = nn.ReLU()\n",
    "                self.dropout = nn.Dropout(p=0.2)\n",
    "                self.fc2 = nn.Linear(config.hidden_nodes, 5)\n",
    "\n",
    "            def forward(self, x):\n",
    "                x = self.flatten(x)\n",
    "                x = self.fc1(x)\n",
    "                x= self.bn1(x)\n",
    "                x = self.relu(x)\n",
    "                x = self.dropout(x)\n",
    "                x = self.fc2(x)\n",
    "                return x  # No softmax needed; use CrossEntropyLoss\n",
    "        model = VGG16()\n",
    "        # -----------------------------\n",
    "        # Training Setup\n",
    "        # -----------------------------\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config.learning_rate, weight_decay=1e-5) # l2 regularization\n",
    "        # shedule = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "        # -----------------------------\n",
    "        # Training Loop\n",
    "        # -----------------------------\n",
    "        EPOCHS = 10\n",
    "        for epoch in range(EPOCHS):\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "\n",
    "            for i, (images, labels) in enumerate(train_loader):\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                # L1 regularization (manual)\n",
    "                # l1_lambda = 1e-5\n",
    "                # l1_loss = sum(torch.sum(torch.abs(param)) for param in model.parameters())\n",
    "                # loss += l1_lambda * l1_loss\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # shedule.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                batch_correct = (preds == labels).sum().item()\n",
    "                train_correct += batch_correct\n",
    "                train_total += labels.size(0)\n",
    "                # Print every 10 batches\n",
    "                if(i + 1) % 10 == 0:\n",
    "                    batch_acc = batch_correct / labels.size(0)\n",
    "                    print(f\"[Batch {i+1}/{len(train_loader)}] Loss: {loss.item():.4f}, Batch Acc: {batch_acc:.4f}\")\n",
    "\n",
    "            train_accuracy = train_correct / train_total\n",
    "            wandb.log({\"epoch\": epoch + 1, \"train_loss\": train_loss, \"train_accuracy\": train_accuracy})\n",
    "            print(f\"Epoch {epoch+1} Summary - Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "            # -----------------------------\n",
    "            # # Evaluation (Optional)\n",
    "            # # -----------------------------\n",
    "        model.eval()\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item()\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                test_correct += (preds == labels).sum().item()\n",
    "                test_total += labels.size(0)\n",
    "        test_accuracy = test_correct / test_total\n",
    "        wandb.log({\"test_loss\": test_loss, \"test_accuracy\": test_accuracy})\n",
    "        print(f\"Test Accuracy: {test_correct / test_total:.4f}\")\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a437f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: my4o60gi with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdroput_rate: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_size: 224\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_epochs: 5\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ajit.kumar4@happiestminds.com/Documents/Drackula/ComputerVisionFromScratch/wandb/run-20250616_220616-my4o60gi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fangselection123-happiest-minds-technologies/VGG-16/runs/my4o60gi' target=\"_blank\">giddy-sweep-7</a></strong> to <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/VGG-16' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/VGG-16/sweeps/ipbh00ef' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/VGG-16/sweeps/ipbh00ef</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/VGG-16' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/VGG-16</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/VGG-16/sweeps/ipbh00ef' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/VGG-16/sweeps/ipbh00ef</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/VGG-16/runs/my4o60gi' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/VGG-16/runs/my4o60gi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 10/251] Loss: 1.6069, Batch Acc: 0.3125\n",
      "[Batch 20/251] Loss: 1.6129, Batch Acc: 0.0625\n",
      "[Batch 30/251] Loss: 1.6101, Batch Acc: 0.1250\n",
      "[Batch 40/251] Loss: 1.6046, Batch Acc: 0.3125\n",
      "[Batch 50/251] Loss: 1.6000, Batch Acc: 0.3125\n",
      "[Batch 60/251] Loss: 1.6071, Batch Acc: 0.2500\n",
      "[Batch 70/251] Loss: 1.5996, Batch Acc: 0.2500\n",
      "[Batch 80/251] Loss: 1.6097, Batch Acc: 0.0000\n",
      "[Batch 90/251] Loss: 1.6111, Batch Acc: 0.1875\n",
      "[Batch 100/251] Loss: 1.6199, Batch Acc: 0.1875\n",
      "[Batch 110/251] Loss: 1.6099, Batch Acc: 0.1250\n",
      "[Batch 120/251] Loss: 1.6121, Batch Acc: 0.1250\n",
      "[Batch 130/251] Loss: 1.5835, Batch Acc: 0.2500\n",
      "[Batch 140/251] Loss: 1.6075, Batch Acc: 0.1875\n",
      "[Batch 150/251] Loss: 1.6114, Batch Acc: 0.3125\n",
      "[Batch 160/251] Loss: 1.6126, Batch Acc: 0.3125\n",
      "[Batch 170/251] Loss: 1.6170, Batch Acc: 0.1875\n",
      "[Batch 180/251] Loss: 1.6107, Batch Acc: 0.1250\n",
      "[Batch 190/251] Loss: 1.6216, Batch Acc: 0.1875\n",
      "[Batch 200/251] Loss: 1.6211, Batch Acc: 0.1875\n",
      "[Batch 210/251] Loss: 1.6080, Batch Acc: 0.1250\n",
      "[Batch 220/251] Loss: 1.6026, Batch Acc: 0.4375\n",
      "[Batch 230/251] Loss: 1.6070, Batch Acc: 0.1875\n",
      "[Batch 240/251] Loss: 1.6175, Batch Acc: 0.1250\n",
      "[Batch 250/251] Loss: 1.6121, Batch Acc: 0.1875\n",
      "Epoch 1 Summary - Loss: 404.4487, Train Accuracy: 0.1916\n",
      "[Batch 10/251] Loss: 1.6119, Batch Acc: 0.1875\n",
      "[Batch 20/251] Loss: 1.6084, Batch Acc: 0.1250\n",
      "[Batch 30/251] Loss: 1.6096, Batch Acc: 0.1250\n",
      "[Batch 40/251] Loss: 1.6081, Batch Acc: 0.1875\n",
      "[Batch 50/251] Loss: 1.6016, Batch Acc: 0.3125\n",
      "[Batch 60/251] Loss: 1.5999, Batch Acc: 0.1875\n",
      "[Batch 70/251] Loss: 1.5944, Batch Acc: 0.1875\n",
      "[Batch 80/251] Loss: 1.6028, Batch Acc: 0.3125\n",
      "[Batch 90/251] Loss: 1.6244, Batch Acc: 0.1875\n",
      "[Batch 100/251] Loss: 1.6038, Batch Acc: 0.1875\n",
      "[Batch 110/251] Loss: 1.6231, Batch Acc: 0.1250\n",
      "[Batch 120/251] Loss: 1.6102, Batch Acc: 0.1250\n",
      "[Batch 130/251] Loss: 1.6097, Batch Acc: 0.1875\n",
      "[Batch 140/251] Loss: 1.6033, Batch Acc: 0.3125\n",
      "[Batch 150/251] Loss: 1.6138, Batch Acc: 0.1250\n"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id, function=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48c4c59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8e587e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d8dfdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
