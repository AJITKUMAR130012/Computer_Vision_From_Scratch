{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "218d8447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cea02f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d6a4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 7yx61jek\n",
      "Sweep URL: https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek\n"
     ]
    }
   ],
   "source": [
    "sweep_config = {\n",
    "    'method' : 'grid',\n",
    "    'metric': {\n",
    "        'name' : 'val_accuracy',\n",
    "        'goal' : 'maximize'\n",
    "    },\n",
    "    'parameters' : {\n",
    "        'batch_size' : { 'values' : [8, 16]},\n",
    "        'learning_rate' : { 'values' : [0.001, 0.0001]},\n",
    "        'hidden_nodes': {'values' : [32, 64]},\n",
    "        'img_size' : {'values' : [16, 64]},\n",
    "        'epochs' : {'values': [5, 10]}\n",
    "    }\n",
    "}\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"5-Flower-Dataset-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad3285f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# STEP 0: Initialize wandb\n",
    "# =======================\n",
    "\n",
    "\n",
    "# wandb.init(project=\"alexnet-flowers-v2\", config={\n",
    "#     \"epochs\": 10,\n",
    "#     \"batch_size\": 16,\n",
    "#     \"learning_rate\": 0.001,\n",
    "#     \"architecture\": \"alexnet\",\n",
    "#     \"pretrained\": True,\n",
    "#     \"input_size\": 128\n",
    "# })\n",
    "\n",
    "# Shortcut to config values\n",
    "# config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34e2326e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CustomFlowersDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.class_to_idx = {}\n",
    "\n",
    "        class_names = sorted(os.listdir(root_dir))\n",
    "        self.class_to_idx = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
    "        for class_name in class_names:\n",
    "            class_dir = os.path. join(root_dir, class_name)\n",
    "            for img_name in os.listdir(class_dir):\n",
    "                img_path =os.path.join(class_dir, img_name)\n",
    "                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    self.image_paths.append(img_path)\n",
    "                    self.labels.append(self.class_to_idx[class_name])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6be26b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56583d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# path = \"/home/ajit.kumar4@happiestminds.com/Documents/Drackula/ComputerVisionFromScratch/Data/flowers/train/.DS_Store\"\n",
    "# path = \"/home/ajit.kumar4@happiestminds.com/Documents/Drackula/ComputerVisionFromScratch/Data/flowers/val/.DS_Store\"\n",
    "\n",
    "# # Remove .DS_Store if it exists\n",
    "# if os.path.exists(path):\n",
    "#     os.remove(path)\n",
    "#     print(f\"Deleted: {path}\")\n",
    "# else:\n",
    "#     print(f\"File does not exist: {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c99467e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    with wandb.init() as run:\n",
    "        config= wandb.config\n",
    "        train_dir=\"Data/flowers/train\"\n",
    "        test_dir=\"Data/flowers/val\"\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((config.img_size, config.img_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "\n",
    "\n",
    "        train_dataset = CustomFlowersDataset(train_dir, transform=transform)\n",
    "        test_dataset = CustomFlowersDataset(test_dir, transform=transform)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "        class SimpleModel(nn.Module):\n",
    "            def __init__(self):\n",
    "                super(SimpleModel, self).__init__()\n",
    "                self.flatten = nn.Flatten()\n",
    "                self.fc1 = nn.Linear(config.img_size * config.img_size * 3, config.hidden_nodes)\n",
    "                self.bn1 = nn.BatchNorm1d(config.hidden_nodes)\n",
    "                # Output layer for 5 classes\n",
    "                self.relu = nn.ReLU()\n",
    "                self.dropout = nn.Dropout(p=0.2)\n",
    "                self.fc2 = nn.Linear(config.hidden_nodes, 5)\n",
    "\n",
    "            def forward(self, x):\n",
    "                x = self.flatten(x)\n",
    "                x = self.fc1(x)\n",
    "                x= self.bn1(x)\n",
    "                x = self.relu(x)\n",
    "                x = self.dropout(x)\n",
    "                x = self.fc2(x)\n",
    "                return x  # No softmax needed; use CrossEntropyLoss\n",
    "        model = SimpleModel()\n",
    "        # -----------------------------\n",
    "        # Training Setup\n",
    "        # -----------------------------\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config.learning_rate, weight_decay=1e-5) # l2 regularization\n",
    "        # shedule = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "        # -----------------------------\n",
    "        # Training Loop\n",
    "        # -----------------------------\n",
    "        EPOCHS = 10\n",
    "        for epoch in range(EPOCHS):\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "\n",
    "            for i, (images, labels) in enumerate(train_loader):\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                # L1 regularization (manual)\n",
    "                # l1_lambda = 1e-5\n",
    "                # l1_loss = sum(torch.sum(torch.abs(param)) for param in model.parameters())\n",
    "                # loss += l1_lambda * l1_loss\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # shedule.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                batch_correct = (preds == labels).sum().item()\n",
    "                train_correct += batch_correct\n",
    "                train_total += labels.size(0)\n",
    "                # Print every 10 batches\n",
    "                if(i + 1) % 10 == 0:\n",
    "                    batch_acc = batch_correct / labels.size(0)\n",
    "                    print(f\"[Batch {i+1}/{len(train_loader)}] Loss: {loss.item():.4f}, Batch Acc: {batch_acc:.4f}\")\n",
    "\n",
    "            train_accuracy = train_correct / train_total\n",
    "            wandb.log({\"epoch\": epoch + 1, \"train_loss\": train_loss, \"train_accuracy\": train_accuracy})\n",
    "            print(f\"Epoch {epoch+1} Summary - Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "            # -----------------------------\n",
    "            # # Evaluation (Optional)\n",
    "            # # -----------------------------\n",
    "        model.eval()\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item()\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                test_correct += (preds == labels).sum().item()\n",
    "                test_total += labels.size(0)\n",
    "        test_accuracy = test_correct / test_total\n",
    "        wandb.log({\"test_loss\": test_loss, \"test_accuracy\": test_accuracy})\n",
    "        print(f\"Test Accuracy: {test_correct / test_total:.4f}\")\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4fe1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 12vwi9s4 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_nodes: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfangselection123\u001b[0m (\u001b[33mfangselection123-happiest-minds-technologies\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ajit.kumar4@happiestminds.com/Documents/Drackula/ComputerVisionFromScratch/wandb/run-20250615_135136-12vwi9s4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/12vwi9s4' target=\"_blank\">stoic-sweep-1</a></strong> to <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/12vwi9s4' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/12vwi9s4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 10/501] Loss: 1.7272, Batch Acc: 0.2500\n",
      "[Batch 20/501] Loss: 1.4052, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.7162, Batch Acc: 0.2500\n",
      "[Batch 40/501] Loss: 1.5729, Batch Acc: 0.2500\n",
      "[Batch 50/501] Loss: 1.4287, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.8150, Batch Acc: 0.1250\n",
      "[Batch 70/501] Loss: 1.7172, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.5038, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.5879, Batch Acc: 0.3750\n",
      "[Batch 100/501] Loss: 1.5946, Batch Acc: 0.0000\n",
      "[Batch 110/501] Loss: 1.5876, Batch Acc: 0.1250\n",
      "[Batch 120/501] Loss: 1.4080, Batch Acc: 0.3750\n",
      "[Batch 130/501] Loss: 1.6717, Batch Acc: 0.2500\n",
      "[Batch 140/501] Loss: 1.6309, Batch Acc: 0.2500\n",
      "[Batch 150/501] Loss: 1.5443, Batch Acc: 0.5000\n",
      "[Batch 160/501] Loss: 1.6434, Batch Acc: 0.2500\n",
      "[Batch 170/501] Loss: 1.5402, Batch Acc: 0.3750\n",
      "[Batch 180/501] Loss: 1.6099, Batch Acc: 0.3750\n",
      "[Batch 190/501] Loss: 1.6427, Batch Acc: 0.1250\n",
      "[Batch 200/501] Loss: 1.6367, Batch Acc: 0.1250\n",
      "[Batch 210/501] Loss: 1.5850, Batch Acc: 0.6250\n",
      "[Batch 220/501] Loss: 1.6428, Batch Acc: 0.1250\n",
      "[Batch 230/501] Loss: 1.7040, Batch Acc: 0.0000\n",
      "[Batch 240/501] Loss: 1.6875, Batch Acc: 0.1250\n",
      "[Batch 250/501] Loss: 1.5428, Batch Acc: 0.3750\n",
      "[Batch 260/501] Loss: 1.4989, Batch Acc: 0.5000\n",
      "[Batch 270/501] Loss: 1.4891, Batch Acc: 0.5000\n",
      "[Batch 280/501] Loss: 1.6861, Batch Acc: 0.2500\n",
      "[Batch 290/501] Loss: 1.5792, Batch Acc: 0.1250\n",
      "[Batch 300/501] Loss: 1.6425, Batch Acc: 0.5000\n",
      "[Batch 310/501] Loss: 1.6007, Batch Acc: 0.1250\n",
      "[Batch 320/501] Loss: 1.6707, Batch Acc: 0.1250\n",
      "[Batch 330/501] Loss: 1.6234, Batch Acc: 0.1250\n",
      "[Batch 340/501] Loss: 1.5797, Batch Acc: 0.3750\n",
      "[Batch 350/501] Loss: 1.6865, Batch Acc: 0.2500\n",
      "[Batch 360/501] Loss: 1.4730, Batch Acc: 0.3750\n",
      "[Batch 370/501] Loss: 1.8094, Batch Acc: 0.2500\n",
      "[Batch 380/501] Loss: 1.5354, Batch Acc: 0.3750\n",
      "[Batch 390/501] Loss: 1.6248, Batch Acc: 0.1250\n",
      "[Batch 400/501] Loss: 1.7059, Batch Acc: 0.3750\n",
      "[Batch 410/501] Loss: 1.4261, Batch Acc: 0.3750\n",
      "[Batch 420/501] Loss: 1.8169, Batch Acc: 0.1250\n",
      "[Batch 430/501] Loss: 1.7613, Batch Acc: 0.1250\n",
      "[Batch 440/501] Loss: 1.6853, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.5276, Batch Acc: 0.5000\n",
      "[Batch 460/501] Loss: 1.5799, Batch Acc: 0.3750\n",
      "[Batch 470/501] Loss: 1.5355, Batch Acc: 0.3750\n",
      "[Batch 480/501] Loss: 1.7039, Batch Acc: 0.3750\n",
      "[Batch 490/501] Loss: 1.5593, Batch Acc: 0.1250\n",
      "[Batch 500/501] Loss: 1.4743, Batch Acc: 0.2500\n",
      "Epoch 1 Summary - Loss: 806.7230, Train Accuracy: 0.2682\n",
      "[Batch 10/501] Loss: 1.5446, Batch Acc: 0.3750\n",
      "[Batch 20/501] Loss: 1.6655, Batch Acc: 0.3750\n",
      "[Batch 30/501] Loss: 1.7794, Batch Acc: 0.3750\n",
      "[Batch 40/501] Loss: 1.5511, Batch Acc: 0.5000\n",
      "[Batch 50/501] Loss: 1.5329, Batch Acc: 0.3750\n",
      "[Batch 60/501] Loss: 1.5531, Batch Acc: 0.1250\n",
      "[Batch 70/501] Loss: 1.6568, Batch Acc: 0.3750\n",
      "[Batch 80/501] Loss: 1.7401, Batch Acc: 0.0000\n",
      "[Batch 90/501] Loss: 1.4324, Batch Acc: 0.3750\n",
      "[Batch 100/501] Loss: 1.8400, Batch Acc: 0.1250\n",
      "[Batch 110/501] Loss: 1.6655, Batch Acc: 0.1250\n",
      "[Batch 120/501] Loss: 1.4461, Batch Acc: 0.3750\n",
      "[Batch 130/501] Loss: 1.5826, Batch Acc: 0.3750\n",
      "[Batch 140/501] Loss: 1.6901, Batch Acc: 0.1250\n",
      "[Batch 150/501] Loss: 1.6109, Batch Acc: 0.3750\n",
      "[Batch 160/501] Loss: 1.5788, Batch Acc: 0.0000\n",
      "[Batch 170/501] Loss: 1.6189, Batch Acc: 0.5000\n",
      "[Batch 180/501] Loss: 1.5146, Batch Acc: 0.3750\n",
      "[Batch 190/501] Loss: 1.5074, Batch Acc: 0.2500\n",
      "[Batch 200/501] Loss: 1.3682, Batch Acc: 0.5000\n",
      "[Batch 210/501] Loss: 1.5674, Batch Acc: 0.2500\n",
      "[Batch 220/501] Loss: 1.7558, Batch Acc: 0.2500\n",
      "[Batch 230/501] Loss: 1.8202, Batch Acc: 0.1250\n",
      "[Batch 240/501] Loss: 1.6427, Batch Acc: 0.0000\n",
      "[Batch 250/501] Loss: 1.5432, Batch Acc: 0.2500\n",
      "[Batch 260/501] Loss: 1.3263, Batch Acc: 0.6250\n",
      "[Batch 270/501] Loss: 1.8159, Batch Acc: 0.2500\n",
      "[Batch 280/501] Loss: 1.7182, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 1.6360, Batch Acc: 0.0000\n",
      "[Batch 300/501] Loss: 1.7498, Batch Acc: 0.1250\n",
      "[Batch 310/501] Loss: 1.8020, Batch Acc: 0.1250\n",
      "[Batch 320/501] Loss: 1.7279, Batch Acc: 0.0000\n",
      "[Batch 330/501] Loss: 1.4650, Batch Acc: 0.5000\n",
      "[Batch 340/501] Loss: 1.4538, Batch Acc: 0.3750\n",
      "[Batch 350/501] Loss: 1.6283, Batch Acc: 0.5000\n",
      "[Batch 360/501] Loss: 1.4888, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 1.4594, Batch Acc: 0.5000\n",
      "[Batch 380/501] Loss: 1.7116, Batch Acc: 0.2500\n",
      "[Batch 390/501] Loss: 1.6281, Batch Acc: 0.2500\n",
      "[Batch 400/501] Loss: 1.6076, Batch Acc: 0.3750\n",
      "[Batch 410/501] Loss: 1.4967, Batch Acc: 0.3750\n",
      "[Batch 420/501] Loss: 1.5626, Batch Acc: 0.2500\n",
      "[Batch 430/501] Loss: 1.6177, Batch Acc: 0.3750\n",
      "[Batch 440/501] Loss: 1.7165, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.5683, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.6080, Batch Acc: 0.3750\n",
      "[Batch 470/501] Loss: 1.7146, Batch Acc: 0.2500\n",
      "[Batch 480/501] Loss: 1.4933, Batch Acc: 0.5000\n",
      "[Batch 490/501] Loss: 1.8068, Batch Acc: 0.1250\n",
      "[Batch 500/501] Loss: 1.6600, Batch Acc: 0.3750\n",
      "Epoch 2 Summary - Loss: 804.2385, Train Accuracy: 0.2702\n",
      "[Batch 10/501] Loss: 1.5150, Batch Acc: 0.1250\n",
      "[Batch 20/501] Loss: 1.5110, Batch Acc: 0.1250\n",
      "[Batch 30/501] Loss: 1.5738, Batch Acc: 0.3750\n",
      "[Batch 40/501] Loss: 1.5609, Batch Acc: 0.3750\n",
      "[Batch 50/501] Loss: 1.7356, Batch Acc: 0.1250\n",
      "[Batch 60/501] Loss: 1.3787, Batch Acc: 0.3750\n",
      "[Batch 70/501] Loss: 1.7494, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.4513, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.5189, Batch Acc: 0.2500\n",
      "[Batch 100/501] Loss: 1.6671, Batch Acc: 0.1250\n",
      "[Batch 110/501] Loss: 1.5149, Batch Acc: 0.5000\n",
      "[Batch 120/501] Loss: 1.5441, Batch Acc: 0.2500\n",
      "[Batch 130/501] Loss: 1.4382, Batch Acc: 0.3750\n",
      "[Batch 140/501] Loss: 1.5703, Batch Acc: 0.2500\n",
      "[Batch 150/501] Loss: 1.6159, Batch Acc: 0.0000\n",
      "[Batch 160/501] Loss: 1.5971, Batch Acc: 0.0000\n",
      "[Batch 170/501] Loss: 1.4974, Batch Acc: 0.3750\n",
      "[Batch 180/501] Loss: 1.5957, Batch Acc: 0.2500\n",
      "[Batch 190/501] Loss: 1.7244, Batch Acc: 0.3750\n",
      "[Batch 200/501] Loss: 1.5517, Batch Acc: 0.2500\n",
      "[Batch 210/501] Loss: 1.6721, Batch Acc: 0.2500\n",
      "[Batch 220/501] Loss: 1.8357, Batch Acc: 0.1250\n",
      "[Batch 230/501] Loss: 1.4086, Batch Acc: 0.5000\n",
      "[Batch 240/501] Loss: 1.4046, Batch Acc: 0.3750\n",
      "[Batch 250/501] Loss: 1.6246, Batch Acc: 0.1250\n",
      "[Batch 260/501] Loss: 1.5089, Batch Acc: 0.2500\n",
      "[Batch 270/501] Loss: 1.7164, Batch Acc: 0.2500\n",
      "[Batch 280/501] Loss: 1.7667, Batch Acc: 0.0000\n",
      "[Batch 290/501] Loss: 1.7688, Batch Acc: 0.2500\n",
      "[Batch 300/501] Loss: 1.6191, Batch Acc: 0.3750\n",
      "[Batch 310/501] Loss: 1.8326, Batch Acc: 0.2500\n",
      "[Batch 320/501] Loss: 1.4447, Batch Acc: 0.6250\n",
      "[Batch 330/501] Loss: 1.7021, Batch Acc: 0.2500\n",
      "[Batch 340/501] Loss: 1.4436, Batch Acc: 0.3750\n",
      "[Batch 350/501] Loss: 1.5447, Batch Acc: 0.3750\n",
      "[Batch 360/501] Loss: 1.6248, Batch Acc: 0.3750\n",
      "[Batch 370/501] Loss: 1.6222, Batch Acc: 0.2500\n",
      "[Batch 380/501] Loss: 1.5707, Batch Acc: 0.5000\n",
      "[Batch 390/501] Loss: 1.4753, Batch Acc: 0.2500\n",
      "[Batch 400/501] Loss: 1.5988, Batch Acc: 0.2500\n",
      "[Batch 410/501] Loss: 1.6256, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.8134, Batch Acc: 0.1250\n",
      "[Batch 430/501] Loss: 1.6592, Batch Acc: 0.2500\n",
      "[Batch 440/501] Loss: 1.5184, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.5861, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.5617, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.5943, Batch Acc: 0.2500\n",
      "[Batch 480/501] Loss: 1.6218, Batch Acc: 0.1250\n",
      "[Batch 490/501] Loss: 1.6319, Batch Acc: 0.3750\n",
      "[Batch 500/501] Loss: 1.5846, Batch Acc: 0.2500\n",
      "Epoch 3 Summary - Loss: 802.9379, Train Accuracy: 0.2682\n",
      "[Batch 10/501] Loss: 1.5568, Batch Acc: 0.2500\n",
      "[Batch 20/501] Loss: 1.9241, Batch Acc: 0.0000\n",
      "[Batch 30/501] Loss: 1.7845, Batch Acc: 0.2500\n",
      "[Batch 40/501] Loss: 1.3651, Batch Acc: 0.3750\n",
      "[Batch 50/501] Loss: 1.7086, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.6634, Batch Acc: 0.2500\n",
      "[Batch 70/501] Loss: 1.5443, Batch Acc: 0.3750\n",
      "[Batch 80/501] Loss: 1.7238, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.4203, Batch Acc: 0.5000\n",
      "[Batch 100/501] Loss: 1.6104, Batch Acc: 0.1250\n",
      "[Batch 110/501] Loss: 1.4048, Batch Acc: 0.3750\n",
      "[Batch 120/501] Loss: 1.7078, Batch Acc: 0.1250\n",
      "[Batch 130/501] Loss: 1.6703, Batch Acc: 0.1250\n",
      "[Batch 140/501] Loss: 1.5513, Batch Acc: 0.5000\n",
      "[Batch 150/501] Loss: 1.5802, Batch Acc: 0.3750\n",
      "[Batch 160/501] Loss: 1.5661, Batch Acc: 0.1250\n",
      "[Batch 170/501] Loss: 1.5166, Batch Acc: 0.5000\n",
      "[Batch 180/501] Loss: 1.6271, Batch Acc: 0.1250\n",
      "[Batch 190/501] Loss: 1.6630, Batch Acc: 0.1250\n",
      "[Batch 200/501] Loss: 1.8350, Batch Acc: 0.0000\n",
      "[Batch 210/501] Loss: 1.4648, Batch Acc: 0.5000\n",
      "[Batch 220/501] Loss: 1.5125, Batch Acc: 0.3750\n",
      "[Batch 230/501] Loss: 1.4475, Batch Acc: 0.3750\n",
      "[Batch 240/501] Loss: 1.6481, Batch Acc: 0.2500\n",
      "[Batch 250/501] Loss: 1.6261, Batch Acc: 0.1250\n",
      "[Batch 260/501] Loss: 1.5348, Batch Acc: 0.3750\n",
      "[Batch 270/501] Loss: 1.6829, Batch Acc: 0.2500\n",
      "[Batch 280/501] Loss: 1.6180, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 1.6243, Batch Acc: 0.1250\n",
      "[Batch 300/501] Loss: 1.5373, Batch Acc: 0.3750\n",
      "[Batch 310/501] Loss: 1.5369, Batch Acc: 0.5000\n",
      "[Batch 320/501] Loss: 1.6343, Batch Acc: 0.1250\n",
      "[Batch 330/501] Loss: 1.4471, Batch Acc: 0.2500\n",
      "[Batch 340/501] Loss: 1.7383, Batch Acc: 0.1250\n",
      "[Batch 350/501] Loss: 1.3800, Batch Acc: 0.2500\n",
      "[Batch 360/501] Loss: 1.5397, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 1.5107, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.4612, Batch Acc: 0.3750\n",
      "[Batch 390/501] Loss: 1.6100, Batch Acc: 0.3750\n",
      "[Batch 400/501] Loss: 1.5428, Batch Acc: 0.3750\n",
      "[Batch 410/501] Loss: 1.6100, Batch Acc: 0.1250\n",
      "[Batch 420/501] Loss: 1.3093, Batch Acc: 0.5000\n",
      "[Batch 430/501] Loss: 1.5374, Batch Acc: 0.2500\n",
      "[Batch 440/501] Loss: 1.7333, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.6221, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.5410, Batch Acc: 0.3750\n",
      "[Batch 470/501] Loss: 1.6568, Batch Acc: 0.2500\n",
      "[Batch 480/501] Loss: 1.5663, Batch Acc: 0.5000\n",
      "[Batch 490/501] Loss: 1.6205, Batch Acc: 0.2500\n",
      "[Batch 500/501] Loss: 1.5724, Batch Acc: 0.1250\n",
      "Epoch 4 Summary - Loss: 801.5953, Train Accuracy: 0.2662\n",
      "[Batch 10/501] Loss: 1.3423, Batch Acc: 0.7500\n",
      "[Batch 20/501] Loss: 1.5010, Batch Acc: 0.3750\n",
      "[Batch 30/501] Loss: 1.6027, Batch Acc: 0.1250\n",
      "[Batch 40/501] Loss: 1.8098, Batch Acc: 0.0000\n",
      "[Batch 50/501] Loss: 1.4350, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.6386, Batch Acc: 0.1250\n",
      "[Batch 70/501] Loss: 1.5669, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.5584, Batch Acc: 0.6250\n",
      "[Batch 90/501] Loss: 1.4525, Batch Acc: 0.5000\n",
      "[Batch 100/501] Loss: 1.7639, Batch Acc: 0.1250\n",
      "[Batch 110/501] Loss: 1.6440, Batch Acc: 0.5000\n",
      "[Batch 120/501] Loss: 1.7673, Batch Acc: 0.0000\n",
      "[Batch 130/501] Loss: 1.6547, Batch Acc: 0.2500\n",
      "[Batch 140/501] Loss: 1.5562, Batch Acc: 0.3750\n",
      "[Batch 150/501] Loss: 1.9216, Batch Acc: 0.0000\n",
      "[Batch 160/501] Loss: 1.5223, Batch Acc: 0.5000\n",
      "[Batch 170/501] Loss: 1.4793, Batch Acc: 0.2500\n",
      "[Batch 180/501] Loss: 1.5663, Batch Acc: 0.2500\n",
      "[Batch 190/501] Loss: 1.5745, Batch Acc: 0.3750\n",
      "[Batch 200/501] Loss: 1.6121, Batch Acc: 0.2500\n",
      "[Batch 210/501] Loss: 1.9092, Batch Acc: 0.1250\n",
      "[Batch 220/501] Loss: 1.8048, Batch Acc: 0.2500\n",
      "[Batch 230/501] Loss: 1.6829, Batch Acc: 0.5000\n",
      "[Batch 240/501] Loss: 1.6637, Batch Acc: 0.2500\n",
      "[Batch 250/501] Loss: 1.5154, Batch Acc: 0.2500\n",
      "[Batch 260/501] Loss: 1.7161, Batch Acc: 0.1250\n",
      "[Batch 270/501] Loss: 1.2267, Batch Acc: 0.7500\n",
      "[Batch 280/501] Loss: 1.6108, Batch Acc: 0.3750\n",
      "[Batch 290/501] Loss: 1.5485, Batch Acc: 0.3750\n",
      "[Batch 300/501] Loss: 1.5241, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 1.6840, Batch Acc: 0.2500\n",
      "[Batch 320/501] Loss: 1.6573, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 1.8591, Batch Acc: 0.1250\n",
      "[Batch 340/501] Loss: 1.6078, Batch Acc: 0.1250\n",
      "[Batch 350/501] Loss: 1.8746, Batch Acc: 0.1250\n",
      "[Batch 360/501] Loss: 1.6850, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 1.5644, Batch Acc: 0.3750\n",
      "[Batch 380/501] Loss: 1.4643, Batch Acc: 0.3750\n",
      "[Batch 390/501] Loss: 1.7065, Batch Acc: 0.0000\n",
      "[Batch 400/501] Loss: 1.3574, Batch Acc: 0.5000\n",
      "[Batch 410/501] Loss: 1.6011, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.5342, Batch Acc: 0.2500\n",
      "[Batch 430/501] Loss: 1.4571, Batch Acc: 0.3750\n",
      "[Batch 440/501] Loss: 1.7407, Batch Acc: 0.1250\n",
      "[Batch 450/501] Loss: 1.4235, Batch Acc: 0.3750\n",
      "[Batch 460/501] Loss: 1.7783, Batch Acc: 0.1250\n",
      "[Batch 470/501] Loss: 1.8362, Batch Acc: 0.2500\n",
      "[Batch 480/501] Loss: 1.4889, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.4886, Batch Acc: 0.1250\n",
      "[Batch 500/501] Loss: 1.5597, Batch Acc: 0.3750\n",
      "Epoch 5 Summary - Loss: 806.0747, Train Accuracy: 0.2637\n",
      "[Batch 10/501] Loss: 1.4766, Batch Acc: 0.3750\n",
      "[Batch 20/501] Loss: 1.6350, Batch Acc: 0.3750\n",
      "[Batch 30/501] Loss: 1.4121, Batch Acc: 0.5000\n",
      "[Batch 40/501] Loss: 1.7838, Batch Acc: 0.2500\n",
      "[Batch 50/501] Loss: 1.4581, Batch Acc: 0.3750\n",
      "[Batch 60/501] Loss: 1.6136, Batch Acc: 0.1250\n",
      "[Batch 70/501] Loss: 1.5357, Batch Acc: 0.3750\n",
      "[Batch 80/501] Loss: 1.4571, Batch Acc: 0.1250\n",
      "[Batch 90/501] Loss: 1.6170, Batch Acc: 0.1250\n",
      "[Batch 100/501] Loss: 1.6153, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 1.4557, Batch Acc: 0.2500\n",
      "[Batch 120/501] Loss: 1.4402, Batch Acc: 0.3750\n",
      "[Batch 130/501] Loss: 1.5104, Batch Acc: 0.1250\n",
      "[Batch 140/501] Loss: 1.7599, Batch Acc: 0.1250\n",
      "[Batch 150/501] Loss: 1.4420, Batch Acc: 0.5000\n",
      "[Batch 160/501] Loss: 1.7998, Batch Acc: 0.3750\n",
      "[Batch 170/501] Loss: 1.6940, Batch Acc: 0.3750\n",
      "[Batch 180/501] Loss: 1.6086, Batch Acc: 0.2500\n",
      "[Batch 190/501] Loss: 1.4930, Batch Acc: 0.3750\n",
      "[Batch 200/501] Loss: 1.5045, Batch Acc: 0.3750\n",
      "[Batch 210/501] Loss: 1.5111, Batch Acc: 0.2500\n",
      "[Batch 220/501] Loss: 1.7427, Batch Acc: 0.2500\n",
      "[Batch 230/501] Loss: 1.6842, Batch Acc: 0.1250\n",
      "[Batch 240/501] Loss: 1.6220, Batch Acc: 0.1250\n",
      "[Batch 250/501] Loss: 1.6484, Batch Acc: 0.2500\n",
      "[Batch 260/501] Loss: 1.6525, Batch Acc: 0.1250\n",
      "[Batch 270/501] Loss: 1.6527, Batch Acc: 0.1250\n",
      "[Batch 280/501] Loss: 1.6891, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 1.5810, Batch Acc: 0.3750\n",
      "[Batch 300/501] Loss: 1.6450, Batch Acc: 0.0000\n",
      "[Batch 310/501] Loss: 1.5814, Batch Acc: 0.5000\n",
      "[Batch 320/501] Loss: 1.6152, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 1.5117, Batch Acc: 0.3750\n",
      "[Batch 340/501] Loss: 1.6065, Batch Acc: 0.2500\n",
      "[Batch 350/501] Loss: 1.5065, Batch Acc: 0.2500\n",
      "[Batch 360/501] Loss: 1.6378, Batch Acc: 0.1250\n",
      "[Batch 370/501] Loss: 1.7879, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.4501, Batch Acc: 0.5000\n",
      "[Batch 390/501] Loss: 1.7822, Batch Acc: 0.0000\n",
      "[Batch 400/501] Loss: 1.6760, Batch Acc: 0.2500\n",
      "[Batch 410/501] Loss: 1.7087, Batch Acc: 0.3750\n",
      "[Batch 420/501] Loss: 1.9161, Batch Acc: 0.1250\n",
      "[Batch 430/501] Loss: 1.7515, Batch Acc: 0.1250\n",
      "[Batch 440/501] Loss: 1.5817, Batch Acc: 0.6250\n",
      "[Batch 450/501] Loss: 1.8170, Batch Acc: 0.1250\n",
      "[Batch 460/501] Loss: 1.6372, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.4860, Batch Acc: 0.5000\n",
      "[Batch 480/501] Loss: 1.6456, Batch Acc: 0.1250\n",
      "[Batch 490/501] Loss: 1.6813, Batch Acc: 0.3750\n",
      "[Batch 500/501] Loss: 1.5313, Batch Acc: 0.3750\n",
      "Epoch 6 Summary - Loss: 804.5375, Train Accuracy: 0.2592\n",
      "[Batch 10/501] Loss: 1.4559, Batch Acc: 0.3750\n",
      "[Batch 20/501] Loss: 1.7679, Batch Acc: 0.0000\n",
      "[Batch 30/501] Loss: 1.5441, Batch Acc: 0.5000\n",
      "[Batch 40/501] Loss: 1.6112, Batch Acc: 0.2500\n",
      "[Batch 50/501] Loss: 1.4581, Batch Acc: 0.5000\n",
      "[Batch 60/501] Loss: 1.5336, Batch Acc: 0.3750\n",
      "[Batch 70/501] Loss: 1.5593, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.5285, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.4795, Batch Acc: 0.2500\n",
      "[Batch 100/501] Loss: 1.6206, Batch Acc: 0.1250\n",
      "[Batch 110/501] Loss: 1.5826, Batch Acc: 0.5000\n",
      "[Batch 120/501] Loss: 1.4196, Batch Acc: 0.5000\n",
      "[Batch 130/501] Loss: 1.6239, Batch Acc: 0.0000\n",
      "[Batch 140/501] Loss: 1.4676, Batch Acc: 0.3750\n",
      "[Batch 150/501] Loss: 1.6563, Batch Acc: 0.1250\n",
      "[Batch 160/501] Loss: 1.5048, Batch Acc: 0.3750\n",
      "[Batch 170/501] Loss: 1.7706, Batch Acc: 0.0000\n",
      "[Batch 180/501] Loss: 1.6835, Batch Acc: 0.2500\n",
      "[Batch 190/501] Loss: 1.3762, Batch Acc: 0.1250\n",
      "[Batch 200/501] Loss: 1.7679, Batch Acc: 0.1250\n",
      "[Batch 210/501] Loss: 1.7422, Batch Acc: 0.1250\n",
      "[Batch 220/501] Loss: 1.6989, Batch Acc: 0.1250\n",
      "[Batch 230/501] Loss: 1.8099, Batch Acc: 0.1250\n",
      "[Batch 240/501] Loss: 1.4231, Batch Acc: 0.3750\n",
      "[Batch 250/501] Loss: 1.6555, Batch Acc: 0.2500\n",
      "[Batch 260/501] Loss: 1.4346, Batch Acc: 0.5000\n",
      "[Batch 270/501] Loss: 1.6272, Batch Acc: 0.1250\n",
      "[Batch 280/501] Loss: 1.6289, Batch Acc: 0.2500\n",
      "[Batch 290/501] Loss: 1.4516, Batch Acc: 0.5000\n",
      "[Batch 300/501] Loss: 1.6899, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 1.6719, Batch Acc: 0.1250\n",
      "[Batch 320/501] Loss: 1.5512, Batch Acc: 0.3750\n",
      "[Batch 330/501] Loss: 1.5208, Batch Acc: 0.1250\n",
      "[Batch 340/501] Loss: 1.6719, Batch Acc: 0.2500\n",
      "[Batch 350/501] Loss: 1.6310, Batch Acc: 0.2500\n",
      "[Batch 360/501] Loss: 1.4546, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 1.6168, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.7154, Batch Acc: 0.2500\n",
      "[Batch 390/501] Loss: 1.6048, Batch Acc: 0.2500\n",
      "[Batch 400/501] Loss: 1.5789, Batch Acc: 0.0000\n",
      "[Batch 410/501] Loss: 1.7444, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.5521, Batch Acc: 0.3750\n",
      "[Batch 430/501] Loss: 1.7914, Batch Acc: 0.0000\n",
      "[Batch 440/501] Loss: 1.7667, Batch Acc: 0.0000\n",
      "[Batch 450/501] Loss: 1.8712, Batch Acc: 0.0000\n",
      "[Batch 460/501] Loss: 1.5816, Batch Acc: 0.3750\n",
      "[Batch 470/501] Loss: 1.7163, Batch Acc: 0.2500\n",
      "[Batch 480/501] Loss: 1.9099, Batch Acc: 0.1250\n",
      "[Batch 490/501] Loss: 1.7928, Batch Acc: 0.1250\n",
      "[Batch 500/501] Loss: 1.5452, Batch Acc: 0.1250\n",
      "Epoch 7 Summary - Loss: 805.7274, Train Accuracy: 0.2565\n",
      "[Batch 10/501] Loss: 1.5350, Batch Acc: 0.2500\n",
      "[Batch 20/501] Loss: 1.5303, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.7343, Batch Acc: 0.1250\n",
      "[Batch 40/501] Loss: 1.3809, Batch Acc: 0.2500\n",
      "[Batch 50/501] Loss: 1.5333, Batch Acc: 0.3750\n",
      "[Batch 60/501] Loss: 1.4780, Batch Acc: 0.2500\n",
      "[Batch 70/501] Loss: 1.5711, Batch Acc: 0.3750\n",
      "[Batch 80/501] Loss: 1.6462, Batch Acc: 0.0000\n",
      "[Batch 90/501] Loss: 1.5105, Batch Acc: 0.5000\n",
      "[Batch 100/501] Loss: 1.5248, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 1.5217, Batch Acc: 0.1250\n",
      "[Batch 120/501] Loss: 1.6145, Batch Acc: 0.3750\n",
      "[Batch 130/501] Loss: 1.4797, Batch Acc: 0.2500\n",
      "[Batch 140/501] Loss: 1.4781, Batch Acc: 0.5000\n",
      "[Batch 150/501] Loss: 1.7443, Batch Acc: 0.1250\n",
      "[Batch 160/501] Loss: 1.6352, Batch Acc: 0.3750\n",
      "[Batch 170/501] Loss: 1.5119, Batch Acc: 0.3750\n",
      "[Batch 180/501] Loss: 1.6407, Batch Acc: 0.1250\n",
      "[Batch 190/501] Loss: 1.4981, Batch Acc: 0.5000\n",
      "[Batch 200/501] Loss: 1.5842, Batch Acc: 0.2500\n",
      "[Batch 210/501] Loss: 1.5108, Batch Acc: 0.1250\n",
      "[Batch 220/501] Loss: 1.7304, Batch Acc: 0.3750\n",
      "[Batch 230/501] Loss: 1.6870, Batch Acc: 0.5000\n",
      "[Batch 240/501] Loss: 1.5933, Batch Acc: 0.1250\n",
      "[Batch 250/501] Loss: 1.5548, Batch Acc: 0.2500\n",
      "[Batch 260/501] Loss: 1.7494, Batch Acc: 0.1250\n",
      "[Batch 270/501] Loss: 1.3050, Batch Acc: 0.7500\n",
      "[Batch 280/501] Loss: 1.4044, Batch Acc: 0.5000\n",
      "[Batch 290/501] Loss: 1.5262, Batch Acc: 0.2500\n",
      "[Batch 300/501] Loss: 1.6092, Batch Acc: 0.3750\n",
      "[Batch 310/501] Loss: 1.5175, Batch Acc: 0.5000\n",
      "[Batch 320/501] Loss: 1.5977, Batch Acc: 0.3750\n",
      "[Batch 330/501] Loss: 1.6451, Batch Acc: 0.3750\n",
      "[Batch 340/501] Loss: 1.8953, Batch Acc: 0.0000\n",
      "[Batch 350/501] Loss: 1.7064, Batch Acc: 0.0000\n",
      "[Batch 360/501] Loss: 1.6386, Batch Acc: 0.1250\n",
      "[Batch 370/501] Loss: 1.7058, Batch Acc: 0.2500\n",
      "[Batch 380/501] Loss: 1.4710, Batch Acc: 0.5000\n",
      "[Batch 390/501] Loss: 1.6405, Batch Acc: 0.5000\n",
      "[Batch 400/501] Loss: 1.7180, Batch Acc: 0.1250\n",
      "[Batch 410/501] Loss: 1.5071, Batch Acc: 0.5000\n",
      "[Batch 420/501] Loss: 1.7110, Batch Acc: 0.3750\n",
      "[Batch 430/501] Loss: 1.7646, Batch Acc: 0.1250\n",
      "[Batch 440/501] Loss: 1.9026, Batch Acc: 0.1250\n",
      "[Batch 450/501] Loss: 1.9639, Batch Acc: 0.1250\n",
      "[Batch 460/501] Loss: 1.4590, Batch Acc: 0.5000\n",
      "[Batch 470/501] Loss: 1.6812, Batch Acc: 0.0000\n",
      "[Batch 480/501] Loss: 1.6390, Batch Acc: 0.3750\n",
      "[Batch 490/501] Loss: 1.6006, Batch Acc: 0.3750\n",
      "[Batch 500/501] Loss: 1.6191, Batch Acc: 0.3750\n",
      "Epoch 8 Summary - Loss: 805.2577, Train Accuracy: 0.2640\n",
      "[Batch 10/501] Loss: 1.6336, Batch Acc: 0.2500\n",
      "[Batch 20/501] Loss: 1.6049, Batch Acc: 0.3750\n",
      "[Batch 30/501] Loss: 1.5624, Batch Acc: 0.2500\n",
      "[Batch 40/501] Loss: 1.6146, Batch Acc: 0.3750\n",
      "[Batch 50/501] Loss: 1.6492, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.6175, Batch Acc: 0.1250\n",
      "[Batch 70/501] Loss: 1.5792, Batch Acc: 0.5000\n",
      "[Batch 80/501] Loss: 1.6031, Batch Acc: 0.1250\n",
      "[Batch 90/501] Loss: 1.7129, Batch Acc: 0.0000\n",
      "[Batch 100/501] Loss: 1.6371, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 1.6865, Batch Acc: 0.2500\n",
      "[Batch 120/501] Loss: 1.5248, Batch Acc: 0.2500\n",
      "[Batch 130/501] Loss: 1.8182, Batch Acc: 0.0000\n",
      "[Batch 140/501] Loss: 1.5876, Batch Acc: 0.1250\n",
      "[Batch 150/501] Loss: 1.7431, Batch Acc: 0.0000\n",
      "[Batch 160/501] Loss: 1.5262, Batch Acc: 0.3750\n",
      "[Batch 170/501] Loss: 1.5203, Batch Acc: 0.3750\n",
      "[Batch 180/501] Loss: 1.4871, Batch Acc: 0.3750\n",
      "[Batch 190/501] Loss: 1.6513, Batch Acc: 0.2500\n",
      "[Batch 200/501] Loss: 1.6287, Batch Acc: 0.1250\n",
      "[Batch 210/501] Loss: 1.5941, Batch Acc: 0.1250\n",
      "[Batch 220/501] Loss: 1.4264, Batch Acc: 0.5000\n",
      "[Batch 230/501] Loss: 1.7118, Batch Acc: 0.1250\n",
      "[Batch 240/501] Loss: 1.6718, Batch Acc: 0.5000\n",
      "[Batch 250/501] Loss: 1.5434, Batch Acc: 0.5000\n",
      "[Batch 260/501] Loss: 1.5350, Batch Acc: 0.3750\n",
      "[Batch 270/501] Loss: 1.4662, Batch Acc: 0.5000\n",
      "[Batch 280/501] Loss: 1.7240, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 1.6295, Batch Acc: 0.2500\n",
      "[Batch 300/501] Loss: 1.7801, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 1.4735, Batch Acc: 0.1250\n",
      "[Batch 320/501] Loss: 1.7191, Batch Acc: 0.0000\n",
      "[Batch 330/501] Loss: 1.5097, Batch Acc: 0.2500\n",
      "[Batch 340/501] Loss: 1.4940, Batch Acc: 0.3750\n",
      "[Batch 350/501] Loss: 1.5919, Batch Acc: 0.2500\n",
      "[Batch 360/501] Loss: 1.6053, Batch Acc: 0.3750\n",
      "[Batch 370/501] Loss: 1.6784, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.7562, Batch Acc: 0.1250\n",
      "[Batch 390/501] Loss: 1.4527, Batch Acc: 0.5000\n",
      "[Batch 400/501] Loss: 1.4994, Batch Acc: 0.2500\n",
      "[Batch 410/501] Loss: 1.6335, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.5999, Batch Acc: 0.3750\n",
      "[Batch 430/501] Loss: 1.8056, Batch Acc: 0.2500\n",
      "[Batch 440/501] Loss: 1.2787, Batch Acc: 0.6250\n",
      "[Batch 450/501] Loss: 1.7822, Batch Acc: 0.0000\n",
      "[Batch 460/501] Loss: 1.5023, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.4870, Batch Acc: 0.2500\n",
      "[Batch 480/501] Loss: 1.5059, Batch Acc: 0.3750\n",
      "[Batch 490/501] Loss: 1.5993, Batch Acc: 0.1250\n",
      "[Batch 500/501] Loss: 1.4748, Batch Acc: 0.6250\n",
      "Epoch 9 Summary - Loss: 801.2414, Train Accuracy: 0.2720\n",
      "[Batch 10/501] Loss: 1.8540, Batch Acc: 0.1250\n",
      "[Batch 20/501] Loss: 1.8251, Batch Acc: 0.0000\n",
      "[Batch 30/501] Loss: 1.4712, Batch Acc: 0.3750\n",
      "[Batch 40/501] Loss: 1.6132, Batch Acc: 0.1250\n",
      "[Batch 50/501] Loss: 1.4339, Batch Acc: 0.5000\n",
      "[Batch 60/501] Loss: 1.7011, Batch Acc: 0.3750\n",
      "[Batch 70/501] Loss: 1.6290, Batch Acc: 0.1250\n",
      "[Batch 80/501] Loss: 1.5963, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.8011, Batch Acc: 0.0000\n",
      "[Batch 100/501] Loss: 1.5512, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 1.5670, Batch Acc: 0.2500\n",
      "[Batch 120/501] Loss: 1.7242, Batch Acc: 0.1250\n",
      "[Batch 130/501] Loss: 1.5358, Batch Acc: 0.3750\n",
      "[Batch 140/501] Loss: 1.5070, Batch Acc: 0.3750\n",
      "[Batch 150/501] Loss: 1.5497, Batch Acc: 0.1250\n",
      "[Batch 160/501] Loss: 1.5830, Batch Acc: 0.2500\n",
      "[Batch 170/501] Loss: 1.7076, Batch Acc: 0.1250\n",
      "[Batch 180/501] Loss: 1.7458, Batch Acc: 0.2500\n",
      "[Batch 190/501] Loss: 1.7366, Batch Acc: 0.2500\n",
      "[Batch 200/501] Loss: 1.7938, Batch Acc: 0.1250\n",
      "[Batch 210/501] Loss: 1.6209, Batch Acc: 0.2500\n",
      "[Batch 220/501] Loss: 1.6195, Batch Acc: 0.0000\n",
      "[Batch 230/501] Loss: 1.5947, Batch Acc: 0.3750\n",
      "[Batch 240/501] Loss: 1.5682, Batch Acc: 0.3750\n",
      "[Batch 250/501] Loss: 1.5438, Batch Acc: 0.1250\n",
      "[Batch 260/501] Loss: 1.5242, Batch Acc: 0.1250\n",
      "[Batch 270/501] Loss: 1.4043, Batch Acc: 0.3750\n",
      "[Batch 280/501] Loss: 1.5589, Batch Acc: 0.3750\n",
      "[Batch 290/501] Loss: 1.6166, Batch Acc: 0.3750\n",
      "[Batch 300/501] Loss: 1.6324, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 1.5333, Batch Acc: 0.2500\n",
      "[Batch 320/501] Loss: 1.5189, Batch Acc: 0.3750\n",
      "[Batch 330/501] Loss: 1.5975, Batch Acc: 0.1250\n",
      "[Batch 340/501] Loss: 1.7541, Batch Acc: 0.2500\n",
      "[Batch 350/501] Loss: 1.6597, Batch Acc: 0.1250\n",
      "[Batch 360/501] Loss: 1.5731, Batch Acc: 0.5000\n",
      "[Batch 370/501] Loss: 1.6631, Batch Acc: 0.3750\n",
      "[Batch 380/501] Loss: 1.4998, Batch Acc: 0.3750\n",
      "[Batch 390/501] Loss: 1.4127, Batch Acc: 0.2500\n",
      "[Batch 400/501] Loss: 1.8187, Batch Acc: 0.0000\n",
      "[Batch 410/501] Loss: 1.2854, Batch Acc: 0.5000\n",
      "[Batch 420/501] Loss: 1.7251, Batch Acc: 0.2500\n",
      "[Batch 430/501] Loss: 1.4807, Batch Acc: 0.1250\n",
      "[Batch 440/501] Loss: 1.6370, Batch Acc: 0.3750\n",
      "[Batch 450/501] Loss: 1.4553, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.5345, Batch Acc: 0.1250\n",
      "[Batch 470/501] Loss: 1.3832, Batch Acc: 0.5000\n",
      "[Batch 480/501] Loss: 1.5323, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.4049, Batch Acc: 0.3750\n",
      "[Batch 500/501] Loss: 1.6109, Batch Acc: 0.1250\n",
      "Epoch 10 Summary - Loss: 805.1322, Train Accuracy: 0.2747\n",
      "Test Accuracy: 0.2908\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_accuracy</td><td>▆▆▆▅▄▂▁▄▇█</td></tr><tr><td>train_loss</td><td>█▅▃▁▇▅▇▆▁▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>test_accuracy</td><td>0.2908</td></tr><tr><td>test_loss</td><td>202.09772</td></tr><tr><td>train_accuracy</td><td>0.2747</td></tr><tr><td>train_loss</td><td>805.1322</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">stoic-sweep-1</strong> at: <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/12vwi9s4' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/12vwi9s4</a><br> View project at: <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250615_135136-12vwi9s4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: wqb11ybm with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_nodes: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ajit.kumar4@happiestminds.com/Documents/Drackula/ComputerVisionFromScratch/wandb/run-20250615_135438-wqb11ybm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/wqb11ybm' target=\"_blank\">confused-sweep-2</a></strong> to <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/wqb11ybm' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/wqb11ybm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 10/501] Loss: 1.5158, Batch Acc: 0.3750\n",
      "[Batch 20/501] Loss: 1.9206, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.5109, Batch Acc: 0.5000\n",
      "[Batch 40/501] Loss: 1.7560, Batch Acc: 0.2500\n",
      "[Batch 50/501] Loss: 1.6506, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.6553, Batch Acc: 0.2500\n",
      "[Batch 70/501] Loss: 1.5192, Batch Acc: 0.3750\n",
      "[Batch 80/501] Loss: 1.9761, Batch Acc: 0.1250\n",
      "[Batch 90/501] Loss: 1.6270, Batch Acc: 0.3750\n",
      "[Batch 100/501] Loss: 1.9550, Batch Acc: 0.0000\n",
      "[Batch 110/501] Loss: 1.7478, Batch Acc: 0.1250\n",
      "[Batch 120/501] Loss: 1.7697, Batch Acc: 0.2500\n",
      "[Batch 130/501] Loss: 1.7731, Batch Acc: 0.2500\n",
      "[Batch 140/501] Loss: 1.5832, Batch Acc: 0.2500\n",
      "[Batch 150/501] Loss: 1.9784, Batch Acc: 0.0000\n",
      "[Batch 160/501] Loss: 1.7573, Batch Acc: 0.1250\n",
      "[Batch 170/501] Loss: 1.6795, Batch Acc: 0.2500\n",
      "[Batch 180/501] Loss: 1.7029, Batch Acc: 0.3750\n",
      "[Batch 190/501] Loss: 1.3742, Batch Acc: 0.2500\n",
      "[Batch 200/501] Loss: 1.7429, Batch Acc: 0.2500\n",
      "[Batch 210/501] Loss: 1.7847, Batch Acc: 0.0000\n",
      "[Batch 220/501] Loss: 1.5338, Batch Acc: 0.3750\n",
      "[Batch 230/501] Loss: 1.6732, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.7402, Batch Acc: 0.2500\n",
      "[Batch 250/501] Loss: 2.2155, Batch Acc: 0.1250\n",
      "[Batch 260/501] Loss: 1.3612, Batch Acc: 0.5000\n",
      "[Batch 270/501] Loss: 1.7086, Batch Acc: 0.1250\n",
      "[Batch 280/501] Loss: 1.8847, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 2.0134, Batch Acc: 0.1250\n",
      "[Batch 300/501] Loss: 1.5806, Batch Acc: 0.5000\n",
      "[Batch 310/501] Loss: 1.5130, Batch Acc: 0.2500\n",
      "[Batch 320/501] Loss: 1.9503, Batch Acc: 0.1250\n",
      "[Batch 330/501] Loss: 1.7800, Batch Acc: 0.2500\n",
      "[Batch 340/501] Loss: 1.9389, Batch Acc: 0.2500\n",
      "[Batch 350/501] Loss: 1.8116, Batch Acc: 0.0000\n",
      "[Batch 360/501] Loss: 1.6192, Batch Acc: 0.3750\n",
      "[Batch 370/501] Loss: 1.5230, Batch Acc: 0.3750\n",
      "[Batch 380/501] Loss: 1.8425, Batch Acc: 0.1250\n",
      "[Batch 390/501] Loss: 1.5681, Batch Acc: 0.2500\n",
      "[Batch 400/501] Loss: 1.7067, Batch Acc: 0.3750\n",
      "[Batch 410/501] Loss: 1.8429, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.5651, Batch Acc: 0.2500\n",
      "[Batch 430/501] Loss: 1.7433, Batch Acc: 0.3750\n",
      "[Batch 440/501] Loss: 1.3982, Batch Acc: 0.3750\n",
      "[Batch 450/501] Loss: 1.6433, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.2841, Batch Acc: 0.6250\n",
      "[Batch 470/501] Loss: 1.9695, Batch Acc: 0.2500\n",
      "[Batch 480/501] Loss: 1.4483, Batch Acc: 0.6250\n",
      "[Batch 490/501] Loss: 1.6562, Batch Acc: 0.2500\n",
      "[Batch 500/501] Loss: 1.6471, Batch Acc: 0.5000\n",
      "Epoch 1 Summary - Loss: 864.1592, Train Accuracy: 0.2273\n",
      "[Batch 10/501] Loss: 1.6180, Batch Acc: 0.2500\n",
      "[Batch 20/501] Loss: 1.6028, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.4259, Batch Acc: 0.2500\n",
      "[Batch 40/501] Loss: 1.4924, Batch Acc: 0.3750\n",
      "[Batch 50/501] Loss: 1.7093, Batch Acc: 0.1250\n",
      "[Batch 60/501] Loss: 1.5995, Batch Acc: 0.2500\n",
      "[Batch 70/501] Loss: 1.9154, Batch Acc: 0.1250\n",
      "[Batch 80/501] Loss: 1.7296, Batch Acc: 0.3750\n",
      "[Batch 90/501] Loss: 1.4883, Batch Acc: 0.3750\n",
      "[Batch 100/501] Loss: 1.6929, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 1.7330, Batch Acc: 0.2500\n",
      "[Batch 120/501] Loss: 1.5103, Batch Acc: 0.3750\n",
      "[Batch 130/501] Loss: 1.6927, Batch Acc: 0.2500\n",
      "[Batch 140/501] Loss: 1.7146, Batch Acc: 0.2500\n",
      "[Batch 150/501] Loss: 1.8196, Batch Acc: 0.1250\n",
      "[Batch 160/501] Loss: 2.1040, Batch Acc: 0.1250\n",
      "[Batch 170/501] Loss: 1.9219, Batch Acc: 0.0000\n",
      "[Batch 180/501] Loss: 1.5728, Batch Acc: 0.3750\n",
      "[Batch 190/501] Loss: 1.6502, Batch Acc: 0.1250\n",
      "[Batch 200/501] Loss: 1.9545, Batch Acc: 0.1250\n",
      "[Batch 210/501] Loss: 1.4967, Batch Acc: 0.5000\n",
      "[Batch 220/501] Loss: 1.3984, Batch Acc: 0.5000\n",
      "[Batch 230/501] Loss: 1.6052, Batch Acc: 0.5000\n",
      "[Batch 240/501] Loss: 1.4241, Batch Acc: 0.3750\n",
      "[Batch 250/501] Loss: 1.8863, Batch Acc: 0.0000\n",
      "[Batch 260/501] Loss: 1.8491, Batch Acc: 0.1250\n",
      "[Batch 270/501] Loss: 1.5618, Batch Acc: 0.3750\n",
      "[Batch 280/501] Loss: 1.5598, Batch Acc: 0.3750\n",
      "[Batch 290/501] Loss: 1.8365, Batch Acc: 0.1250\n",
      "[Batch 300/501] Loss: 1.6465, Batch Acc: 0.3750\n",
      "[Batch 310/501] Loss: 1.7693, Batch Acc: 0.3750\n",
      "[Batch 320/501] Loss: 1.7685, Batch Acc: 0.3750\n",
      "[Batch 330/501] Loss: 1.9004, Batch Acc: 0.1250\n",
      "[Batch 340/501] Loss: 1.5897, Batch Acc: 0.3750\n",
      "[Batch 350/501] Loss: 1.6336, Batch Acc: 0.2500\n",
      "[Batch 360/501] Loss: 1.8753, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 1.5805, Batch Acc: 0.2500\n",
      "[Batch 380/501] Loss: 1.7415, Batch Acc: 0.2500\n",
      "[Batch 390/501] Loss: 1.9714, Batch Acc: 0.1250\n",
      "[Batch 400/501] Loss: 2.0530, Batch Acc: 0.0000\n",
      "[Batch 410/501] Loss: 1.7350, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.4948, Batch Acc: 0.5000\n",
      "[Batch 430/501] Loss: 1.4105, Batch Acc: 0.2500\n",
      "[Batch 440/501] Loss: 1.6029, Batch Acc: 0.3750\n",
      "[Batch 450/501] Loss: 1.6213, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.6166, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.9393, Batch Acc: 0.1250\n",
      "[Batch 480/501] Loss: 1.7748, Batch Acc: 0.1250\n",
      "[Batch 490/501] Loss: 1.7896, Batch Acc: 0.1250\n",
      "[Batch 500/501] Loss: 1.8493, Batch Acc: 0.1250\n",
      "Epoch 2 Summary - Loss: 865.3281, Train Accuracy: 0.2300\n",
      "[Batch 10/501] Loss: 2.2092, Batch Acc: 0.0000\n",
      "[Batch 20/501] Loss: 1.5099, Batch Acc: 0.1250\n",
      "[Batch 30/501] Loss: 1.4067, Batch Acc: 0.5000\n",
      "[Batch 40/501] Loss: 1.7980, Batch Acc: 0.2500\n",
      "[Batch 50/501] Loss: 2.0510, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.9734, Batch Acc: 0.0000\n",
      "[Batch 70/501] Loss: 1.7291, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.4187, Batch Acc: 0.5000\n",
      "[Batch 90/501] Loss: 1.6265, Batch Acc: 0.1250\n",
      "[Batch 100/501] Loss: 1.8508, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 1.7111, Batch Acc: 0.3750\n",
      "[Batch 120/501] Loss: 1.5736, Batch Acc: 0.3750\n",
      "[Batch 130/501] Loss: 2.0903, Batch Acc: 0.1250\n",
      "[Batch 140/501] Loss: 1.7959, Batch Acc: 0.0000\n",
      "[Batch 150/501] Loss: 1.5510, Batch Acc: 0.3750\n",
      "[Batch 160/501] Loss: 1.4186, Batch Acc: 0.3750\n",
      "[Batch 170/501] Loss: 1.7764, Batch Acc: 0.2500\n",
      "[Batch 180/501] Loss: 1.6736, Batch Acc: 0.0000\n",
      "[Batch 190/501] Loss: 1.9965, Batch Acc: 0.1250\n",
      "[Batch 200/501] Loss: 2.0053, Batch Acc: 0.0000\n",
      "[Batch 210/501] Loss: 1.5474, Batch Acc: 0.2500\n",
      "[Batch 220/501] Loss: 1.4563, Batch Acc: 0.2500\n",
      "[Batch 230/501] Loss: 1.8677, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.8118, Batch Acc: 0.2500\n",
      "[Batch 250/501] Loss: 1.6073, Batch Acc: 0.1250\n",
      "[Batch 260/501] Loss: 1.6995, Batch Acc: 0.1250\n",
      "[Batch 270/501] Loss: 1.6854, Batch Acc: 0.2500\n",
      "[Batch 280/501] Loss: 1.5762, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 1.7514, Batch Acc: 0.1250\n",
      "[Batch 300/501] Loss: 1.8701, Batch Acc: 0.1250\n",
      "[Batch 310/501] Loss: 1.7367, Batch Acc: 0.1250\n",
      "[Batch 320/501] Loss: 1.8308, Batch Acc: 0.0000\n",
      "[Batch 330/501] Loss: 1.3434, Batch Acc: 0.6250\n",
      "[Batch 340/501] Loss: 1.6714, Batch Acc: 0.2500\n",
      "[Batch 350/501] Loss: 1.9462, Batch Acc: 0.1250\n",
      "[Batch 360/501] Loss: 1.9474, Batch Acc: 0.0000\n",
      "[Batch 370/501] Loss: 1.9159, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.6993, Batch Acc: 0.3750\n",
      "[Batch 390/501] Loss: 1.4665, Batch Acc: 0.3750\n",
      "[Batch 400/501] Loss: 1.4235, Batch Acc: 0.2500\n",
      "[Batch 410/501] Loss: 1.6315, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.4164, Batch Acc: 0.5000\n",
      "[Batch 430/501] Loss: 2.0457, Batch Acc: 0.1250\n",
      "[Batch 440/501] Loss: 1.4845, Batch Acc: 0.1250\n",
      "[Batch 450/501] Loss: 1.6823, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.5189, Batch Acc: 0.1250\n",
      "[Batch 470/501] Loss: 1.7561, Batch Acc: 0.1250\n",
      "[Batch 480/501] Loss: 1.9692, Batch Acc: 0.1250\n",
      "[Batch 490/501] Loss: 1.2882, Batch Acc: 0.3750\n",
      "[Batch 500/501] Loss: 1.8053, Batch Acc: 0.1250\n",
      "Epoch 3 Summary - Loss: 865.1734, Train Accuracy: 0.2178\n",
      "[Batch 10/501] Loss: 1.8243, Batch Acc: 0.0000\n",
      "[Batch 20/501] Loss: 1.7293, Batch Acc: 0.1250\n",
      "[Batch 30/501] Loss: 1.5643, Batch Acc: 0.2500\n",
      "[Batch 40/501] Loss: 2.0942, Batch Acc: 0.1250\n",
      "[Batch 50/501] Loss: 1.8958, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.9269, Batch Acc: 0.1250\n",
      "[Batch 70/501] Loss: 1.7579, Batch Acc: 0.3750\n",
      "[Batch 80/501] Loss: 1.7805, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.8806, Batch Acc: 0.2500\n",
      "[Batch 100/501] Loss: 1.8664, Batch Acc: 0.3750\n",
      "[Batch 110/501] Loss: 1.9962, Batch Acc: 0.0000\n",
      "[Batch 120/501] Loss: 1.6051, Batch Acc: 0.2500\n",
      "[Batch 130/501] Loss: 1.7849, Batch Acc: 0.1250\n",
      "[Batch 140/501] Loss: 1.7536, Batch Acc: 0.3750\n",
      "[Batch 150/501] Loss: 1.5419, Batch Acc: 0.1250\n",
      "[Batch 160/501] Loss: 2.0810, Batch Acc: 0.0000\n",
      "[Batch 170/501] Loss: 1.5700, Batch Acc: 0.1250\n",
      "[Batch 180/501] Loss: 1.6921, Batch Acc: 0.3750\n",
      "[Batch 190/501] Loss: 1.9180, Batch Acc: 0.1250\n",
      "[Batch 200/501] Loss: 1.7805, Batch Acc: 0.2500\n",
      "[Batch 210/501] Loss: 1.7982, Batch Acc: 0.1250\n",
      "[Batch 220/501] Loss: 2.0724, Batch Acc: 0.0000\n",
      "[Batch 230/501] Loss: 1.9998, Batch Acc: 0.0000\n",
      "[Batch 240/501] Loss: 1.6081, Batch Acc: 0.1250\n",
      "[Batch 250/501] Loss: 1.7572, Batch Acc: 0.1250\n",
      "[Batch 260/501] Loss: 1.7976, Batch Acc: 0.1250\n",
      "[Batch 270/501] Loss: 1.8002, Batch Acc: 0.2500\n",
      "[Batch 280/501] Loss: 1.8523, Batch Acc: 0.2500\n",
      "[Batch 290/501] Loss: 1.3987, Batch Acc: 0.2500\n",
      "[Batch 300/501] Loss: 1.6385, Batch Acc: 0.1250\n",
      "[Batch 310/501] Loss: 1.6400, Batch Acc: 0.3750\n",
      "[Batch 320/501] Loss: 1.9062, Batch Acc: 0.1250\n",
      "[Batch 330/501] Loss: 2.1244, Batch Acc: 0.0000\n",
      "[Batch 340/501] Loss: 1.5019, Batch Acc: 0.3750\n",
      "[Batch 350/501] Loss: 1.5130, Batch Acc: 0.1250\n",
      "[Batch 360/501] Loss: 1.7486, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 1.8799, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.8100, Batch Acc: 0.1250\n",
      "[Batch 390/501] Loss: 1.4435, Batch Acc: 0.3750\n",
      "[Batch 400/501] Loss: 1.7094, Batch Acc: 0.2500\n",
      "[Batch 410/501] Loss: 1.8152, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.4994, Batch Acc: 0.3750\n",
      "[Batch 430/501] Loss: 1.8335, Batch Acc: 0.2500\n",
      "[Batch 440/501] Loss: 1.9778, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.4055, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.9432, Batch Acc: 0.1250\n",
      "[Batch 470/501] Loss: 1.6260, Batch Acc: 0.2500\n",
      "[Batch 480/501] Loss: 1.6951, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.4999, Batch Acc: 0.6250\n",
      "[Batch 500/501] Loss: 1.8034, Batch Acc: 0.1250\n",
      "Epoch 4 Summary - Loss: 866.8256, Train Accuracy: 0.2226\n",
      "[Batch 10/501] Loss: 2.0248, Batch Acc: 0.0000\n",
      "[Batch 20/501] Loss: 1.7716, Batch Acc: 0.3750\n",
      "[Batch 30/501] Loss: 1.8713, Batch Acc: 0.0000\n",
      "[Batch 40/501] Loss: 1.4954, Batch Acc: 0.2500\n",
      "[Batch 50/501] Loss: 1.8785, Batch Acc: 0.1250\n",
      "[Batch 60/501] Loss: 1.4989, Batch Acc: 0.3750\n",
      "[Batch 70/501] Loss: 1.9026, Batch Acc: 0.1250\n",
      "[Batch 80/501] Loss: 1.7281, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.4276, Batch Acc: 0.3750\n",
      "[Batch 100/501] Loss: 1.2615, Batch Acc: 0.7500\n",
      "[Batch 110/501] Loss: 2.0378, Batch Acc: 0.1250\n",
      "[Batch 120/501] Loss: 2.0689, Batch Acc: 0.1250\n",
      "[Batch 130/501] Loss: 1.8427, Batch Acc: 0.1250\n",
      "[Batch 140/501] Loss: 2.0066, Batch Acc: 0.0000\n",
      "[Batch 150/501] Loss: 1.8817, Batch Acc: 0.3750\n",
      "[Batch 160/501] Loss: 1.4206, Batch Acc: 0.2500\n",
      "[Batch 170/501] Loss: 2.0272, Batch Acc: 0.0000\n",
      "[Batch 180/501] Loss: 1.9330, Batch Acc: 0.1250\n",
      "[Batch 190/501] Loss: 1.5603, Batch Acc: 0.3750\n",
      "[Batch 200/501] Loss: 1.9440, Batch Acc: 0.1250\n",
      "[Batch 210/501] Loss: 1.2354, Batch Acc: 0.2500\n",
      "[Batch 220/501] Loss: 1.6672, Batch Acc: 0.1250\n",
      "[Batch 230/501] Loss: 1.9674, Batch Acc: 0.1250\n",
      "[Batch 240/501] Loss: 1.6937, Batch Acc: 0.2500\n",
      "[Batch 250/501] Loss: 2.0351, Batch Acc: 0.1250\n",
      "[Batch 260/501] Loss: 1.5326, Batch Acc: 0.5000\n",
      "[Batch 270/501] Loss: 1.8205, Batch Acc: 0.1250\n",
      "[Batch 280/501] Loss: 1.5996, Batch Acc: 0.2500\n",
      "[Batch 290/501] Loss: 1.7629, Batch Acc: 0.1250\n",
      "[Batch 300/501] Loss: 1.7844, Batch Acc: 0.1250\n",
      "[Batch 310/501] Loss: 1.6347, Batch Acc: 0.3750\n",
      "[Batch 320/501] Loss: 2.0605, Batch Acc: 0.1250\n",
      "[Batch 330/501] Loss: 1.4033, Batch Acc: 0.1250\n",
      "[Batch 340/501] Loss: 1.9635, Batch Acc: 0.1250\n",
      "[Batch 350/501] Loss: 1.7124, Batch Acc: 0.2500\n",
      "[Batch 360/501] Loss: 1.5750, Batch Acc: 0.3750\n",
      "[Batch 370/501] Loss: 1.9085, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.5963, Batch Acc: 0.0000\n",
      "[Batch 390/501] Loss: 1.5773, Batch Acc: 0.2500\n",
      "[Batch 400/501] Loss: 1.5036, Batch Acc: 0.3750\n",
      "[Batch 410/501] Loss: 2.0220, Batch Acc: 0.0000\n",
      "[Batch 420/501] Loss: 1.8728, Batch Acc: 0.1250\n",
      "[Batch 430/501] Loss: 1.5782, Batch Acc: 0.2500\n",
      "[Batch 440/501] Loss: 1.5441, Batch Acc: 0.3750\n",
      "[Batch 450/501] Loss: 1.4930, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.5863, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.7879, Batch Acc: 0.1250\n",
      "[Batch 480/501] Loss: 1.8012, Batch Acc: 0.1250\n",
      "[Batch 490/501] Loss: 1.7095, Batch Acc: 0.2500\n",
      "[Batch 500/501] Loss: 1.6786, Batch Acc: 0.3750\n",
      "Epoch 5 Summary - Loss: 866.0438, Train Accuracy: 0.2241\n",
      "[Batch 10/501] Loss: 1.9348, Batch Acc: 0.1250\n",
      "[Batch 20/501] Loss: 1.9486, Batch Acc: 0.0000\n",
      "[Batch 30/501] Loss: 2.1684, Batch Acc: 0.0000\n",
      "[Batch 40/501] Loss: 1.9865, Batch Acc: 0.1250\n",
      "[Batch 50/501] Loss: 1.9127, Batch Acc: 0.1250\n",
      "[Batch 60/501] Loss: 1.8369, Batch Acc: 0.3750\n",
      "[Batch 70/501] Loss: 1.6447, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 2.0847, Batch Acc: 0.1250\n",
      "[Batch 90/501] Loss: 1.2635, Batch Acc: 0.3750\n",
      "[Batch 100/501] Loss: 1.7734, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 1.7026, Batch Acc: 0.1250\n",
      "[Batch 120/501] Loss: 1.2711, Batch Acc: 0.5000\n",
      "[Batch 130/501] Loss: 1.6785, Batch Acc: 0.2500\n",
      "[Batch 140/501] Loss: 1.3607, Batch Acc: 0.3750\n",
      "[Batch 150/501] Loss: 1.9457, Batch Acc: 0.2500\n",
      "[Batch 160/501] Loss: 2.1810, Batch Acc: 0.1250\n",
      "[Batch 170/501] Loss: 1.6733, Batch Acc: 0.3750\n",
      "[Batch 180/501] Loss: 1.4370, Batch Acc: 0.3750\n",
      "[Batch 190/501] Loss: 2.0049, Batch Acc: 0.0000\n",
      "[Batch 200/501] Loss: 1.6833, Batch Acc: 0.2500\n",
      "[Batch 210/501] Loss: 1.7383, Batch Acc: 0.2500\n",
      "[Batch 220/501] Loss: 1.6908, Batch Acc: 0.1250\n",
      "[Batch 230/501] Loss: 2.0941, Batch Acc: 0.0000\n",
      "[Batch 240/501] Loss: 1.6285, Batch Acc: 0.3750\n",
      "[Batch 250/501] Loss: 1.7444, Batch Acc: 0.1250\n",
      "[Batch 260/501] Loss: 1.5651, Batch Acc: 0.3750\n",
      "[Batch 270/501] Loss: 1.4632, Batch Acc: 0.3750\n",
      "[Batch 280/501] Loss: 1.6345, Batch Acc: 0.2500\n",
      "[Batch 290/501] Loss: 1.9640, Batch Acc: 0.2500\n",
      "[Batch 300/501] Loss: 1.7340, Batch Acc: 0.3750\n",
      "[Batch 310/501] Loss: 1.6448, Batch Acc: 0.0000\n",
      "[Batch 320/501] Loss: 1.5430, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 1.6720, Batch Acc: 0.2500\n",
      "[Batch 340/501] Loss: 1.5834, Batch Acc: 0.3750\n",
      "[Batch 350/501] Loss: 1.7671, Batch Acc: 0.0000\n",
      "[Batch 360/501] Loss: 1.6993, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 2.1136, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.9108, Batch Acc: 0.2500\n",
      "[Batch 390/501] Loss: 1.3920, Batch Acc: 0.3750\n",
      "[Batch 400/501] Loss: 1.8150, Batch Acc: 0.1250\n",
      "[Batch 410/501] Loss: 1.9763, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 2.0168, Batch Acc: 0.0000\n",
      "[Batch 430/501] Loss: 1.4692, Batch Acc: 0.3750\n",
      "[Batch 440/501] Loss: 1.5705, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.4526, Batch Acc: 0.1250\n",
      "[Batch 460/501] Loss: 1.5083, Batch Acc: 0.1250\n",
      "[Batch 470/501] Loss: 1.4991, Batch Acc: 0.3750\n",
      "[Batch 480/501] Loss: 1.6355, Batch Acc: 0.3750\n",
      "[Batch 490/501] Loss: 1.9689, Batch Acc: 0.0000\n",
      "[Batch 500/501] Loss: 1.3306, Batch Acc: 0.5000\n",
      "Epoch 6 Summary - Loss: 866.1128, Train Accuracy: 0.2203\n",
      "[Batch 10/501] Loss: 1.9083, Batch Acc: 0.0000\n",
      "[Batch 20/501] Loss: 1.8518, Batch Acc: 0.1250\n",
      "[Batch 30/501] Loss: 1.9871, Batch Acc: 0.1250\n",
      "[Batch 40/501] Loss: 1.7732, Batch Acc: 0.1250\n",
      "[Batch 50/501] Loss: 1.6645, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.9673, Batch Acc: 0.2500\n",
      "[Batch 70/501] Loss: 1.6788, Batch Acc: 0.1250\n",
      "[Batch 80/501] Loss: 1.3770, Batch Acc: 0.5000\n",
      "[Batch 90/501] Loss: 1.8458, Batch Acc: 0.1250\n",
      "[Batch 100/501] Loss: 1.4908, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 1.9355, Batch Acc: 0.1250\n",
      "[Batch 120/501] Loss: 1.6441, Batch Acc: 0.3750\n",
      "[Batch 130/501] Loss: 1.7199, Batch Acc: 0.3750\n",
      "[Batch 140/501] Loss: 2.0266, Batch Acc: 0.0000\n",
      "[Batch 150/501] Loss: 1.5626, Batch Acc: 0.2500\n",
      "[Batch 160/501] Loss: 1.8950, Batch Acc: 0.0000\n",
      "[Batch 170/501] Loss: 1.8153, Batch Acc: 0.1250\n",
      "[Batch 180/501] Loss: 1.9783, Batch Acc: 0.1250\n",
      "[Batch 190/501] Loss: 1.9369, Batch Acc: 0.1250\n",
      "[Batch 200/501] Loss: 1.9407, Batch Acc: 0.0000\n",
      "[Batch 210/501] Loss: 1.6772, Batch Acc: 0.2500\n",
      "[Batch 220/501] Loss: 1.1934, Batch Acc: 0.5000\n",
      "[Batch 230/501] Loss: 1.7254, Batch Acc: 0.3750\n",
      "[Batch 240/501] Loss: 1.7542, Batch Acc: 0.1250\n",
      "[Batch 250/501] Loss: 1.6728, Batch Acc: 0.2500\n",
      "[Batch 260/501] Loss: 1.6028, Batch Acc: 0.1250\n",
      "[Batch 270/501] Loss: 1.8887, Batch Acc: 0.1250\n",
      "[Batch 280/501] Loss: 1.9963, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 1.9816, Batch Acc: 0.0000\n",
      "[Batch 300/501] Loss: 1.6445, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 1.6878, Batch Acc: 0.3750\n",
      "[Batch 320/501] Loss: 2.0325, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 2.1321, Batch Acc: 0.0000\n",
      "[Batch 340/501] Loss: 1.6224, Batch Acc: 0.3750\n",
      "[Batch 350/501] Loss: 1.7916, Batch Acc: 0.3750\n",
      "[Batch 360/501] Loss: 1.7686, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 1.6940, Batch Acc: 0.5000\n",
      "[Batch 380/501] Loss: 1.5013, Batch Acc: 0.3750\n",
      "[Batch 390/501] Loss: 2.0173, Batch Acc: 0.0000\n",
      "[Batch 400/501] Loss: 1.7328, Batch Acc: 0.3750\n",
      "[Batch 410/501] Loss: 1.7215, Batch Acc: 0.5000\n",
      "[Batch 420/501] Loss: 1.7171, Batch Acc: 0.1250\n",
      "[Batch 430/501] Loss: 2.0216, Batch Acc: 0.1250\n",
      "[Batch 440/501] Loss: 1.9732, Batch Acc: 0.0000\n",
      "[Batch 450/501] Loss: 1.8368, Batch Acc: 0.1250\n",
      "[Batch 460/501] Loss: 1.8101, Batch Acc: 0.0000\n",
      "[Batch 470/501] Loss: 1.6911, Batch Acc: 0.3750\n",
      "[Batch 480/501] Loss: 1.5152, Batch Acc: 0.5000\n",
      "[Batch 490/501] Loss: 1.8295, Batch Acc: 0.2500\n",
      "[Batch 500/501] Loss: 1.8701, Batch Acc: 0.2500\n",
      "Epoch 7 Summary - Loss: 866.9440, Train Accuracy: 0.2226\n",
      "[Batch 10/501] Loss: 1.9690, Batch Acc: 0.2500\n",
      "[Batch 20/501] Loss: 1.3765, Batch Acc: 0.5000\n",
      "[Batch 30/501] Loss: 1.9002, Batch Acc: 0.0000\n",
      "[Batch 40/501] Loss: 1.9636, Batch Acc: 0.1250\n",
      "[Batch 50/501] Loss: 1.4333, Batch Acc: 0.3750\n",
      "[Batch 60/501] Loss: 1.5201, Batch Acc: 0.5000\n",
      "[Batch 70/501] Loss: 1.6334, Batch Acc: 0.3750\n",
      "[Batch 80/501] Loss: 1.5439, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.6850, Batch Acc: 0.2500\n",
      "[Batch 100/501] Loss: 1.4318, Batch Acc: 0.5000\n",
      "[Batch 110/501] Loss: 1.4357, Batch Acc: 0.3750\n",
      "[Batch 120/501] Loss: 1.5656, Batch Acc: 0.5000\n",
      "[Batch 130/501] Loss: 1.4866, Batch Acc: 0.5000\n",
      "[Batch 140/501] Loss: 1.9147, Batch Acc: 0.1250\n",
      "[Batch 150/501] Loss: 1.9992, Batch Acc: 0.1250\n",
      "[Batch 160/501] Loss: 1.8047, Batch Acc: 0.2500\n",
      "[Batch 170/501] Loss: 1.8313, Batch Acc: 0.0000\n",
      "[Batch 180/501] Loss: 1.5816, Batch Acc: 0.1250\n",
      "[Batch 190/501] Loss: 1.6667, Batch Acc: 0.3750\n",
      "[Batch 200/501] Loss: 1.4606, Batch Acc: 0.3750\n",
      "[Batch 210/501] Loss: 1.8407, Batch Acc: 0.1250\n",
      "[Batch 220/501] Loss: 2.1604, Batch Acc: 0.1250\n",
      "[Batch 230/501] Loss: 1.8046, Batch Acc: 0.1250\n",
      "[Batch 240/501] Loss: 1.5894, Batch Acc: 0.5000\n",
      "[Batch 250/501] Loss: 1.6641, Batch Acc: 0.1250\n",
      "[Batch 260/501] Loss: 2.0920, Batch Acc: 0.1250\n",
      "[Batch 270/501] Loss: 1.6459, Batch Acc: 0.2500\n",
      "[Batch 280/501] Loss: 1.8631, Batch Acc: 0.2500\n",
      "[Batch 290/501] Loss: 2.0930, Batch Acc: 0.0000\n",
      "[Batch 300/501] Loss: 1.5943, Batch Acc: 0.5000\n",
      "[Batch 310/501] Loss: 1.4617, Batch Acc: 0.3750\n",
      "[Batch 320/501] Loss: 1.9999, Batch Acc: 0.1250\n",
      "[Batch 330/501] Loss: 1.8117, Batch Acc: 0.1250\n",
      "[Batch 340/501] Loss: 1.7284, Batch Acc: 0.0000\n",
      "[Batch 350/501] Loss: 1.6210, Batch Acc: 0.5000\n",
      "[Batch 360/501] Loss: 1.4524, Batch Acc: 0.3750\n",
      "[Batch 370/501] Loss: 1.5490, Batch Acc: 0.2500\n",
      "[Batch 380/501] Loss: 1.9234, Batch Acc: 0.0000\n",
      "[Batch 390/501] Loss: 1.7344, Batch Acc: 0.2500\n",
      "[Batch 400/501] Loss: 1.7629, Batch Acc: 0.2500\n",
      "[Batch 410/501] Loss: 1.5126, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.5776, Batch Acc: 0.2500\n",
      "[Batch 430/501] Loss: 1.5928, Batch Acc: 0.3750\n",
      "[Batch 440/501] Loss: 1.8161, Batch Acc: 0.1250\n",
      "[Batch 450/501] Loss: 1.6173, Batch Acc: 0.1250\n",
      "[Batch 460/501] Loss: 1.6909, Batch Acc: 0.1250\n",
      "[Batch 470/501] Loss: 1.8370, Batch Acc: 0.2500\n",
      "[Batch 480/501] Loss: 1.6469, Batch Acc: 0.1250\n",
      "[Batch 490/501] Loss: 1.7249, Batch Acc: 0.3750\n",
      "[Batch 500/501] Loss: 1.8081, Batch Acc: 0.2500\n",
      "Epoch 8 Summary - Loss: 865.4521, Train Accuracy: 0.2255\n",
      "[Batch 10/501] Loss: 1.8048, Batch Acc: 0.1250\n",
      "[Batch 20/501] Loss: 1.4054, Batch Acc: 0.5000\n",
      "[Batch 30/501] Loss: 1.8477, Batch Acc: 0.0000\n",
      "[Batch 40/501] Loss: 1.9155, Batch Acc: 0.0000\n",
      "[Batch 50/501] Loss: 1.8296, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.5816, Batch Acc: 0.3750\n",
      "[Batch 70/501] Loss: 1.6482, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.6843, Batch Acc: 0.3750\n",
      "[Batch 90/501] Loss: 1.3258, Batch Acc: 0.2500\n",
      "[Batch 100/501] Loss: 1.9274, Batch Acc: 0.0000\n",
      "[Batch 110/501] Loss: 1.9542, Batch Acc: 0.1250\n",
      "[Batch 120/501] Loss: 1.5367, Batch Acc: 0.1250\n",
      "[Batch 130/501] Loss: 1.5562, Batch Acc: 0.2500\n",
      "[Batch 140/501] Loss: 2.1714, Batch Acc: 0.0000\n",
      "[Batch 150/501] Loss: 1.6803, Batch Acc: 0.2500\n",
      "[Batch 160/501] Loss: 1.8224, Batch Acc: 0.2500\n",
      "[Batch 170/501] Loss: 1.6524, Batch Acc: 0.2500\n",
      "[Batch 180/501] Loss: 1.5444, Batch Acc: 0.6250\n",
      "[Batch 190/501] Loss: 1.8818, Batch Acc: 0.0000\n",
      "[Batch 200/501] Loss: 1.3554, Batch Acc: 0.5000\n",
      "[Batch 210/501] Loss: 1.9339, Batch Acc: 0.1250\n",
      "[Batch 220/501] Loss: 1.5333, Batch Acc: 0.5000\n",
      "[Batch 230/501] Loss: 1.6269, Batch Acc: 0.1250\n",
      "[Batch 240/501] Loss: 1.7584, Batch Acc: 0.1250\n",
      "[Batch 250/501] Loss: 1.9270, Batch Acc: 0.0000\n",
      "[Batch 260/501] Loss: 1.5326, Batch Acc: 0.2500\n",
      "[Batch 270/501] Loss: 1.7282, Batch Acc: 0.3750\n",
      "[Batch 280/501] Loss: 1.7926, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 1.4122, Batch Acc: 0.3750\n",
      "[Batch 300/501] Loss: 1.7118, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 1.3715, Batch Acc: 0.5000\n",
      "[Batch 320/501] Loss: 1.6594, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 1.8403, Batch Acc: 0.2500\n",
      "[Batch 340/501] Loss: 1.6424, Batch Acc: 0.1250\n",
      "[Batch 350/501] Loss: 1.5232, Batch Acc: 0.5000\n",
      "[Batch 360/501] Loss: 2.0999, Batch Acc: 0.1250\n",
      "[Batch 370/501] Loss: 1.6054, Batch Acc: 0.0000\n",
      "[Batch 380/501] Loss: 1.4193, Batch Acc: 0.3750\n",
      "[Batch 390/501] Loss: 1.5751, Batch Acc: 0.3750\n",
      "[Batch 400/501] Loss: 1.3131, Batch Acc: 0.5000\n",
      "[Batch 410/501] Loss: 1.8130, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.7955, Batch Acc: 0.2500\n",
      "[Batch 430/501] Loss: 1.5565, Batch Acc: 0.2500\n",
      "[Batch 440/501] Loss: 1.4733, Batch Acc: 0.3750\n",
      "[Batch 450/501] Loss: 2.0678, Batch Acc: 0.1250\n",
      "[Batch 460/501] Loss: 1.4445, Batch Acc: 0.5000\n",
      "[Batch 470/501] Loss: 1.9579, Batch Acc: 0.2500\n",
      "[Batch 480/501] Loss: 1.9943, Batch Acc: 0.0000\n",
      "[Batch 490/501] Loss: 2.0023, Batch Acc: 0.1250\n",
      "[Batch 500/501] Loss: 2.1475, Batch Acc: 0.1250\n",
      "Epoch 9 Summary - Loss: 866.4386, Train Accuracy: 0.2211\n",
      "[Batch 10/501] Loss: 1.7154, Batch Acc: 0.2500\n",
      "[Batch 20/501] Loss: 1.8849, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.8068, Batch Acc: 0.2500\n",
      "[Batch 40/501] Loss: 1.6492, Batch Acc: 0.2500\n",
      "[Batch 50/501] Loss: 1.9313, Batch Acc: 0.1250\n",
      "[Batch 60/501] Loss: 1.8494, Batch Acc: 0.1250\n",
      "[Batch 70/501] Loss: 1.5037, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.6865, Batch Acc: 0.1250\n",
      "[Batch 90/501] Loss: 1.7285, Batch Acc: 0.1250\n",
      "[Batch 100/501] Loss: 1.7974, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 1.5767, Batch Acc: 0.1250\n",
      "[Batch 120/501] Loss: 1.7481, Batch Acc: 0.2500\n",
      "[Batch 130/501] Loss: 1.6278, Batch Acc: 0.3750\n",
      "[Batch 140/501] Loss: 1.4150, Batch Acc: 0.3750\n",
      "[Batch 150/501] Loss: 1.6900, Batch Acc: 0.2500\n",
      "[Batch 160/501] Loss: 1.5621, Batch Acc: 0.3750\n",
      "[Batch 170/501] Loss: 1.4939, Batch Acc: 0.3750\n",
      "[Batch 180/501] Loss: 1.7515, Batch Acc: 0.1250\n",
      "[Batch 190/501] Loss: 1.5027, Batch Acc: 0.2500\n",
      "[Batch 200/501] Loss: 1.4705, Batch Acc: 0.2500\n",
      "[Batch 210/501] Loss: 1.6124, Batch Acc: 0.1250\n",
      "[Batch 220/501] Loss: 1.5451, Batch Acc: 0.5000\n",
      "[Batch 230/501] Loss: 1.6114, Batch Acc: 0.5000\n",
      "[Batch 240/501] Loss: 1.8002, Batch Acc: 0.2500\n",
      "[Batch 250/501] Loss: 1.7220, Batch Acc: 0.1250\n",
      "[Batch 260/501] Loss: 1.7462, Batch Acc: 0.0000\n",
      "[Batch 270/501] Loss: 1.5038, Batch Acc: 0.3750\n",
      "[Batch 280/501] Loss: 1.8939, Batch Acc: 0.0000\n",
      "[Batch 290/501] Loss: 2.0571, Batch Acc: 0.0000\n",
      "[Batch 300/501] Loss: 1.7853, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 1.4564, Batch Acc: 0.5000\n",
      "[Batch 320/501] Loss: 1.9109, Batch Acc: 0.1250\n",
      "[Batch 330/501] Loss: 1.8882, Batch Acc: 0.1250\n",
      "[Batch 340/501] Loss: 1.7763, Batch Acc: 0.2500\n",
      "[Batch 350/501] Loss: 1.5948, Batch Acc: 0.3750\n",
      "[Batch 360/501] Loss: 1.8165, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 1.7517, Batch Acc: 0.2500\n",
      "[Batch 380/501] Loss: 1.5839, Batch Acc: 0.2500\n",
      "[Batch 390/501] Loss: 1.8342, Batch Acc: 0.2500\n",
      "[Batch 400/501] Loss: 1.9028, Batch Acc: 0.1250\n",
      "[Batch 410/501] Loss: 1.9540, Batch Acc: 0.1250\n",
      "[Batch 420/501] Loss: 1.9252, Batch Acc: 0.1250\n",
      "[Batch 430/501] Loss: 1.6930, Batch Acc: 0.2500\n",
      "[Batch 440/501] Loss: 1.7213, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.9441, Batch Acc: 0.1250\n",
      "[Batch 460/501] Loss: 1.5788, Batch Acc: 0.5000\n",
      "[Batch 470/501] Loss: 1.7643, Batch Acc: 0.3750\n",
      "[Batch 480/501] Loss: 1.7431, Batch Acc: 0.3750\n",
      "[Batch 490/501] Loss: 1.5129, Batch Acc: 0.5000\n",
      "[Batch 500/501] Loss: 2.0556, Batch Acc: 0.2500\n",
      "Epoch 10 Summary - Loss: 862.8489, Train Accuracy: 0.2313\n",
      "Test Accuracy: 0.2265\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_accuracy</td><td>▆▇▁▃▄▂▃▅▃█</td></tr><tr><td>train_loss</td><td>▃▅▅█▆▇█▅▇▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>test_accuracy</td><td>0.22651</td></tr><tr><td>test_loss</td><td>216.93709</td></tr><tr><td>train_accuracy</td><td>0.23129</td></tr><tr><td>train_loss</td><td>862.84889</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">confused-sweep-2</strong> at: <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/wqb11ybm' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/wqb11ybm</a><br> View project at: <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250615_135438-wqb11ybm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ev4e6mk0 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_nodes: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ajit.kumar4@happiestminds.com/Documents/Drackula/ComputerVisionFromScratch/wandb/run-20250615_135807-ev4e6mk0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/ev4e6mk0' target=\"_blank\">fearless-sweep-3</a></strong> to <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/ev4e6mk0' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/ev4e6mk0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 10/501] Loss: 1.7277, Batch Acc: 0.2500\n",
      "[Batch 20/501] Loss: 1.6530, Batch Acc: 0.0000\n",
      "[Batch 30/501] Loss: 1.5584, Batch Acc: 0.2500\n",
      "[Batch 40/501] Loss: 1.4483, Batch Acc: 0.1250\n",
      "[Batch 50/501] Loss: 1.7063, Batch Acc: 0.1250\n",
      "[Batch 60/501] Loss: 1.8284, Batch Acc: 0.0000\n",
      "[Batch 70/501] Loss: 1.7197, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 2.0053, Batch Acc: 0.1250\n",
      "[Batch 90/501] Loss: 1.7958, Batch Acc: 0.0000\n",
      "[Batch 100/501] Loss: 1.8303, Batch Acc: 0.1250\n",
      "[Batch 110/501] Loss: 1.4650, Batch Acc: 0.3750\n",
      "[Batch 120/501] Loss: 1.7780, Batch Acc: 0.1250\n",
      "[Batch 130/501] Loss: 1.9043, Batch Acc: 0.1250\n",
      "[Batch 140/501] Loss: 1.6985, Batch Acc: 0.2500\n",
      "[Batch 150/501] Loss: 1.7990, Batch Acc: 0.2500\n",
      "[Batch 160/501] Loss: 1.5871, Batch Acc: 0.5000\n",
      "[Batch 170/501] Loss: 1.9533, Batch Acc: 0.0000\n",
      "[Batch 180/501] Loss: 1.6064, Batch Acc: 0.1250\n",
      "[Batch 190/501] Loss: 1.9658, Batch Acc: 0.0000\n",
      "[Batch 200/501] Loss: 1.5599, Batch Acc: 0.2500\n",
      "[Batch 210/501] Loss: 1.6074, Batch Acc: 0.2500\n",
      "[Batch 220/501] Loss: 1.9662, Batch Acc: 0.0000\n",
      "[Batch 230/501] Loss: 1.6775, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.3841, Batch Acc: 0.3750\n",
      "[Batch 250/501] Loss: 1.5153, Batch Acc: 0.5000\n",
      "[Batch 260/501] Loss: 1.5096, Batch Acc: 0.3750\n",
      "[Batch 270/501] Loss: 1.5652, Batch Acc: 0.2500\n",
      "[Batch 280/501] Loss: 1.8195, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 1.7832, Batch Acc: 0.1250\n",
      "[Batch 300/501] Loss: 1.4577, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 1.6087, Batch Acc: 0.1250\n",
      "[Batch 320/501] Loss: 1.8728, Batch Acc: 0.1250\n",
      "[Batch 330/501] Loss: 1.5872, Batch Acc: 0.1250\n",
      "[Batch 340/501] Loss: 1.5560, Batch Acc: 0.1250\n",
      "[Batch 350/501] Loss: 1.7033, Batch Acc: 0.1250\n",
      "[Batch 360/501] Loss: 1.7659, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 1.6613, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.4996, Batch Acc: 0.3750\n",
      "[Batch 390/501] Loss: 1.6252, Batch Acc: 0.2500\n",
      "[Batch 400/501] Loss: 1.7457, Batch Acc: 0.2500\n",
      "[Batch 410/501] Loss: 1.6875, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.5700, Batch Acc: 0.1250\n",
      "[Batch 430/501] Loss: 1.5365, Batch Acc: 0.1250\n",
      "[Batch 440/501] Loss: 1.4617, Batch Acc: 0.1250\n",
      "[Batch 450/501] Loss: 1.7714, Batch Acc: 0.0000\n",
      "[Batch 460/501] Loss: 1.7451, Batch Acc: 0.1250\n",
      "[Batch 470/501] Loss: 1.7428, Batch Acc: 0.2500\n",
      "[Batch 480/501] Loss: 1.6226, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.6805, Batch Acc: 0.2500\n",
      "[Batch 500/501] Loss: 1.6448, Batch Acc: 0.2500\n",
      "Epoch 1 Summary - Loss: 834.3962, Train Accuracy: 0.2198\n",
      "[Batch 10/501] Loss: 1.5087, Batch Acc: 0.1250\n",
      "[Batch 20/501] Loss: 1.7036, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.5076, Batch Acc: 0.1250\n",
      "[Batch 40/501] Loss: 1.6656, Batch Acc: 0.1250\n",
      "[Batch 50/501] Loss: 1.5997, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.6550, Batch Acc: 0.2500\n",
      "[Batch 70/501] Loss: 1.3820, Batch Acc: 0.3750\n",
      "[Batch 80/501] Loss: 1.5157, Batch Acc: 0.5000\n",
      "[Batch 90/501] Loss: 1.5585, Batch Acc: 0.2500\n",
      "[Batch 100/501] Loss: 1.7386, Batch Acc: 0.1250\n",
      "[Batch 110/501] Loss: 1.7374, Batch Acc: 0.2500\n",
      "[Batch 120/501] Loss: 1.7451, Batch Acc: 0.2500\n",
      "[Batch 130/501] Loss: 1.6685, Batch Acc: 0.1250\n",
      "[Batch 140/501] Loss: 1.9785, Batch Acc: 0.1250\n",
      "[Batch 150/501] Loss: 1.5822, Batch Acc: 0.2500\n",
      "[Batch 160/501] Loss: 1.5914, Batch Acc: 0.2500\n",
      "[Batch 170/501] Loss: 1.6348, Batch Acc: 0.1250\n",
      "[Batch 180/501] Loss: 1.5851, Batch Acc: 0.3750\n",
      "[Batch 190/501] Loss: 1.6142, Batch Acc: 0.3750\n",
      "[Batch 200/501] Loss: 1.6226, Batch Acc: 0.2500\n",
      "[Batch 210/501] Loss: 1.7295, Batch Acc: 0.2500\n",
      "[Batch 220/501] Loss: 1.8978, Batch Acc: 0.1250\n",
      "[Batch 230/501] Loss: 1.7867, Batch Acc: 0.0000\n",
      "[Batch 240/501] Loss: 1.5640, Batch Acc: 0.5000\n",
      "[Batch 250/501] Loss: 1.7406, Batch Acc: 0.0000\n",
      "[Batch 260/501] Loss: 1.5174, Batch Acc: 0.1250\n",
      "[Batch 270/501] Loss: 1.7675, Batch Acc: 0.0000\n",
      "[Batch 280/501] Loss: 1.6636, Batch Acc: 0.2500\n",
      "[Batch 290/501] Loss: 1.6198, Batch Acc: 0.2500\n",
      "[Batch 300/501] Loss: 1.6526, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 1.8128, Batch Acc: 0.0000\n",
      "[Batch 320/501] Loss: 1.8085, Batch Acc: 0.1250\n",
      "[Batch 330/501] Loss: 1.4087, Batch Acc: 0.3750\n",
      "[Batch 340/501] Loss: 1.5538, Batch Acc: 0.2500\n",
      "[Batch 350/501] Loss: 1.5134, Batch Acc: 0.5000\n",
      "[Batch 360/501] Loss: 1.5408, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 1.8126, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.5681, Batch Acc: 0.2500\n",
      "[Batch 390/501] Loss: 1.5695, Batch Acc: 0.3750\n",
      "[Batch 400/501] Loss: 1.6153, Batch Acc: 0.2500\n",
      "[Batch 410/501] Loss: 1.6427, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.6750, Batch Acc: 0.3750\n",
      "[Batch 430/501] Loss: 1.3342, Batch Acc: 0.5000\n",
      "[Batch 440/501] Loss: 1.8982, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.6716, Batch Acc: 0.0000\n",
      "[Batch 460/501] Loss: 1.6479, Batch Acc: 0.1250\n",
      "[Batch 470/501] Loss: 1.7244, Batch Acc: 0.2500\n",
      "[Batch 480/501] Loss: 1.5278, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.6218, Batch Acc: 0.1250\n",
      "[Batch 500/501] Loss: 1.6758, Batch Acc: 0.1250\n",
      "Epoch 2 Summary - Loss: 830.8026, Train Accuracy: 0.2236\n",
      "[Batch 10/501] Loss: 1.8274, Batch Acc: 0.2500\n",
      "[Batch 20/501] Loss: 1.5578, Batch Acc: 0.3750\n",
      "[Batch 30/501] Loss: 1.7689, Batch Acc: 0.2500\n",
      "[Batch 40/501] Loss: 1.8294, Batch Acc: 0.2500\n",
      "[Batch 50/501] Loss: 1.7526, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.5047, Batch Acc: 0.3750\n",
      "[Batch 70/501] Loss: 1.8130, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.7827, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.3190, Batch Acc: 0.5000\n",
      "[Batch 100/501] Loss: 1.5955, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 1.5640, Batch Acc: 0.2500\n",
      "[Batch 120/501] Loss: 1.6273, Batch Acc: 0.1250\n",
      "[Batch 130/501] Loss: 1.5937, Batch Acc: 0.5000\n",
      "[Batch 140/501] Loss: 1.4684, Batch Acc: 0.2500\n",
      "[Batch 150/501] Loss: 1.4745, Batch Acc: 0.3750\n",
      "[Batch 160/501] Loss: 1.8261, Batch Acc: 0.1250\n",
      "[Batch 170/501] Loss: 1.5208, Batch Acc: 0.3750\n",
      "[Batch 180/501] Loss: 1.3723, Batch Acc: 0.3750\n",
      "[Batch 190/501] Loss: 1.3953, Batch Acc: 0.2500\n",
      "[Batch 200/501] Loss: 1.4437, Batch Acc: 0.2500\n",
      "[Batch 210/501] Loss: 1.5988, Batch Acc: 0.2500\n",
      "[Batch 220/501] Loss: 1.8791, Batch Acc: 0.1250\n",
      "[Batch 230/501] Loss: 1.7583, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.5936, Batch Acc: 0.2500\n",
      "[Batch 250/501] Loss: 1.6509, Batch Acc: 0.1250\n",
      "[Batch 260/501] Loss: 1.8049, Batch Acc: 0.1250\n",
      "[Batch 270/501] Loss: 1.7561, Batch Acc: 0.2500\n",
      "[Batch 280/501] Loss: 1.5047, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 1.7042, Batch Acc: 0.1250\n",
      "[Batch 300/501] Loss: 1.4430, Batch Acc: 0.1250\n",
      "[Batch 310/501] Loss: 1.5207, Batch Acc: 0.1250\n",
      "[Batch 320/501] Loss: 1.5458, Batch Acc: 0.3750\n",
      "[Batch 330/501] Loss: 1.8504, Batch Acc: 0.1250\n",
      "[Batch 340/501] Loss: 1.6065, Batch Acc: 0.1250\n",
      "[Batch 350/501] Loss: 1.6324, Batch Acc: 0.5000\n",
      "[Batch 360/501] Loss: 1.5793, Batch Acc: 0.3750\n",
      "[Batch 370/501] Loss: 1.5857, Batch Acc: 0.2500\n",
      "[Batch 380/501] Loss: 1.6907, Batch Acc: 0.2500\n",
      "[Batch 390/501] Loss: 1.4219, Batch Acc: 0.3750\n",
      "[Batch 400/501] Loss: 1.8158, Batch Acc: 0.1250\n",
      "[Batch 410/501] Loss: 1.5104, Batch Acc: 0.3750\n",
      "[Batch 420/501] Loss: 1.9044, Batch Acc: 0.2500\n",
      "[Batch 430/501] Loss: 1.6920, Batch Acc: 0.1250\n",
      "[Batch 440/501] Loss: 1.6983, Batch Acc: 0.1250\n",
      "[Batch 450/501] Loss: 1.4119, Batch Acc: 0.3750\n",
      "[Batch 460/501] Loss: 1.8248, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.7516, Batch Acc: 0.1250\n",
      "[Batch 480/501] Loss: 1.8345, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.8849, Batch Acc: 0.0000\n",
      "[Batch 500/501] Loss: 1.7491, Batch Acc: 0.0000\n",
      "Epoch 3 Summary - Loss: 832.1087, Train Accuracy: 0.2275\n",
      "[Batch 10/501] Loss: 1.5105, Batch Acc: 0.3750\n",
      "[Batch 20/501] Loss: 1.6690, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.5766, Batch Acc: 0.3750\n",
      "[Batch 40/501] Loss: 1.8169, Batch Acc: 0.1250\n",
      "[Batch 50/501] Loss: 1.5348, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.5878, Batch Acc: 0.2500\n",
      "[Batch 70/501] Loss: 1.8695, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.9862, Batch Acc: 0.0000\n",
      "[Batch 90/501] Loss: 1.6311, Batch Acc: 0.2500\n",
      "[Batch 100/501] Loss: 1.6836, Batch Acc: 0.1250\n",
      "[Batch 110/501] Loss: 1.6350, Batch Acc: 0.2500\n",
      "[Batch 120/501] Loss: 1.6976, Batch Acc: 0.1250\n",
      "[Batch 130/501] Loss: 1.8528, Batch Acc: 0.0000\n",
      "[Batch 140/501] Loss: 1.4804, Batch Acc: 0.5000\n",
      "[Batch 150/501] Loss: 1.5381, Batch Acc: 0.3750\n",
      "[Batch 160/501] Loss: 1.5508, Batch Acc: 0.1250\n",
      "[Batch 170/501] Loss: 1.3010, Batch Acc: 0.3750\n",
      "[Batch 180/501] Loss: 1.6939, Batch Acc: 0.1250\n",
      "[Batch 190/501] Loss: 1.7100, Batch Acc: 0.1250\n",
      "[Batch 200/501] Loss: 1.5583, Batch Acc: 0.3750\n",
      "[Batch 210/501] Loss: 1.7598, Batch Acc: 0.1250\n",
      "[Batch 220/501] Loss: 1.5014, Batch Acc: 0.0000\n",
      "[Batch 230/501] Loss: 1.5468, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.6579, Batch Acc: 0.0000\n",
      "[Batch 250/501] Loss: 1.7393, Batch Acc: 0.1250\n",
      "[Batch 260/501] Loss: 1.4864, Batch Acc: 0.5000\n",
      "[Batch 270/501] Loss: 1.4223, Batch Acc: 0.6250\n",
      "[Batch 280/501] Loss: 1.9050, Batch Acc: 0.0000\n",
      "[Batch 290/501] Loss: 1.8918, Batch Acc: 0.3750\n",
      "[Batch 300/501] Loss: 1.9716, Batch Acc: 0.1250\n",
      "[Batch 310/501] Loss: 1.7227, Batch Acc: 0.2500\n",
      "[Batch 320/501] Loss: 1.6400, Batch Acc: 0.3750\n",
      "[Batch 330/501] Loss: 1.6305, Batch Acc: 0.1250\n",
      "[Batch 340/501] Loss: 1.4758, Batch Acc: 0.1250\n",
      "[Batch 350/501] Loss: 1.8598, Batch Acc: 0.1250\n",
      "[Batch 360/501] Loss: 1.7366, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 1.7153, Batch Acc: 0.2500\n",
      "[Batch 380/501] Loss: 1.9590, Batch Acc: 0.0000\n",
      "[Batch 390/501] Loss: 1.6045, Batch Acc: 0.3750\n",
      "[Batch 400/501] Loss: 1.7332, Batch Acc: 0.1250\n",
      "[Batch 410/501] Loss: 1.7669, Batch Acc: 0.0000\n",
      "[Batch 420/501] Loss: 1.4952, Batch Acc: 0.2500\n",
      "[Batch 430/501] Loss: 1.6103, Batch Acc: 0.3750\n",
      "[Batch 440/501] Loss: 1.8403, Batch Acc: 0.1250\n",
      "[Batch 450/501] Loss: 1.7907, Batch Acc: 0.0000\n",
      "[Batch 460/501] Loss: 1.6709, Batch Acc: 0.1250\n",
      "[Batch 470/501] Loss: 1.4836, Batch Acc: 0.1250\n",
      "[Batch 480/501] Loss: 1.8711, Batch Acc: 0.0000\n",
      "[Batch 490/501] Loss: 1.5077, Batch Acc: 0.2500\n",
      "[Batch 500/501] Loss: 1.9048, Batch Acc: 0.0000\n",
      "Epoch 4 Summary - Loss: 831.8979, Train Accuracy: 0.2238\n",
      "[Batch 10/501] Loss: 1.6682, Batch Acc: 0.0000\n",
      "[Batch 20/501] Loss: 1.7724, Batch Acc: 0.1250\n",
      "[Batch 30/501] Loss: 1.6131, Batch Acc: 0.1250\n",
      "[Batch 40/501] Loss: 1.6646, Batch Acc: 0.0000\n",
      "[Batch 50/501] Loss: 1.6451, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.5554, Batch Acc: 0.2500\n",
      "[Batch 70/501] Loss: 1.7664, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.5434, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.8470, Batch Acc: 0.2500\n",
      "[Batch 100/501] Loss: 1.7114, Batch Acc: 0.0000\n",
      "[Batch 110/501] Loss: 1.4512, Batch Acc: 0.3750\n",
      "[Batch 120/501] Loss: 1.3824, Batch Acc: 0.2500\n",
      "[Batch 130/501] Loss: 1.7655, Batch Acc: 0.3750\n",
      "[Batch 140/501] Loss: 1.5440, Batch Acc: 0.3750\n",
      "[Batch 150/501] Loss: 1.4451, Batch Acc: 0.5000\n",
      "[Batch 160/501] Loss: 1.6545, Batch Acc: 0.2500\n",
      "[Batch 170/501] Loss: 1.7327, Batch Acc: 0.3750\n",
      "[Batch 180/501] Loss: 1.4948, Batch Acc: 0.2500\n",
      "[Batch 190/501] Loss: 1.8779, Batch Acc: 0.1250\n",
      "[Batch 200/501] Loss: 1.9484, Batch Acc: 0.1250\n",
      "[Batch 210/501] Loss: 1.5459, Batch Acc: 0.1250\n",
      "[Batch 220/501] Loss: 1.5491, Batch Acc: 0.3750\n",
      "[Batch 230/501] Loss: 1.8872, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.8637, Batch Acc: 0.1250\n",
      "[Batch 250/501] Loss: 1.7839, Batch Acc: 0.3750\n",
      "[Batch 260/501] Loss: 1.7818, Batch Acc: 0.1250\n",
      "[Batch 270/501] Loss: 1.7408, Batch Acc: 0.1250\n",
      "[Batch 280/501] Loss: 1.9750, Batch Acc: 0.0000\n",
      "[Batch 290/501] Loss: 1.4696, Batch Acc: 0.2500\n",
      "[Batch 300/501] Loss: 1.6673, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 1.4868, Batch Acc: 0.2500\n",
      "[Batch 320/501] Loss: 1.5232, Batch Acc: 0.0000\n",
      "[Batch 330/501] Loss: 1.6501, Batch Acc: 0.2500\n",
      "[Batch 340/501] Loss: 1.7297, Batch Acc: 0.1250\n",
      "[Batch 350/501] Loss: 1.8324, Batch Acc: 0.2500\n",
      "[Batch 360/501] Loss: 1.6938, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 1.2474, Batch Acc: 0.6250\n",
      "[Batch 380/501] Loss: 2.0170, Batch Acc: 0.1250\n",
      "[Batch 390/501] Loss: 1.3159, Batch Acc: 0.3750\n",
      "[Batch 400/501] Loss: 1.8860, Batch Acc: 0.1250\n",
      "[Batch 410/501] Loss: 1.5657, Batch Acc: 0.1250\n",
      "[Batch 420/501] Loss: 1.5026, Batch Acc: 0.3750\n",
      "[Batch 430/501] Loss: 1.5995, Batch Acc: 0.1250\n",
      "[Batch 440/501] Loss: 1.6539, Batch Acc: 0.1250\n",
      "[Batch 450/501] Loss: 1.3216, Batch Acc: 0.5000\n",
      "[Batch 460/501] Loss: 1.7467, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 2.0044, Batch Acc: 0.1250\n",
      "[Batch 480/501] Loss: 1.5100, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.5250, Batch Acc: 0.3750\n",
      "[Batch 500/501] Loss: 1.6127, Batch Acc: 0.2500\n",
      "Epoch 5 Summary - Loss: 833.9178, Train Accuracy: 0.2186\n",
      "[Batch 10/501] Loss: 1.6929, Batch Acc: 0.2500\n",
      "[Batch 20/501] Loss: 1.9045, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.7393, Batch Acc: 0.0000\n",
      "[Batch 40/501] Loss: 1.8146, Batch Acc: 0.3750\n",
      "[Batch 50/501] Loss: 1.5235, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.6279, Batch Acc: 0.2500\n",
      "[Batch 70/501] Loss: 1.5738, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.8102, Batch Acc: 0.1250\n",
      "[Batch 90/501] Loss: 1.6508, Batch Acc: 0.2500\n",
      "[Batch 100/501] Loss: 1.5534, Batch Acc: 0.3750\n",
      "[Batch 110/501] Loss: 1.6199, Batch Acc: 0.1250\n",
      "[Batch 120/501] Loss: 1.4744, Batch Acc: 0.3750\n",
      "[Batch 130/501] Loss: 1.5329, Batch Acc: 0.1250\n",
      "[Batch 140/501] Loss: 1.5031, Batch Acc: 0.3750\n",
      "[Batch 150/501] Loss: 1.8651, Batch Acc: 0.1250\n",
      "[Batch 160/501] Loss: 1.6875, Batch Acc: 0.1250\n",
      "[Batch 170/501] Loss: 1.7765, Batch Acc: 0.2500\n",
      "[Batch 180/501] Loss: 1.7143, Batch Acc: 0.1250\n",
      "[Batch 190/501] Loss: 1.3581, Batch Acc: 0.5000\n",
      "[Batch 200/501] Loss: 1.7691, Batch Acc: 0.0000\n",
      "[Batch 210/501] Loss: 1.6786, Batch Acc: 0.1250\n",
      "[Batch 220/501] Loss: 1.6379, Batch Acc: 0.2500\n",
      "[Batch 230/501] Loss: 2.0239, Batch Acc: 0.1250\n",
      "[Batch 240/501] Loss: 1.5358, Batch Acc: 0.2500\n",
      "[Batch 250/501] Loss: 1.5087, Batch Acc: 0.2500\n",
      "[Batch 260/501] Loss: 1.5231, Batch Acc: 0.2500\n",
      "[Batch 270/501] Loss: 1.5146, Batch Acc: 0.3750\n",
      "[Batch 280/501] Loss: 1.6117, Batch Acc: 0.2500\n",
      "[Batch 290/501] Loss: 1.6522, Batch Acc: 0.2500\n",
      "[Batch 300/501] Loss: 1.7989, Batch Acc: 0.1250\n",
      "[Batch 310/501] Loss: 1.7595, Batch Acc: 0.2500\n",
      "[Batch 320/501] Loss: 1.8808, Batch Acc: 0.1250\n",
      "[Batch 330/501] Loss: 1.6636, Batch Acc: 0.1250\n",
      "[Batch 340/501] Loss: 1.7863, Batch Acc: 0.2500\n",
      "[Batch 350/501] Loss: 1.7847, Batch Acc: 0.0000\n",
      "[Batch 360/501] Loss: 1.8070, Batch Acc: 0.0000\n",
      "[Batch 370/501] Loss: 1.7691, Batch Acc: 0.2500\n",
      "[Batch 380/501] Loss: 1.5122, Batch Acc: 0.3750\n",
      "[Batch 390/501] Loss: 1.9419, Batch Acc: 0.0000\n",
      "[Batch 400/501] Loss: 2.0145, Batch Acc: 0.1250\n",
      "[Batch 410/501] Loss: 1.6966, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.9090, Batch Acc: 0.1250\n",
      "[Batch 430/501] Loss: 1.5197, Batch Acc: 0.1250\n",
      "[Batch 440/501] Loss: 1.5658, Batch Acc: 0.1250\n",
      "[Batch 450/501] Loss: 1.6166, Batch Acc: 0.3750\n",
      "[Batch 460/501] Loss: 1.6780, Batch Acc: 0.0000\n",
      "[Batch 470/501] Loss: 1.5450, Batch Acc: 0.1250\n",
      "[Batch 480/501] Loss: 1.3925, Batch Acc: 0.3750\n",
      "[Batch 490/501] Loss: 1.8232, Batch Acc: 0.2500\n",
      "[Batch 500/501] Loss: 1.8475, Batch Acc: 0.0000\n",
      "Epoch 6 Summary - Loss: 829.0603, Train Accuracy: 0.2193\n",
      "[Batch 10/501] Loss: 1.6671, Batch Acc: 0.1250\n",
      "[Batch 20/501] Loss: 1.3566, Batch Acc: 0.3750\n",
      "[Batch 30/501] Loss: 1.7121, Batch Acc: 0.2500\n",
      "[Batch 40/501] Loss: 1.6723, Batch Acc: 0.1250\n",
      "[Batch 50/501] Loss: 1.4196, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.5323, Batch Acc: 0.1250\n",
      "[Batch 70/501] Loss: 1.8992, Batch Acc: 0.0000\n",
      "[Batch 80/501] Loss: 1.9147, Batch Acc: 0.1250\n",
      "[Batch 90/501] Loss: 1.7340, Batch Acc: 0.0000\n",
      "[Batch 100/501] Loss: 1.7017, Batch Acc: 0.1250\n",
      "[Batch 110/501] Loss: 1.5146, Batch Acc: 0.2500\n",
      "[Batch 120/501] Loss: 1.7190, Batch Acc: 0.2500\n",
      "[Batch 130/501] Loss: 1.8749, Batch Acc: 0.2500\n",
      "[Batch 140/501] Loss: 1.7305, Batch Acc: 0.1250\n",
      "[Batch 150/501] Loss: 1.6236, Batch Acc: 0.0000\n",
      "[Batch 160/501] Loss: 1.4613, Batch Acc: 0.2500\n",
      "[Batch 170/501] Loss: 1.6678, Batch Acc: 0.0000\n",
      "[Batch 180/501] Loss: 1.5499, Batch Acc: 0.1250\n",
      "[Batch 190/501] Loss: 1.6555, Batch Acc: 0.1250\n",
      "[Batch 200/501] Loss: 1.6012, Batch Acc: 0.2500\n",
      "[Batch 210/501] Loss: 1.5900, Batch Acc: 0.1250\n",
      "[Batch 220/501] Loss: 1.6501, Batch Acc: 0.3750\n",
      "[Batch 230/501] Loss: 1.6367, Batch Acc: 0.1250\n",
      "[Batch 240/501] Loss: 1.9232, Batch Acc: 0.1250\n",
      "[Batch 250/501] Loss: 1.5532, Batch Acc: 0.2500\n",
      "[Batch 260/501] Loss: 1.6034, Batch Acc: 0.3750\n",
      "[Batch 270/501] Loss: 1.7253, Batch Acc: 0.2500\n",
      "[Batch 280/501] Loss: 1.8180, Batch Acc: 0.2500\n",
      "[Batch 290/501] Loss: 1.3661, Batch Acc: 0.3750\n",
      "[Batch 300/501] Loss: 1.4921, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 2.0159, Batch Acc: 0.0000\n",
      "[Batch 320/501] Loss: 1.6423, Batch Acc: 0.0000\n",
      "[Batch 330/501] Loss: 1.6990, Batch Acc: 0.0000\n",
      "[Batch 340/501] Loss: 1.8749, Batch Acc: 0.1250\n",
      "[Batch 350/501] Loss: 1.6392, Batch Acc: 0.2500\n",
      "[Batch 360/501] Loss: 1.6728, Batch Acc: 0.3750\n",
      "[Batch 370/501] Loss: 1.4752, Batch Acc: 0.2500\n",
      "[Batch 380/501] Loss: 1.4255, Batch Acc: 0.7500\n",
      "[Batch 390/501] Loss: 1.6978, Batch Acc: 0.0000\n",
      "[Batch 400/501] Loss: 1.5977, Batch Acc: 0.2500\n",
      "[Batch 410/501] Loss: 1.8545, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.4360, Batch Acc: 0.1250\n",
      "[Batch 430/501] Loss: 1.6388, Batch Acc: 0.3750\n",
      "[Batch 440/501] Loss: 1.5581, Batch Acc: 0.1250\n",
      "[Batch 450/501] Loss: 2.2092, Batch Acc: 0.0000\n",
      "[Batch 460/501] Loss: 1.5419, Batch Acc: 0.3750\n",
      "[Batch 470/501] Loss: 1.3413, Batch Acc: 0.5000\n",
      "[Batch 480/501] Loss: 1.4770, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 2.1407, Batch Acc: 0.2500\n",
      "[Batch 500/501] Loss: 1.4796, Batch Acc: 0.2500\n",
      "Epoch 7 Summary - Loss: 831.9933, Train Accuracy: 0.2163\n",
      "[Batch 10/501] Loss: 1.6231, Batch Acc: 0.2500\n",
      "[Batch 20/501] Loss: 1.3453, Batch Acc: 0.5000\n",
      "[Batch 30/501] Loss: 1.7706, Batch Acc: 0.2500\n",
      "[Batch 40/501] Loss: 1.5504, Batch Acc: 0.3750\n",
      "[Batch 50/501] Loss: 1.7147, Batch Acc: 0.1250\n",
      "[Batch 60/501] Loss: 1.7535, Batch Acc: 0.1250\n",
      "[Batch 70/501] Loss: 1.6516, Batch Acc: 0.0000\n",
      "[Batch 80/501] Loss: 1.7949, Batch Acc: 0.1250\n",
      "[Batch 90/501] Loss: 1.6852, Batch Acc: 0.1250\n",
      "[Batch 100/501] Loss: 1.8396, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 1.6817, Batch Acc: 0.1250\n",
      "[Batch 120/501] Loss: 1.5050, Batch Acc: 0.2500\n",
      "[Batch 130/501] Loss: 1.2269, Batch Acc: 0.7500\n",
      "[Batch 140/501] Loss: 1.4923, Batch Acc: 0.2500\n",
      "[Batch 150/501] Loss: 1.8172, Batch Acc: 0.1250\n",
      "[Batch 160/501] Loss: 1.5338, Batch Acc: 0.3750\n",
      "[Batch 170/501] Loss: 1.6651, Batch Acc: 0.2500\n",
      "[Batch 180/501] Loss: 1.3440, Batch Acc: 0.3750\n",
      "[Batch 190/501] Loss: 1.6365, Batch Acc: 0.0000\n",
      "[Batch 200/501] Loss: 1.5887, Batch Acc: 0.1250\n",
      "[Batch 210/501] Loss: 1.6922, Batch Acc: 0.2500\n",
      "[Batch 220/501] Loss: 1.5067, Batch Acc: 0.5000\n",
      "[Batch 230/501] Loss: 1.5340, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.6917, Batch Acc: 0.2500\n",
      "[Batch 250/501] Loss: 1.5161, Batch Acc: 0.1250\n",
      "[Batch 260/501] Loss: 1.6553, Batch Acc: 0.2500\n",
      "[Batch 270/501] Loss: 1.4978, Batch Acc: 0.3750\n",
      "[Batch 280/501] Loss: 1.4679, Batch Acc: 0.5000\n",
      "[Batch 290/501] Loss: 1.8681, Batch Acc: 0.1250\n",
      "[Batch 300/501] Loss: 1.7024, Batch Acc: 0.1250\n",
      "[Batch 310/501] Loss: 1.5626, Batch Acc: 0.2500\n",
      "[Batch 320/501] Loss: 1.5872, Batch Acc: 0.3750\n",
      "[Batch 330/501] Loss: 1.4693, Batch Acc: 0.5000\n",
      "[Batch 340/501] Loss: 1.6103, Batch Acc: 0.1250\n",
      "[Batch 350/501] Loss: 1.3981, Batch Acc: 0.5000\n",
      "[Batch 360/501] Loss: 1.8837, Batch Acc: 0.1250\n",
      "[Batch 370/501] Loss: 1.5013, Batch Acc: 0.2500\n",
      "[Batch 380/501] Loss: 1.3940, Batch Acc: 0.3750\n",
      "[Batch 390/501] Loss: 1.4551, Batch Acc: 0.2500\n",
      "[Batch 400/501] Loss: 1.6847, Batch Acc: 0.3750\n",
      "[Batch 410/501] Loss: 1.4155, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.7253, Batch Acc: 0.2500\n",
      "[Batch 430/501] Loss: 1.3861, Batch Acc: 0.3750\n",
      "[Batch 440/501] Loss: 1.6849, Batch Acc: 0.1250\n",
      "[Batch 450/501] Loss: 1.6962, Batch Acc: 0.1250\n",
      "[Batch 460/501] Loss: 1.9284, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.5492, Batch Acc: 0.2500\n",
      "[Batch 480/501] Loss: 1.8388, Batch Acc: 0.1250\n",
      "[Batch 490/501] Loss: 2.0132, Batch Acc: 0.0000\n",
      "[Batch 500/501] Loss: 1.4719, Batch Acc: 0.1250\n",
      "Epoch 8 Summary - Loss: 826.5425, Train Accuracy: 0.2236\n",
      "[Batch 10/501] Loss: 1.7010, Batch Acc: 0.2500\n",
      "[Batch 20/501] Loss: 1.6091, Batch Acc: 0.3750\n",
      "[Batch 30/501] Loss: 1.5577, Batch Acc: 0.2500\n",
      "[Batch 40/501] Loss: 1.8728, Batch Acc: 0.1250\n",
      "[Batch 50/501] Loss: 1.7893, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.6234, Batch Acc: 0.2500\n",
      "[Batch 70/501] Loss: 1.4583, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.6893, Batch Acc: 0.1250\n",
      "[Batch 90/501] Loss: 1.3920, Batch Acc: 0.2500\n",
      "[Batch 100/501] Loss: 1.7204, Batch Acc: 0.1250\n",
      "[Batch 110/501] Loss: 1.3961, Batch Acc: 0.2500\n",
      "[Batch 120/501] Loss: 1.5367, Batch Acc: 0.1250\n",
      "[Batch 130/501] Loss: 1.7518, Batch Acc: 0.1250\n",
      "[Batch 140/501] Loss: 1.2464, Batch Acc: 0.3750\n",
      "[Batch 150/501] Loss: 1.6206, Batch Acc: 0.1250\n",
      "[Batch 160/501] Loss: 1.7429, Batch Acc: 0.3750\n",
      "[Batch 170/501] Loss: 1.7309, Batch Acc: 0.0000\n",
      "[Batch 180/501] Loss: 1.3842, Batch Acc: 0.3750\n",
      "[Batch 190/501] Loss: 1.5321, Batch Acc: 0.1250\n",
      "[Batch 200/501] Loss: 1.8313, Batch Acc: 0.0000\n",
      "[Batch 210/501] Loss: 1.5467, Batch Acc: 0.3750\n",
      "[Batch 220/501] Loss: 1.6602, Batch Acc: 0.2500\n",
      "[Batch 230/501] Loss: 1.6432, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.8914, Batch Acc: 0.3750\n",
      "[Batch 250/501] Loss: 1.5051, Batch Acc: 0.3750\n",
      "[Batch 260/501] Loss: 1.5289, Batch Acc: 0.3750\n",
      "[Batch 270/501] Loss: 1.6176, Batch Acc: 0.3750\n",
      "[Batch 280/501] Loss: 1.9543, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 1.9049, Batch Acc: 0.0000\n",
      "[Batch 300/501] Loss: 1.7062, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 1.7227, Batch Acc: 0.1250\n",
      "[Batch 320/501] Loss: 1.5384, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 1.4560, Batch Acc: 0.1250\n",
      "[Batch 340/501] Loss: 1.5383, Batch Acc: 0.2500\n",
      "[Batch 350/501] Loss: 1.9780, Batch Acc: 0.2500\n",
      "[Batch 360/501] Loss: 1.9401, Batch Acc: 0.0000\n",
      "[Batch 370/501] Loss: 1.4682, Batch Acc: 0.3750\n",
      "[Batch 380/501] Loss: 1.5495, Batch Acc: 0.0000\n",
      "[Batch 390/501] Loss: 1.7248, Batch Acc: 0.2500\n",
      "[Batch 400/501] Loss: 1.4136, Batch Acc: 0.5000\n",
      "[Batch 410/501] Loss: 1.8377, Batch Acc: 0.0000\n",
      "[Batch 420/501] Loss: 1.7163, Batch Acc: 0.3750\n",
      "[Batch 430/501] Loss: 1.6244, Batch Acc: 0.2500\n",
      "[Batch 440/501] Loss: 1.7412, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.2274, Batch Acc: 0.3750\n",
      "[Batch 460/501] Loss: 1.5722, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.7279, Batch Acc: 0.1250\n",
      "[Batch 480/501] Loss: 1.6976, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.4056, Batch Acc: 0.3750\n",
      "[Batch 500/501] Loss: 1.4464, Batch Acc: 0.3750\n",
      "Epoch 9 Summary - Loss: 829.1985, Train Accuracy: 0.2255\n",
      "[Batch 10/501] Loss: 1.9068, Batch Acc: 0.1250\n",
      "[Batch 20/501] Loss: 1.6839, Batch Acc: 0.3750\n",
      "[Batch 30/501] Loss: 1.5733, Batch Acc: 0.1250\n",
      "[Batch 40/501] Loss: 1.5341, Batch Acc: 0.2500\n",
      "[Batch 50/501] Loss: 1.5541, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.5145, Batch Acc: 0.3750\n",
      "[Batch 70/501] Loss: 1.9476, Batch Acc: 0.0000\n",
      "[Batch 80/501] Loss: 1.7018, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.7741, Batch Acc: 0.0000\n",
      "[Batch 100/501] Loss: 1.4920, Batch Acc: 0.3750\n",
      "[Batch 110/501] Loss: 1.5021, Batch Acc: 0.3750\n",
      "[Batch 120/501] Loss: 1.4302, Batch Acc: 0.3750\n",
      "[Batch 130/501] Loss: 1.6507, Batch Acc: 0.3750\n",
      "[Batch 140/501] Loss: 1.4252, Batch Acc: 0.1250\n",
      "[Batch 150/501] Loss: 1.8713, Batch Acc: 0.1250\n",
      "[Batch 160/501] Loss: 1.7703, Batch Acc: 0.1250\n",
      "[Batch 170/501] Loss: 1.4608, Batch Acc: 0.3750\n",
      "[Batch 180/501] Loss: 1.5269, Batch Acc: 0.2500\n",
      "[Batch 190/501] Loss: 1.7870, Batch Acc: 0.0000\n",
      "[Batch 200/501] Loss: 1.5889, Batch Acc: 0.3750\n",
      "[Batch 210/501] Loss: 1.7396, Batch Acc: 0.2500\n",
      "[Batch 220/501] Loss: 1.5331, Batch Acc: 0.1250\n",
      "[Batch 230/501] Loss: 1.5795, Batch Acc: 0.3750\n",
      "[Batch 240/501] Loss: 1.7863, Batch Acc: 0.1250\n",
      "[Batch 250/501] Loss: 1.5516, Batch Acc: 0.2500\n",
      "[Batch 260/501] Loss: 1.5393, Batch Acc: 0.1250\n",
      "[Batch 270/501] Loss: 1.4321, Batch Acc: 0.6250\n",
      "[Batch 280/501] Loss: 1.8508, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 1.6676, Batch Acc: 0.5000\n",
      "[Batch 300/501] Loss: 1.5639, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 1.5463, Batch Acc: 0.2500\n",
      "[Batch 320/501] Loss: 1.4910, Batch Acc: 0.1250\n",
      "[Batch 330/501] Loss: 1.7244, Batch Acc: 0.1250\n",
      "[Batch 340/501] Loss: 1.5305, Batch Acc: 0.3750\n",
      "[Batch 350/501] Loss: 1.7675, Batch Acc: 0.1250\n",
      "[Batch 360/501] Loss: 1.7246, Batch Acc: 0.1250\n",
      "[Batch 370/501] Loss: 1.3446, Batch Acc: 0.5000\n",
      "[Batch 380/501] Loss: 1.6864, Batch Acc: 0.3750\n",
      "[Batch 390/501] Loss: 1.4919, Batch Acc: 0.0000\n",
      "[Batch 400/501] Loss: 1.5617, Batch Acc: 0.2500\n",
      "[Batch 410/501] Loss: 1.8605, Batch Acc: 0.1250\n",
      "[Batch 420/501] Loss: 1.6983, Batch Acc: 0.1250\n",
      "[Batch 430/501] Loss: 2.0873, Batch Acc: 0.0000\n",
      "[Batch 440/501] Loss: 1.3857, Batch Acc: 0.5000\n",
      "[Batch 450/501] Loss: 1.5715, Batch Acc: 0.3750\n",
      "[Batch 460/501] Loss: 1.9938, Batch Acc: 0.0000\n",
      "[Batch 470/501] Loss: 1.5475, Batch Acc: 0.2500\n",
      "[Batch 480/501] Loss: 1.6249, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.5964, Batch Acc: 0.2500\n",
      "[Batch 500/501] Loss: 1.9172, Batch Acc: 0.1250\n",
      "Epoch 10 Summary - Loss: 829.3298, Train Accuracy: 0.2183\n",
      "Test Accuracy: 0.2522\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_accuracy</td><td>▃▆█▆▂▃▁▆▇▂</td></tr><tr><td>train_loss</td><td>█▅▆▆█▃▆▁▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>test_accuracy</td><td>0.25223</td></tr><tr><td>test_loss</td><td>209.41888</td></tr><tr><td>train_accuracy</td><td>0.21831</td></tr><tr><td>train_loss</td><td>829.32982</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fearless-sweep-3</strong> at: <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/ev4e6mk0' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/ev4e6mk0</a><br> View project at: <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250615_135807-ev4e6mk0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7b1dpkxu with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_nodes: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ajit.kumar4@happiestminds.com/Documents/Drackula/ComputerVisionFromScratch/wandb/run-20250615_140129-7b1dpkxu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/7b1dpkxu' target=\"_blank\">sleek-sweep-4</a></strong> to <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/7b1dpkxu' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/7b1dpkxu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 10/501] Loss: 1.6864, Batch Acc: 0.3750\n",
      "[Batch 20/501] Loss: 1.4515, Batch Acc: 0.5000\n",
      "[Batch 30/501] Loss: 1.3863, Batch Acc: 0.3750\n",
      "[Batch 40/501] Loss: 1.6873, Batch Acc: 0.1250\n",
      "[Batch 50/501] Loss: 1.3603, Batch Acc: 0.3750\n",
      "[Batch 60/501] Loss: 1.3786, Batch Acc: 0.6250\n",
      "[Batch 70/501] Loss: 1.3656, Batch Acc: 0.3750\n",
      "[Batch 80/501] Loss: 1.5057, Batch Acc: 0.5000\n",
      "[Batch 90/501] Loss: 1.5236, Batch Acc: 0.2500\n",
      "[Batch 100/501] Loss: 1.8195, Batch Acc: 0.1250\n",
      "[Batch 110/501] Loss: 1.6637, Batch Acc: 0.2500\n",
      "[Batch 120/501] Loss: 1.8137, Batch Acc: 0.0000\n",
      "[Batch 130/501] Loss: 1.5816, Batch Acc: 0.3750\n",
      "[Batch 140/501] Loss: 1.6868, Batch Acc: 0.1250\n",
      "[Batch 150/501] Loss: 1.5722, Batch Acc: 0.2500\n",
      "[Batch 160/501] Loss: 1.3699, Batch Acc: 0.5000\n",
      "[Batch 170/501] Loss: 1.8014, Batch Acc: 0.2500\n",
      "[Batch 180/501] Loss: 1.3613, Batch Acc: 0.3750\n",
      "[Batch 190/501] Loss: 1.5178, Batch Acc: 0.3750\n",
      "[Batch 200/501] Loss: 1.6952, Batch Acc: 0.2500\n",
      "[Batch 210/501] Loss: 1.6071, Batch Acc: 0.3750\n",
      "[Batch 220/501] Loss: 1.6642, Batch Acc: 0.5000\n",
      "[Batch 230/501] Loss: 1.5383, Batch Acc: 0.3750\n",
      "[Batch 240/501] Loss: 1.9296, Batch Acc: 0.0000\n",
      "[Batch 250/501] Loss: 1.5457, Batch Acc: 0.3750\n",
      "[Batch 260/501] Loss: 1.8177, Batch Acc: 0.1250\n",
      "[Batch 270/501] Loss: 1.6699, Batch Acc: 0.2500\n",
      "[Batch 280/501] Loss: 1.3980, Batch Acc: 0.5000\n",
      "[Batch 290/501] Loss: 1.6223, Batch Acc: 0.1250\n",
      "[Batch 300/501] Loss: 1.6130, Batch Acc: 0.1250\n",
      "[Batch 310/501] Loss: 1.8320, Batch Acc: 0.2500\n",
      "[Batch 320/501] Loss: 1.5618, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 1.6379, Batch Acc: 0.2500\n",
      "[Batch 340/501] Loss: 1.9055, Batch Acc: 0.1250\n",
      "[Batch 350/501] Loss: 1.5558, Batch Acc: 0.2500\n",
      "[Batch 360/501] Loss: 1.4267, Batch Acc: 0.3750\n",
      "[Batch 370/501] Loss: 1.7571, Batch Acc: 0.0000\n",
      "[Batch 380/501] Loss: 1.8021, Batch Acc: 0.1250\n",
      "[Batch 390/501] Loss: 1.6213, Batch Acc: 0.2500\n",
      "[Batch 400/501] Loss: 1.4740, Batch Acc: 0.2500\n",
      "[Batch 410/501] Loss: 1.7106, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 2.0740, Batch Acc: 0.0000\n",
      "[Batch 430/501] Loss: 1.6894, Batch Acc: 0.2500\n",
      "[Batch 440/501] Loss: 1.3756, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.7163, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.6514, Batch Acc: 0.3750\n",
      "[Batch 470/501] Loss: 1.7731, Batch Acc: 0.0000\n",
      "[Batch 480/501] Loss: 1.6473, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.4667, Batch Acc: 0.2500\n",
      "[Batch 500/501] Loss: 1.6222, Batch Acc: 0.2500\n",
      "Epoch 1 Summary - Loss: 823.4218, Train Accuracy: 0.2410\n",
      "[Batch 10/501] Loss: 1.5722, Batch Acc: 0.3750\n",
      "[Batch 20/501] Loss: 1.5733, Batch Acc: 0.3750\n",
      "[Batch 30/501] Loss: 1.5787, Batch Acc: 0.2500\n",
      "[Batch 40/501] Loss: 1.5952, Batch Acc: 0.1250\n",
      "[Batch 50/501] Loss: 1.3639, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.5239, Batch Acc: 0.2500\n",
      "[Batch 70/501] Loss: 1.7082, Batch Acc: 0.1250\n",
      "[Batch 80/501] Loss: 1.4921, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.5059, Batch Acc: 0.5000\n",
      "[Batch 100/501] Loss: 1.5603, Batch Acc: 0.3750\n",
      "[Batch 110/501] Loss: 1.6465, Batch Acc: 0.2500\n",
      "[Batch 120/501] Loss: 1.6144, Batch Acc: 0.2500\n",
      "[Batch 130/501] Loss: 1.7477, Batch Acc: 0.2500\n",
      "[Batch 140/501] Loss: 1.4790, Batch Acc: 0.2500\n",
      "[Batch 150/501] Loss: 1.6618, Batch Acc: 0.1250\n",
      "[Batch 160/501] Loss: 1.5433, Batch Acc: 0.2500\n",
      "[Batch 170/501] Loss: 1.4929, Batch Acc: 0.3750\n",
      "[Batch 180/501] Loss: 1.6181, Batch Acc: 0.0000\n",
      "[Batch 190/501] Loss: 1.8614, Batch Acc: 0.1250\n",
      "[Batch 200/501] Loss: 1.4788, Batch Acc: 0.5000\n",
      "[Batch 210/501] Loss: 1.6041, Batch Acc: 0.3750\n",
      "[Batch 220/501] Loss: 1.7724, Batch Acc: 0.1250\n",
      "[Batch 230/501] Loss: 1.2458, Batch Acc: 0.6250\n",
      "[Batch 240/501] Loss: 1.3505, Batch Acc: 0.5000\n",
      "[Batch 250/501] Loss: 1.6267, Batch Acc: 0.2500\n",
      "[Batch 260/501] Loss: 1.5222, Batch Acc: 0.5000\n",
      "[Batch 270/501] Loss: 1.8484, Batch Acc: 0.1250\n",
      "[Batch 280/501] Loss: 1.5551, Batch Acc: 0.5000\n",
      "[Batch 290/501] Loss: 1.8615, Batch Acc: 0.0000\n",
      "[Batch 300/501] Loss: 1.1787, Batch Acc: 0.6250\n",
      "[Batch 310/501] Loss: 1.5351, Batch Acc: 0.3750\n",
      "[Batch 320/501] Loss: 1.3632, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 1.8263, Batch Acc: 0.2500\n",
      "[Batch 340/501] Loss: 1.6005, Batch Acc: 0.3750\n",
      "[Batch 350/501] Loss: 1.6899, Batch Acc: 0.1250\n",
      "[Batch 360/501] Loss: 1.7436, Batch Acc: 0.1250\n",
      "[Batch 370/501] Loss: 1.3027, Batch Acc: 0.5000\n",
      "[Batch 380/501] Loss: 1.9523, Batch Acc: 0.0000\n",
      "[Batch 390/501] Loss: 1.7308, Batch Acc: 0.1250\n",
      "[Batch 400/501] Loss: 1.9819, Batch Acc: 0.0000\n",
      "[Batch 410/501] Loss: 1.6961, Batch Acc: 0.1250\n",
      "[Batch 420/501] Loss: 1.7209, Batch Acc: 0.2500\n",
      "[Batch 430/501] Loss: 1.5818, Batch Acc: 0.2500\n",
      "[Batch 440/501] Loss: 1.9872, Batch Acc: 0.0000\n",
      "[Batch 450/501] Loss: 1.7227, Batch Acc: 0.1250\n",
      "[Batch 460/501] Loss: 1.8840, Batch Acc: 0.1250\n",
      "[Batch 470/501] Loss: 1.5944, Batch Acc: 0.1250\n",
      "[Batch 480/501] Loss: 1.4738, Batch Acc: 0.1250\n",
      "[Batch 490/501] Loss: 1.6066, Batch Acc: 0.1250\n",
      "[Batch 500/501] Loss: 1.6603, Batch Acc: 0.1250\n",
      "Epoch 2 Summary - Loss: 818.8899, Train Accuracy: 0.2408\n",
      "[Batch 10/501] Loss: 1.6943, Batch Acc: 0.1250\n",
      "[Batch 20/501] Loss: 1.6862, Batch Acc: 0.1250\n",
      "[Batch 30/501] Loss: 1.6703, Batch Acc: 0.1250\n",
      "[Batch 40/501] Loss: 1.4395, Batch Acc: 0.2500\n",
      "[Batch 50/501] Loss: 1.5174, Batch Acc: 0.3750\n",
      "[Batch 60/501] Loss: 1.4731, Batch Acc: 0.3750\n",
      "[Batch 70/501] Loss: 1.8765, Batch Acc: 0.1250\n",
      "[Batch 80/501] Loss: 1.7854, Batch Acc: 0.1250\n",
      "[Batch 90/501] Loss: 1.8151, Batch Acc: 0.0000\n",
      "[Batch 100/501] Loss: 1.7052, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 1.6704, Batch Acc: 0.1250\n",
      "[Batch 120/501] Loss: 1.6553, Batch Acc: 0.2500\n",
      "[Batch 130/501] Loss: 1.6147, Batch Acc: 0.2500\n",
      "[Batch 140/501] Loss: 1.4282, Batch Acc: 0.3750\n",
      "[Batch 150/501] Loss: 1.7440, Batch Acc: 0.2500\n",
      "[Batch 160/501] Loss: 1.4275, Batch Acc: 0.3750\n",
      "[Batch 170/501] Loss: 1.7524, Batch Acc: 0.1250\n",
      "[Batch 180/501] Loss: 1.7065, Batch Acc: 0.1250\n",
      "[Batch 190/501] Loss: 1.6356, Batch Acc: 0.2500\n",
      "[Batch 200/501] Loss: 1.7472, Batch Acc: 0.2500\n",
      "[Batch 210/501] Loss: 1.3232, Batch Acc: 0.5000\n",
      "[Batch 220/501] Loss: 2.0552, Batch Acc: 0.0000\n",
      "[Batch 230/501] Loss: 1.6227, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.4221, Batch Acc: 0.3750\n",
      "[Batch 250/501] Loss: 1.4644, Batch Acc: 0.6250\n",
      "[Batch 260/501] Loss: 1.4752, Batch Acc: 0.3750\n",
      "[Batch 270/501] Loss: 1.3974, Batch Acc: 0.2500\n",
      "[Batch 280/501] Loss: 1.5932, Batch Acc: 0.2500\n",
      "[Batch 290/501] Loss: 1.8610, Batch Acc: 0.2500\n",
      "[Batch 300/501] Loss: 1.4256, Batch Acc: 0.3750\n",
      "[Batch 310/501] Loss: 1.6754, Batch Acc: 0.3750\n",
      "[Batch 320/501] Loss: 1.6054, Batch Acc: 0.3750\n",
      "[Batch 330/501] Loss: 1.6637, Batch Acc: 0.1250\n",
      "[Batch 340/501] Loss: 1.3989, Batch Acc: 0.3750\n",
      "[Batch 350/501] Loss: 1.8348, Batch Acc: 0.1250\n",
      "[Batch 360/501] Loss: 1.4307, Batch Acc: 0.3750\n",
      "[Batch 370/501] Loss: 1.7222, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.5494, Batch Acc: 0.5000\n",
      "[Batch 390/501] Loss: 1.8854, Batch Acc: 0.0000\n",
      "[Batch 400/501] Loss: 1.6618, Batch Acc: 0.2500\n",
      "[Batch 410/501] Loss: 1.6833, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.7908, Batch Acc: 0.1250\n",
      "[Batch 430/501] Loss: 1.7685, Batch Acc: 0.2500\n",
      "[Batch 440/501] Loss: 1.9135, Batch Acc: 0.0000\n",
      "[Batch 450/501] Loss: 1.9972, Batch Acc: 0.1250\n",
      "[Batch 460/501] Loss: 1.2951, Batch Acc: 0.5000\n",
      "[Batch 470/501] Loss: 1.7320, Batch Acc: 0.3750\n",
      "[Batch 480/501] Loss: 1.7300, Batch Acc: 0.1250\n",
      "[Batch 490/501] Loss: 1.7137, Batch Acc: 0.2500\n",
      "[Batch 500/501] Loss: 1.5223, Batch Acc: 0.3750\n",
      "Epoch 3 Summary - Loss: 820.7256, Train Accuracy: 0.2445\n",
      "[Batch 10/501] Loss: 1.6744, Batch Acc: 0.2500\n",
      "[Batch 20/501] Loss: 1.6026, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.4012, Batch Acc: 0.2500\n",
      "[Batch 40/501] Loss: 1.6014, Batch Acc: 0.2500\n",
      "[Batch 50/501] Loss: 1.7472, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.4918, Batch Acc: 0.1250\n",
      "[Batch 70/501] Loss: 1.5672, Batch Acc: 0.3750\n",
      "[Batch 80/501] Loss: 1.7319, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.3345, Batch Acc: 0.5000\n",
      "[Batch 100/501] Loss: 1.7670, Batch Acc: 0.1250\n",
      "[Batch 110/501] Loss: 1.8897, Batch Acc: 0.1250\n",
      "[Batch 120/501] Loss: 1.6435, Batch Acc: 0.2500\n",
      "[Batch 130/501] Loss: 1.6627, Batch Acc: 0.0000\n",
      "[Batch 140/501] Loss: 1.4672, Batch Acc: 0.2500\n",
      "[Batch 150/501] Loss: 1.5286, Batch Acc: 0.3750\n",
      "[Batch 160/501] Loss: 1.2568, Batch Acc: 0.5000\n",
      "[Batch 170/501] Loss: 1.7550, Batch Acc: 0.3750\n",
      "[Batch 180/501] Loss: 1.6787, Batch Acc: 0.2500\n",
      "[Batch 190/501] Loss: 1.7117, Batch Acc: 0.2500\n",
      "[Batch 200/501] Loss: 1.5197, Batch Acc: 0.2500\n",
      "[Batch 210/501] Loss: 1.7212, Batch Acc: 0.0000\n",
      "[Batch 220/501] Loss: 1.3555, Batch Acc: 0.5000\n",
      "[Batch 230/501] Loss: 1.4537, Batch Acc: 0.3750\n",
      "[Batch 240/501] Loss: 1.5454, Batch Acc: 0.2500\n",
      "[Batch 250/501] Loss: 1.7049, Batch Acc: 0.3750\n",
      "[Batch 260/501] Loss: 1.5126, Batch Acc: 0.3750\n",
      "[Batch 270/501] Loss: 1.4845, Batch Acc: 0.3750\n",
      "[Batch 280/501] Loss: 1.5453, Batch Acc: 0.2500\n",
      "[Batch 290/501] Loss: 1.6147, Batch Acc: 0.2500\n",
      "[Batch 300/501] Loss: 1.7839, Batch Acc: 0.1250\n",
      "[Batch 310/501] Loss: 1.5353, Batch Acc: 0.1250\n",
      "[Batch 320/501] Loss: 1.8162, Batch Acc: 0.1250\n",
      "[Batch 330/501] Loss: 1.6408, Batch Acc: 0.2500\n",
      "[Batch 340/501] Loss: 1.5729, Batch Acc: 0.3750\n",
      "[Batch 350/501] Loss: 1.5715, Batch Acc: 0.2500\n",
      "[Batch 360/501] Loss: 1.8861, Batch Acc: 0.1250\n",
      "[Batch 370/501] Loss: 1.4035, Batch Acc: 0.5000\n",
      "[Batch 380/501] Loss: 1.5863, Batch Acc: 0.2500\n",
      "[Batch 390/501] Loss: 1.7271, Batch Acc: 0.1250\n",
      "[Batch 400/501] Loss: 1.6052, Batch Acc: 0.2500\n",
      "[Batch 410/501] Loss: 1.8947, Batch Acc: 0.0000\n",
      "[Batch 420/501] Loss: 1.7200, Batch Acc: 0.1250\n",
      "[Batch 430/501] Loss: 1.8560, Batch Acc: 0.1250\n",
      "[Batch 440/501] Loss: 1.3215, Batch Acc: 0.3750\n",
      "[Batch 450/501] Loss: 1.7171, Batch Acc: 0.1250\n",
      "[Batch 460/501] Loss: 2.1589, Batch Acc: 0.0000\n",
      "[Batch 470/501] Loss: 1.7705, Batch Acc: 0.1250\n",
      "[Batch 480/501] Loss: 1.4554, Batch Acc: 0.3750\n",
      "[Batch 490/501] Loss: 1.7098, Batch Acc: 0.1250\n",
      "[Batch 500/501] Loss: 1.9533, Batch Acc: 0.1250\n",
      "Epoch 4 Summary - Loss: 821.5775, Train Accuracy: 0.2395\n",
      "[Batch 10/501] Loss: 1.5837, Batch Acc: 0.1250\n",
      "[Batch 20/501] Loss: 1.6642, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.6286, Batch Acc: 0.5000\n",
      "[Batch 40/501] Loss: 1.7498, Batch Acc: 0.1250\n",
      "[Batch 50/501] Loss: 1.6826, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.5818, Batch Acc: 0.3750\n",
      "[Batch 70/501] Loss: 1.6879, Batch Acc: 0.0000\n",
      "[Batch 80/501] Loss: 1.6646, Batch Acc: 0.0000\n",
      "[Batch 90/501] Loss: 1.5893, Batch Acc: 0.1250\n",
      "[Batch 100/501] Loss: 2.0861, Batch Acc: 0.0000\n",
      "[Batch 110/501] Loss: 1.5657, Batch Acc: 0.2500\n",
      "[Batch 120/501] Loss: 1.6870, Batch Acc: 0.2500\n",
      "[Batch 130/501] Loss: 1.5739, Batch Acc: 0.2500\n",
      "[Batch 140/501] Loss: 1.7186, Batch Acc: 0.1250\n",
      "[Batch 150/501] Loss: 1.6022, Batch Acc: 0.3750\n",
      "[Batch 160/501] Loss: 1.4544, Batch Acc: 0.3750\n",
      "[Batch 170/501] Loss: 1.5640, Batch Acc: 0.2500\n",
      "[Batch 180/501] Loss: 1.8778, Batch Acc: 0.1250\n",
      "[Batch 190/501] Loss: 1.9378, Batch Acc: 0.1250\n",
      "[Batch 200/501] Loss: 1.4931, Batch Acc: 0.3750\n",
      "[Batch 210/501] Loss: 1.5631, Batch Acc: 0.3750\n",
      "[Batch 220/501] Loss: 1.8166, Batch Acc: 0.1250\n",
      "[Batch 230/501] Loss: 1.7240, Batch Acc: 0.1250\n",
      "[Batch 240/501] Loss: 1.5368, Batch Acc: 0.2500\n",
      "[Batch 250/501] Loss: 1.6364, Batch Acc: 0.1250\n",
      "[Batch 260/501] Loss: 1.6587, Batch Acc: 0.2500\n",
      "[Batch 270/501] Loss: 1.7406, Batch Acc: 0.1250\n",
      "[Batch 280/501] Loss: 1.7522, Batch Acc: 0.0000\n",
      "[Batch 290/501] Loss: 1.7811, Batch Acc: 0.0000\n",
      "[Batch 300/501] Loss: 1.8106, Batch Acc: 0.0000\n",
      "[Batch 310/501] Loss: 1.6746, Batch Acc: 0.2500\n",
      "[Batch 320/501] Loss: 1.7897, Batch Acc: 0.1250\n",
      "[Batch 330/501] Loss: 1.7865, Batch Acc: 0.1250\n",
      "[Batch 340/501] Loss: 1.5271, Batch Acc: 0.5000\n",
      "[Batch 350/501] Loss: 1.6921, Batch Acc: 0.1250\n",
      "[Batch 360/501] Loss: 1.5010, Batch Acc: 0.3750\n",
      "[Batch 370/501] Loss: 1.8731, Batch Acc: 0.0000\n",
      "[Batch 380/501] Loss: 1.5763, Batch Acc: 0.0000\n",
      "[Batch 390/501] Loss: 1.5453, Batch Acc: 0.2500\n",
      "[Batch 400/501] Loss: 1.9223, Batch Acc: 0.1250\n",
      "[Batch 410/501] Loss: 1.6156, Batch Acc: 0.1250\n",
      "[Batch 420/501] Loss: 1.4787, Batch Acc: 0.5000\n",
      "[Batch 430/501] Loss: 1.4547, Batch Acc: 0.3750\n",
      "[Batch 440/501] Loss: 1.8130, Batch Acc: 0.1250\n",
      "[Batch 450/501] Loss: 2.0285, Batch Acc: 0.0000\n",
      "[Batch 460/501] Loss: 1.4521, Batch Acc: 0.5000\n",
      "[Batch 470/501] Loss: 1.7401, Batch Acc: 0.1250\n",
      "[Batch 480/501] Loss: 1.4738, Batch Acc: 0.3750\n",
      "[Batch 490/501] Loss: 1.5676, Batch Acc: 0.3750\n",
      "[Batch 500/501] Loss: 1.4380, Batch Acc: 0.2500\n",
      "Epoch 5 Summary - Loss: 821.2677, Train Accuracy: 0.2385\n",
      "[Batch 10/501] Loss: 1.8273, Batch Acc: 0.0000\n",
      "[Batch 20/501] Loss: 1.7623, Batch Acc: 0.1250\n",
      "[Batch 30/501] Loss: 1.9174, Batch Acc: 0.0000\n",
      "[Batch 40/501] Loss: 1.7437, Batch Acc: 0.1250\n",
      "[Batch 50/501] Loss: 1.7415, Batch Acc: 0.1250\n",
      "[Batch 60/501] Loss: 1.7617, Batch Acc: 0.2500\n",
      "[Batch 70/501] Loss: 1.8221, Batch Acc: 0.0000\n",
      "[Batch 80/501] Loss: 1.7515, Batch Acc: 0.1250\n",
      "[Batch 90/501] Loss: 1.6117, Batch Acc: 0.2500\n",
      "[Batch 100/501] Loss: 1.7248, Batch Acc: 0.1250\n",
      "[Batch 110/501] Loss: 1.5485, Batch Acc: 0.3750\n",
      "[Batch 120/501] Loss: 1.5714, Batch Acc: 0.3750\n",
      "[Batch 130/501] Loss: 1.6862, Batch Acc: 0.1250\n",
      "[Batch 140/501] Loss: 1.7980, Batch Acc: 0.1250\n",
      "[Batch 150/501] Loss: 1.5971, Batch Acc: 0.1250\n",
      "[Batch 160/501] Loss: 1.9731, Batch Acc: 0.1250\n",
      "[Batch 170/501] Loss: 1.5716, Batch Acc: 0.1250\n",
      "[Batch 180/501] Loss: 1.6033, Batch Acc: 0.2500\n",
      "[Batch 190/501] Loss: 1.7824, Batch Acc: 0.1250\n",
      "[Batch 200/501] Loss: 1.7177, Batch Acc: 0.1250\n",
      "[Batch 210/501] Loss: 1.7096, Batch Acc: 0.1250\n",
      "[Batch 220/501] Loss: 1.9000, Batch Acc: 0.1250\n",
      "[Batch 230/501] Loss: 1.4402, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.6497, Batch Acc: 0.1250\n",
      "[Batch 250/501] Loss: 1.2501, Batch Acc: 0.5000\n",
      "[Batch 260/501] Loss: 1.9745, Batch Acc: 0.0000\n",
      "[Batch 270/501] Loss: 1.7071, Batch Acc: 0.1250\n",
      "[Batch 280/501] Loss: 1.4193, Batch Acc: 0.5000\n",
      "[Batch 290/501] Loss: 1.4680, Batch Acc: 0.3750\n",
      "[Batch 300/501] Loss: 1.6627, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 1.7264, Batch Acc: 0.1250\n",
      "[Batch 320/501] Loss: 1.6286, Batch Acc: 0.0000\n",
      "[Batch 330/501] Loss: 1.7251, Batch Acc: 0.1250\n",
      "[Batch 340/501] Loss: 1.9031, Batch Acc: 0.0000\n",
      "[Batch 350/501] Loss: 1.6787, Batch Acc: 0.2500\n",
      "[Batch 360/501] Loss: 2.0880, Batch Acc: 0.1250\n",
      "[Batch 370/501] Loss: 1.4877, Batch Acc: 0.5000\n",
      "[Batch 380/501] Loss: 1.4119, Batch Acc: 0.3750\n",
      "[Batch 390/501] Loss: 1.7837, Batch Acc: 0.1250\n",
      "[Batch 400/501] Loss: 1.5621, Batch Acc: 0.2500\n",
      "[Batch 410/501] Loss: 1.8352, Batch Acc: 0.1250\n",
      "[Batch 420/501] Loss: 1.4895, Batch Acc: 0.3750\n",
      "[Batch 430/501] Loss: 1.4944, Batch Acc: 0.3750\n",
      "[Batch 440/501] Loss: 2.0138, Batch Acc: 0.1250\n",
      "[Batch 450/501] Loss: 1.8686, Batch Acc: 0.0000\n",
      "[Batch 460/501] Loss: 1.6778, Batch Acc: 0.1250\n",
      "[Batch 470/501] Loss: 1.7603, Batch Acc: 0.2500\n",
      "[Batch 480/501] Loss: 1.6553, Batch Acc: 0.1250\n",
      "[Batch 490/501] Loss: 1.8067, Batch Acc: 0.1250\n",
      "[Batch 500/501] Loss: 1.5376, Batch Acc: 0.2500\n",
      "Epoch 6 Summary - Loss: 820.1248, Train Accuracy: 0.2473\n",
      "[Batch 10/501] Loss: 1.8236, Batch Acc: 0.1250\n",
      "[Batch 20/501] Loss: 1.4726, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.7183, Batch Acc: 0.1250\n",
      "[Batch 40/501] Loss: 1.7626, Batch Acc: 0.0000\n",
      "[Batch 50/501] Loss: 1.7460, Batch Acc: 0.1250\n",
      "[Batch 60/501] Loss: 1.5227, Batch Acc: 0.2500\n",
      "[Batch 70/501] Loss: 1.7768, Batch Acc: 0.1250\n",
      "[Batch 80/501] Loss: 1.2997, Batch Acc: 0.6250\n",
      "[Batch 90/501] Loss: 1.8515, Batch Acc: 0.1250\n",
      "[Batch 100/501] Loss: 1.1903, Batch Acc: 0.6250\n",
      "[Batch 110/501] Loss: 1.7053, Batch Acc: 0.1250\n",
      "[Batch 120/501] Loss: 1.6583, Batch Acc: 0.1250\n",
      "[Batch 130/501] Loss: 1.5666, Batch Acc: 0.2500\n",
      "[Batch 140/501] Loss: 1.5627, Batch Acc: 0.2500\n",
      "[Batch 150/501] Loss: 1.3175, Batch Acc: 0.3750\n",
      "[Batch 160/501] Loss: 1.6822, Batch Acc: 0.1250\n",
      "[Batch 170/501] Loss: 1.6850, Batch Acc: 0.2500\n",
      "[Batch 180/501] Loss: 1.8108, Batch Acc: 0.0000\n",
      "[Batch 190/501] Loss: 1.4892, Batch Acc: 0.3750\n",
      "[Batch 200/501] Loss: 1.6292, Batch Acc: 0.2500\n",
      "[Batch 210/501] Loss: 1.6018, Batch Acc: 0.2500\n",
      "[Batch 220/501] Loss: 1.5986, Batch Acc: 0.1250\n",
      "[Batch 230/501] Loss: 1.6227, Batch Acc: 0.3750\n",
      "[Batch 240/501] Loss: 1.6577, Batch Acc: 0.2500\n",
      "[Batch 250/501] Loss: 1.7885, Batch Acc: 0.2500\n",
      "[Batch 260/501] Loss: 1.4764, Batch Acc: 0.3750\n",
      "[Batch 270/501] Loss: 1.7704, Batch Acc: 0.0000\n",
      "[Batch 280/501] Loss: 1.4842, Batch Acc: 0.2500\n",
      "[Batch 290/501] Loss: 1.4688, Batch Acc: 0.5000\n",
      "[Batch 300/501] Loss: 1.5975, Batch Acc: 0.3750\n",
      "[Batch 310/501] Loss: 1.6373, Batch Acc: 0.2500\n",
      "[Batch 320/501] Loss: 1.4202, Batch Acc: 0.5000\n",
      "[Batch 330/501] Loss: 1.6387, Batch Acc: 0.0000\n",
      "[Batch 340/501] Loss: 1.4202, Batch Acc: 0.3750\n",
      "[Batch 350/501] Loss: 1.6117, Batch Acc: 0.1250\n",
      "[Batch 360/501] Loss: 1.9094, Batch Acc: 0.0000\n",
      "[Batch 370/501] Loss: 1.7037, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.6842, Batch Acc: 0.1250\n",
      "[Batch 390/501] Loss: 1.8547, Batch Acc: 0.0000\n",
      "[Batch 400/501] Loss: 1.7134, Batch Acc: 0.1250\n",
      "[Batch 410/501] Loss: 1.7899, Batch Acc: 0.1250\n",
      "[Batch 420/501] Loss: 1.5091, Batch Acc: 0.5000\n",
      "[Batch 430/501] Loss: 1.6373, Batch Acc: 0.2500\n",
      "[Batch 440/501] Loss: 1.9040, Batch Acc: 0.1250\n",
      "[Batch 450/501] Loss: 1.5400, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.7313, Batch Acc: 0.1250\n",
      "[Batch 470/501] Loss: 1.7933, Batch Acc: 0.1250\n",
      "[Batch 480/501] Loss: 1.5344, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.3725, Batch Acc: 0.3750\n",
      "[Batch 500/501] Loss: 1.6209, Batch Acc: 0.3750\n",
      "Epoch 7 Summary - Loss: 820.5599, Train Accuracy: 0.2310\n",
      "[Batch 10/501] Loss: 1.7709, Batch Acc: 0.0000\n",
      "[Batch 20/501] Loss: 1.6754, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.7010, Batch Acc: 0.3750\n",
      "[Batch 40/501] Loss: 1.6885, Batch Acc: 0.2500\n",
      "[Batch 50/501] Loss: 1.9743, Batch Acc: 0.0000\n",
      "[Batch 60/501] Loss: 1.5906, Batch Acc: 0.1250\n",
      "[Batch 70/501] Loss: 1.7181, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.4609, Batch Acc: 0.5000\n",
      "[Batch 90/501] Loss: 1.4536, Batch Acc: 0.2500\n",
      "[Batch 100/501] Loss: 1.5212, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 1.5743, Batch Acc: 0.2500\n",
      "[Batch 120/501] Loss: 1.6219, Batch Acc: 0.0000\n",
      "[Batch 130/501] Loss: 1.5877, Batch Acc: 0.1250\n",
      "[Batch 140/501] Loss: 1.6787, Batch Acc: 0.3750\n",
      "[Batch 150/501] Loss: 1.4294, Batch Acc: 0.1250\n",
      "[Batch 160/501] Loss: 1.8826, Batch Acc: 0.1250\n",
      "[Batch 170/501] Loss: 1.5963, Batch Acc: 0.3750\n",
      "[Batch 180/501] Loss: 1.7669, Batch Acc: 0.0000\n",
      "[Batch 190/501] Loss: 1.4736, Batch Acc: 0.3750\n",
      "[Batch 200/501] Loss: 1.6458, Batch Acc: 0.3750\n",
      "[Batch 210/501] Loss: 1.7070, Batch Acc: 0.0000\n",
      "[Batch 220/501] Loss: 1.4929, Batch Acc: 0.2500\n",
      "[Batch 230/501] Loss: 1.5041, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.7798, Batch Acc: 0.0000\n",
      "[Batch 250/501] Loss: 1.5273, Batch Acc: 0.2500\n",
      "[Batch 260/501] Loss: 1.7563, Batch Acc: 0.2500\n",
      "[Batch 270/501] Loss: 1.3170, Batch Acc: 0.3750\n",
      "[Batch 280/501] Loss: 1.5897, Batch Acc: 0.2500\n",
      "[Batch 290/501] Loss: 1.5079, Batch Acc: 0.3750\n",
      "[Batch 300/501] Loss: 1.7428, Batch Acc: 0.1250\n",
      "[Batch 310/501] Loss: 1.8896, Batch Acc: 0.1250\n",
      "[Batch 320/501] Loss: 1.7072, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 1.6183, Batch Acc: 0.2500\n",
      "[Batch 340/501] Loss: 1.6742, Batch Acc: 0.1250\n",
      "[Batch 350/501] Loss: 1.6741, Batch Acc: 0.2500\n",
      "[Batch 360/501] Loss: 1.5111, Batch Acc: 0.1250\n",
      "[Batch 370/501] Loss: 1.5950, Batch Acc: 0.2500\n",
      "[Batch 380/501] Loss: 1.7359, Batch Acc: 0.2500\n",
      "[Batch 390/501] Loss: 1.5841, Batch Acc: 0.2500\n",
      "[Batch 400/501] Loss: 1.6519, Batch Acc: 0.1250\n",
      "[Batch 410/501] Loss: 1.6218, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.4934, Batch Acc: 0.3750\n",
      "[Batch 430/501] Loss: 1.5769, Batch Acc: 0.3750\n",
      "[Batch 440/501] Loss: 1.5893, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.8480, Batch Acc: 0.1250\n",
      "[Batch 460/501] Loss: 1.8050, Batch Acc: 0.1250\n",
      "[Batch 470/501] Loss: 1.5066, Batch Acc: 0.2500\n",
      "[Batch 480/501] Loss: 1.9201, Batch Acc: 0.1250\n",
      "[Batch 490/501] Loss: 1.5628, Batch Acc: 0.2500\n",
      "[Batch 500/501] Loss: 1.4586, Batch Acc: 0.3750\n",
      "Epoch 8 Summary - Loss: 815.2319, Train Accuracy: 0.2435\n",
      "[Batch 10/501] Loss: 1.9238, Batch Acc: 0.0000\n",
      "[Batch 20/501] Loss: 1.7436, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.6554, Batch Acc: 0.1250\n",
      "[Batch 40/501] Loss: 1.7011, Batch Acc: 0.1250\n",
      "[Batch 50/501] Loss: 1.7973, Batch Acc: 0.1250\n",
      "[Batch 60/501] Loss: 1.7540, Batch Acc: 0.1250\n",
      "[Batch 70/501] Loss: 1.3584, Batch Acc: 0.5000\n",
      "[Batch 80/501] Loss: 1.5498, Batch Acc: 0.1250\n",
      "[Batch 90/501] Loss: 1.4884, Batch Acc: 0.3750\n",
      "[Batch 100/501] Loss: 1.6205, Batch Acc: 0.0000\n",
      "[Batch 110/501] Loss: 1.6819, Batch Acc: 0.2500\n",
      "[Batch 120/501] Loss: 1.5902, Batch Acc: 0.3750\n",
      "[Batch 130/501] Loss: 1.4254, Batch Acc: 0.5000\n",
      "[Batch 140/501] Loss: 1.6139, Batch Acc: 0.3750\n",
      "[Batch 150/501] Loss: 1.7203, Batch Acc: 0.2500\n",
      "[Batch 160/501] Loss: 1.8649, Batch Acc: 0.1250\n",
      "[Batch 170/501] Loss: 1.7565, Batch Acc: 0.1250\n",
      "[Batch 180/501] Loss: 1.3324, Batch Acc: 0.3750\n",
      "[Batch 190/501] Loss: 1.7963, Batch Acc: 0.1250\n",
      "[Batch 200/501] Loss: 1.5155, Batch Acc: 0.3750\n",
      "[Batch 210/501] Loss: 1.5958, Batch Acc: 0.3750\n",
      "[Batch 220/501] Loss: 1.8755, Batch Acc: 0.1250\n",
      "[Batch 230/501] Loss: 1.6146, Batch Acc: 0.3750\n",
      "[Batch 240/501] Loss: 1.4844, Batch Acc: 0.5000\n",
      "[Batch 250/501] Loss: 1.7495, Batch Acc: 0.0000\n",
      "[Batch 260/501] Loss: 1.7809, Batch Acc: 0.1250\n",
      "[Batch 270/501] Loss: 1.4144, Batch Acc: 0.1250\n",
      "[Batch 280/501] Loss: 1.7482, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 1.6265, Batch Acc: 0.2500\n",
      "[Batch 300/501] Loss: 1.6807, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 1.7605, Batch Acc: 0.0000\n",
      "[Batch 320/501] Loss: 1.4525, Batch Acc: 0.3750\n",
      "[Batch 330/501] Loss: 1.3714, Batch Acc: 0.5000\n",
      "[Batch 340/501] Loss: 1.7325, Batch Acc: 0.2500\n",
      "[Batch 350/501] Loss: 1.5336, Batch Acc: 0.3750\n",
      "[Batch 360/501] Loss: 1.6034, Batch Acc: 0.1250\n",
      "[Batch 370/501] Loss: 1.6655, Batch Acc: 0.2500\n",
      "[Batch 380/501] Loss: 1.8173, Batch Acc: 0.1250\n",
      "[Batch 390/501] Loss: 1.6939, Batch Acc: 0.2500\n",
      "[Batch 400/501] Loss: 1.7405, Batch Acc: 0.1250\n",
      "[Batch 410/501] Loss: 1.6925, Batch Acc: 0.3750\n",
      "[Batch 420/501] Loss: 1.7003, Batch Acc: 0.2500\n",
      "[Batch 430/501] Loss: 1.6709, Batch Acc: 0.1250\n",
      "[Batch 440/501] Loss: 1.7209, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.9940, Batch Acc: 0.0000\n",
      "[Batch 460/501] Loss: 1.7662, Batch Acc: 0.1250\n",
      "[Batch 470/501] Loss: 1.6309, Batch Acc: 0.1250\n",
      "[Batch 480/501] Loss: 1.7843, Batch Acc: 0.0000\n",
      "[Batch 490/501] Loss: 1.7407, Batch Acc: 0.1250\n",
      "[Batch 500/501] Loss: 1.8114, Batch Acc: 0.1250\n",
      "Epoch 9 Summary - Loss: 820.8588, Train Accuracy: 0.2365\n",
      "[Batch 10/501] Loss: 1.4415, Batch Acc: 0.2500\n",
      "[Batch 20/501] Loss: 1.8158, Batch Acc: 0.0000\n",
      "[Batch 30/501] Loss: 1.6975, Batch Acc: 0.2500\n",
      "[Batch 40/501] Loss: 1.9283, Batch Acc: 0.0000\n",
      "[Batch 50/501] Loss: 1.3877, Batch Acc: 0.5000\n",
      "[Batch 60/501] Loss: 1.1777, Batch Acc: 0.5000\n",
      "[Batch 70/501] Loss: 1.8257, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.4230, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.7678, Batch Acc: 0.1250\n",
      "[Batch 100/501] Loss: 1.3025, Batch Acc: 0.6250\n",
      "[Batch 110/501] Loss: 1.7182, Batch Acc: 0.1250\n",
      "[Batch 120/501] Loss: 1.5521, Batch Acc: 0.2500\n",
      "[Batch 130/501] Loss: 1.3822, Batch Acc: 0.5000\n",
      "[Batch 140/501] Loss: 1.7874, Batch Acc: 0.0000\n",
      "[Batch 150/501] Loss: 1.7926, Batch Acc: 0.1250\n",
      "[Batch 160/501] Loss: 1.5442, Batch Acc: 0.3750\n",
      "[Batch 170/501] Loss: 1.6990, Batch Acc: 0.1250\n",
      "[Batch 180/501] Loss: 1.5959, Batch Acc: 0.2500\n",
      "[Batch 190/501] Loss: 1.8189, Batch Acc: 0.2500\n",
      "[Batch 200/501] Loss: 1.7763, Batch Acc: 0.0000\n",
      "[Batch 210/501] Loss: 1.9616, Batch Acc: 0.0000\n",
      "[Batch 220/501] Loss: 1.7954, Batch Acc: 0.0000\n",
      "[Batch 230/501] Loss: 1.4462, Batch Acc: 0.3750\n",
      "[Batch 240/501] Loss: 1.4957, Batch Acc: 0.3750\n",
      "[Batch 250/501] Loss: 1.5944, Batch Acc: 0.1250\n",
      "[Batch 260/501] Loss: 1.7722, Batch Acc: 0.0000\n",
      "[Batch 270/501] Loss: 1.5865, Batch Acc: 0.1250\n",
      "[Batch 280/501] Loss: 1.5343, Batch Acc: 0.2500\n",
      "[Batch 290/501] Loss: 1.4838, Batch Acc: 0.3750\n",
      "[Batch 300/501] Loss: 1.7757, Batch Acc: 0.3750\n",
      "[Batch 310/501] Loss: 1.7424, Batch Acc: 0.1250\n",
      "[Batch 320/501] Loss: 1.6603, Batch Acc: 0.3750\n",
      "[Batch 330/501] Loss: 1.6455, Batch Acc: 0.2500\n",
      "[Batch 340/501] Loss: 1.5983, Batch Acc: 0.2500\n",
      "[Batch 350/501] Loss: 1.3087, Batch Acc: 0.6250\n",
      "[Batch 360/501] Loss: 1.5794, Batch Acc: 0.1250\n",
      "[Batch 370/501] Loss: 1.5976, Batch Acc: 0.2500\n",
      "[Batch 380/501] Loss: 1.7277, Batch Acc: 0.2500\n",
      "[Batch 390/501] Loss: 1.7624, Batch Acc: 0.2500\n",
      "[Batch 400/501] Loss: 1.6743, Batch Acc: 0.3750\n",
      "[Batch 410/501] Loss: 1.6451, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.7794, Batch Acc: 0.2500\n",
      "[Batch 430/501] Loss: 1.6469, Batch Acc: 0.3750\n",
      "[Batch 440/501] Loss: 1.4545, Batch Acc: 0.3750\n",
      "[Batch 450/501] Loss: 1.6752, Batch Acc: 0.3750\n",
      "[Batch 460/501] Loss: 1.6266, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.7285, Batch Acc: 0.1250\n",
      "[Batch 480/501] Loss: 1.6620, Batch Acc: 0.1250\n",
      "[Batch 490/501] Loss: 1.6592, Batch Acc: 0.2500\n",
      "[Batch 500/501] Loss: 1.7825, Batch Acc: 0.2500\n",
      "Epoch 10 Summary - Loss: 820.6157, Train Accuracy: 0.2410\n",
      "Test Accuracy: 0.2275\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_accuracy</td><td>▅▅▇▅▄█▁▆▃▅</td></tr><tr><td>train_loss</td><td>█▄▆▆▆▅▆▁▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>test_accuracy</td><td>0.2275</td></tr><tr><td>test_loss</td><td>202.74</td></tr><tr><td>train_accuracy</td><td>0.24102</td></tr><tr><td>train_loss</td><td>820.61566</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sleek-sweep-4</strong> at: <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/7b1dpkxu' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/7b1dpkxu</a><br> View project at: <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250615_140129-7b1dpkxu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4srb7kkk with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_nodes: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.6s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ajit.kumar4@happiestminds.com/Documents/Drackula/ComputerVisionFromScratch/wandb/run-20250615_140459-4srb7kkk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/4srb7kkk' target=\"_blank\">stellar-sweep-5</a></strong> to <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/4srb7kkk' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/4srb7kkk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 10/501] Loss: 1.6599, Batch Acc: 0.3750\n",
      "[Batch 20/501] Loss: 1.5089, Batch Acc: 0.3750\n",
      "[Batch 30/501] Loss: 1.8170, Batch Acc: 0.1250\n",
      "[Batch 40/501] Loss: 1.5502, Batch Acc: 0.2500\n",
      "[Batch 50/501] Loss: 1.7217, Batch Acc: 0.0000\n",
      "[Batch 60/501] Loss: 1.6262, Batch Acc: 0.3750\n",
      "[Batch 70/501] Loss: 1.6610, Batch Acc: 0.1250\n",
      "[Batch 80/501] Loss: 1.5030, Batch Acc: 0.3750\n",
      "[Batch 90/501] Loss: 1.4850, Batch Acc: 0.2500\n",
      "[Batch 100/501] Loss: 1.7934, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 1.8644, Batch Acc: 0.1250\n",
      "[Batch 120/501] Loss: 1.6546, Batch Acc: 0.0000\n",
      "[Batch 130/501] Loss: 1.5647, Batch Acc: 0.2500\n",
      "[Batch 140/501] Loss: 1.6868, Batch Acc: 0.1250\n",
      "[Batch 150/501] Loss: 1.6453, Batch Acc: 0.1250\n",
      "[Batch 160/501] Loss: 1.5782, Batch Acc: 0.3750\n",
      "[Batch 170/501] Loss: 1.7097, Batch Acc: 0.2500\n",
      "[Batch 180/501] Loss: 1.8476, Batch Acc: 0.1250\n",
      "[Batch 190/501] Loss: 1.7876, Batch Acc: 0.2500\n",
      "[Batch 200/501] Loss: 1.7837, Batch Acc: 0.1250\n",
      "[Batch 210/501] Loss: 1.5967, Batch Acc: 0.2500\n",
      "[Batch 220/501] Loss: 1.8106, Batch Acc: 0.1250\n",
      "[Batch 230/501] Loss: 1.5394, Batch Acc: 0.5000\n",
      "[Batch 240/501] Loss: 1.8145, Batch Acc: 0.3750\n",
      "[Batch 250/501] Loss: 1.8761, Batch Acc: 0.1250\n",
      "[Batch 260/501] Loss: 1.6641, Batch Acc: 0.2500\n",
      "[Batch 270/501] Loss: 1.8139, Batch Acc: 0.5000\n",
      "[Batch 280/501] Loss: 1.7253, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 1.6037, Batch Acc: 0.3750\n",
      "[Batch 300/501] Loss: 1.5901, Batch Acc: 0.3750\n",
      "[Batch 310/501] Loss: 1.4842, Batch Acc: 0.1250\n",
      "[Batch 320/501] Loss: 1.5093, Batch Acc: 0.3750\n",
      "[Batch 330/501] Loss: 1.6593, Batch Acc: 0.5000\n",
      "[Batch 340/501] Loss: 1.6736, Batch Acc: 0.1250\n",
      "[Batch 350/501] Loss: 1.3886, Batch Acc: 0.3750\n",
      "[Batch 360/501] Loss: 1.7632, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 1.7703, Batch Acc: 0.0000\n",
      "[Batch 380/501] Loss: 1.4605, Batch Acc: 0.3750\n",
      "[Batch 390/501] Loss: 1.5426, Batch Acc: 0.2500\n",
      "[Batch 400/501] Loss: 1.7237, Batch Acc: 0.1250\n",
      "[Batch 410/501] Loss: 1.8447, Batch Acc: 0.1250\n",
      "[Batch 420/501] Loss: 2.0954, Batch Acc: 0.0000\n",
      "[Batch 430/501] Loss: 1.6586, Batch Acc: 0.3750\n",
      "[Batch 440/501] Loss: 1.5114, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.4820, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.6398, Batch Acc: 0.1250\n",
      "[Batch 470/501] Loss: 1.9916, Batch Acc: 0.0000\n",
      "[Batch 480/501] Loss: 1.7899, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.4951, Batch Acc: 0.2500\n",
      "[Batch 500/501] Loss: 1.6303, Batch Acc: 0.2500\n",
      "Epoch 1 Summary - Loss: 817.8923, Train Accuracy: 0.2670\n",
      "[Batch 10/501] Loss: 1.3572, Batch Acc: 0.3750\n",
      "[Batch 20/501] Loss: 1.5287, Batch Acc: 0.3750\n",
      "[Batch 30/501] Loss: 1.5258, Batch Acc: 0.3750\n",
      "[Batch 40/501] Loss: 1.8915, Batch Acc: 0.0000\n",
      "[Batch 50/501] Loss: 1.3667, Batch Acc: 0.5000\n",
      "[Batch 60/501] Loss: 1.8116, Batch Acc: 0.0000\n",
      "[Batch 70/501] Loss: 1.8485, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.5426, Batch Acc: 0.1250\n",
      "[Batch 90/501] Loss: 1.7642, Batch Acc: 0.2500\n",
      "[Batch 100/501] Loss: 1.5894, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 1.5052, Batch Acc: 0.2500\n",
      "[Batch 120/501] Loss: 1.9406, Batch Acc: 0.0000\n",
      "[Batch 130/501] Loss: 1.5761, Batch Acc: 0.1250\n",
      "[Batch 140/501] Loss: 1.5650, Batch Acc: 0.2500\n",
      "[Batch 150/501] Loss: 1.3834, Batch Acc: 0.1250\n",
      "[Batch 160/501] Loss: 1.7360, Batch Acc: 0.0000\n",
      "[Batch 170/501] Loss: 1.8058, Batch Acc: 0.2500\n",
      "[Batch 180/501] Loss: 1.7951, Batch Acc: 0.1250\n",
      "[Batch 190/501] Loss: 1.8929, Batch Acc: 0.1250\n",
      "[Batch 200/501] Loss: 1.7144, Batch Acc: 0.2500\n",
      "[Batch 210/501] Loss: 1.6103, Batch Acc: 0.2500\n",
      "[Batch 220/501] Loss: 1.5998, Batch Acc: 0.3750\n",
      "[Batch 230/501] Loss: 1.5038, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.8733, Batch Acc: 0.1250\n",
      "[Batch 250/501] Loss: 1.8274, Batch Acc: 0.2500\n",
      "[Batch 260/501] Loss: 1.5916, Batch Acc: 0.3750\n",
      "[Batch 270/501] Loss: 1.6252, Batch Acc: 0.1250\n",
      "[Batch 280/501] Loss: 1.6147, Batch Acc: 0.5000\n",
      "[Batch 290/501] Loss: 1.6698, Batch Acc: 0.1250\n",
      "[Batch 300/501] Loss: 1.5720, Batch Acc: 0.3750\n",
      "[Batch 310/501] Loss: 1.7406, Batch Acc: 0.3750\n",
      "[Batch 320/501] Loss: 1.5185, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 1.5481, Batch Acc: 0.3750\n",
      "[Batch 340/501] Loss: 1.5733, Batch Acc: 0.3750\n",
      "[Batch 350/501] Loss: 1.5517, Batch Acc: 0.2500\n",
      "[Batch 360/501] Loss: 1.7001, Batch Acc: 0.1250\n",
      "[Batch 370/501] Loss: 1.4095, Batch Acc: 0.3750\n",
      "[Batch 380/501] Loss: 1.4673, Batch Acc: 0.3750\n",
      "[Batch 390/501] Loss: 1.5353, Batch Acc: 0.5000\n",
      "[Batch 400/501] Loss: 1.5296, Batch Acc: 0.2500\n",
      "[Batch 410/501] Loss: 1.8324, Batch Acc: 0.3750\n",
      "[Batch 420/501] Loss: 1.9070, Batch Acc: 0.2500\n",
      "[Batch 430/501] Loss: 1.6282, Batch Acc: 0.1250\n",
      "[Batch 440/501] Loss: 1.4941, Batch Acc: 0.3750\n",
      "[Batch 450/501] Loss: 1.3470, Batch Acc: 0.5000\n",
      "[Batch 460/501] Loss: 1.7206, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.4848, Batch Acc: 0.5000\n",
      "[Batch 480/501] Loss: 1.6460, Batch Acc: 0.3750\n",
      "[Batch 490/501] Loss: 1.5007, Batch Acc: 0.5000\n",
      "[Batch 500/501] Loss: 1.5961, Batch Acc: 0.6250\n",
      "Epoch 2 Summary - Loss: 817.0637, Train Accuracy: 0.2595\n",
      "[Batch 10/501] Loss: 1.7092, Batch Acc: 0.2500\n",
      "[Batch 20/501] Loss: 1.5264, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.5180, Batch Acc: 0.3750\n",
      "[Batch 40/501] Loss: 1.7562, Batch Acc: 0.0000\n",
      "[Batch 50/501] Loss: 1.5428, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.7749, Batch Acc: 0.0000\n",
      "[Batch 70/501] Loss: 2.0208, Batch Acc: 0.0000\n",
      "[Batch 80/501] Loss: 1.5459, Batch Acc: 0.6250\n",
      "[Batch 90/501] Loss: 1.5238, Batch Acc: 0.3750\n",
      "[Batch 100/501] Loss: 1.9320, Batch Acc: 0.0000\n",
      "[Batch 110/501] Loss: 1.6476, Batch Acc: 0.3750\n",
      "[Batch 120/501] Loss: 1.7474, Batch Acc: 0.2500\n",
      "[Batch 130/501] Loss: 1.6278, Batch Acc: 0.2500\n",
      "[Batch 140/501] Loss: 1.6664, Batch Acc: 0.1250\n",
      "[Batch 150/501] Loss: 1.7589, Batch Acc: 0.2500\n",
      "[Batch 160/501] Loss: 1.6156, Batch Acc: 0.1250\n",
      "[Batch 170/501] Loss: 1.4503, Batch Acc: 0.2500\n",
      "[Batch 180/501] Loss: 1.9064, Batch Acc: 0.0000\n",
      "[Batch 190/501] Loss: 1.5059, Batch Acc: 0.5000\n",
      "[Batch 200/501] Loss: 1.5832, Batch Acc: 0.2500\n",
      "[Batch 210/501] Loss: 1.8276, Batch Acc: 0.1250\n",
      "[Batch 220/501] Loss: 1.4242, Batch Acc: 0.2500\n",
      "[Batch 230/501] Loss: 1.6264, Batch Acc: 0.1250\n",
      "[Batch 240/501] Loss: 1.5479, Batch Acc: 0.2500\n",
      "[Batch 250/501] Loss: 1.6198, Batch Acc: 0.2500\n",
      "[Batch 260/501] Loss: 1.6722, Batch Acc: 0.2500\n",
      "[Batch 270/501] Loss: 1.4259, Batch Acc: 0.3750\n",
      "[Batch 280/501] Loss: 1.5734, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 1.6715, Batch Acc: 0.3750\n",
      "[Batch 300/501] Loss: 1.9243, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 1.5828, Batch Acc: 0.2500\n",
      "[Batch 320/501] Loss: 1.7839, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 1.3915, Batch Acc: 0.5000\n",
      "[Batch 340/501] Loss: 1.4489, Batch Acc: 0.3750\n",
      "[Batch 350/501] Loss: 1.6557, Batch Acc: 0.0000\n",
      "[Batch 360/501] Loss: 1.6379, Batch Acc: 0.5000\n",
      "[Batch 370/501] Loss: 1.6795, Batch Acc: 0.2500\n",
      "[Batch 380/501] Loss: 1.5105, Batch Acc: 0.3750\n",
      "[Batch 390/501] Loss: 1.6373, Batch Acc: 0.0000\n",
      "[Batch 400/501] Loss: 1.5130, Batch Acc: 0.3750\n",
      "[Batch 410/501] Loss: 1.7078, Batch Acc: 0.3750\n",
      "[Batch 420/501] Loss: 1.4542, Batch Acc: 0.5000\n",
      "[Batch 430/501] Loss: 1.9404, Batch Acc: 0.0000\n",
      "[Batch 440/501] Loss: 1.6964, Batch Acc: 0.5000\n",
      "[Batch 450/501] Loss: 1.6340, Batch Acc: 0.1250\n",
      "[Batch 460/501] Loss: 1.7662, Batch Acc: 0.1250\n",
      "[Batch 470/501] Loss: 1.6852, Batch Acc: 0.1250\n",
      "[Batch 480/501] Loss: 1.5684, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.8233, Batch Acc: 0.3750\n",
      "[Batch 500/501] Loss: 1.9067, Batch Acc: 0.0000\n",
      "Epoch 3 Summary - Loss: 815.5887, Train Accuracy: 0.2517\n",
      "[Batch 10/501] Loss: 1.6752, Batch Acc: 0.3750\n",
      "[Batch 20/501] Loss: 1.3129, Batch Acc: 0.5000\n",
      "[Batch 30/501] Loss: 1.5080, Batch Acc: 0.5000\n",
      "[Batch 40/501] Loss: 1.7404, Batch Acc: 0.2500\n",
      "[Batch 50/501] Loss: 1.9123, Batch Acc: 0.1250\n",
      "[Batch 60/501] Loss: 1.4549, Batch Acc: 0.3750\n",
      "[Batch 70/501] Loss: 1.7051, Batch Acc: 0.1250\n",
      "[Batch 80/501] Loss: 1.4386, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.4069, Batch Acc: 0.5000\n",
      "[Batch 100/501] Loss: 1.5902, Batch Acc: 0.1250\n",
      "[Batch 110/501] Loss: 1.6985, Batch Acc: 0.2500\n",
      "[Batch 120/501] Loss: 1.7757, Batch Acc: 0.2500\n",
      "[Batch 130/501] Loss: 1.5746, Batch Acc: 0.2500\n",
      "[Batch 140/501] Loss: 1.7893, Batch Acc: 0.0000\n",
      "[Batch 150/501] Loss: 1.7290, Batch Acc: 0.2500\n",
      "[Batch 160/501] Loss: 1.4424, Batch Acc: 0.5000\n",
      "[Batch 170/501] Loss: 1.7132, Batch Acc: 0.2500\n",
      "[Batch 180/501] Loss: 1.8967, Batch Acc: 0.0000\n",
      "[Batch 190/501] Loss: 1.5081, Batch Acc: 0.2500\n",
      "[Batch 200/501] Loss: 1.4219, Batch Acc: 0.3750\n",
      "[Batch 210/501] Loss: 1.3123, Batch Acc: 0.3750\n",
      "[Batch 220/501] Loss: 1.7963, Batch Acc: 0.0000\n",
      "[Batch 230/501] Loss: 1.4234, Batch Acc: 0.3750\n",
      "[Batch 240/501] Loss: 1.5063, Batch Acc: 0.3750\n",
      "[Batch 250/501] Loss: 1.9935, Batch Acc: 0.0000\n",
      "[Batch 260/501] Loss: 1.7864, Batch Acc: 0.3750\n",
      "[Batch 270/501] Loss: 1.7475, Batch Acc: 0.0000\n",
      "[Batch 280/501] Loss: 1.3878, Batch Acc: 0.5000\n",
      "[Batch 290/501] Loss: 1.5799, Batch Acc: 0.3750\n",
      "[Batch 300/501] Loss: 1.5376, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 1.9193, Batch Acc: 0.0000\n",
      "[Batch 320/501] Loss: 1.7819, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 1.5727, Batch Acc: 0.3750\n",
      "[Batch 340/501] Loss: 1.4137, Batch Acc: 0.3750\n",
      "[Batch 350/501] Loss: 1.5762, Batch Acc: 0.2500\n",
      "[Batch 360/501] Loss: 1.6883, Batch Acc: 0.3750\n",
      "[Batch 370/501] Loss: 1.6661, Batch Acc: 0.2500\n",
      "[Batch 380/501] Loss: 1.5114, Batch Acc: 0.1250\n",
      "[Batch 390/501] Loss: 1.6636, Batch Acc: 0.1250\n",
      "[Batch 400/501] Loss: 1.5187, Batch Acc: 0.3750\n",
      "[Batch 410/501] Loss: 1.7856, Batch Acc: 0.1250\n",
      "[Batch 420/501] Loss: 1.8322, Batch Acc: 0.2500\n",
      "[Batch 430/501] Loss: 1.3237, Batch Acc: 0.6250\n",
      "[Batch 440/501] Loss: 1.6960, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.3879, Batch Acc: 0.5000\n",
      "[Batch 460/501] Loss: 1.5976, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.5516, Batch Acc: 0.5000\n",
      "[Batch 480/501] Loss: 1.8794, Batch Acc: 0.0000\n",
      "[Batch 490/501] Loss: 1.6269, Batch Acc: 0.2500\n",
      "[Batch 500/501] Loss: 1.8683, Batch Acc: 0.1250\n",
      "Epoch 4 Summary - Loss: 817.6204, Train Accuracy: 0.2605\n",
      "[Batch 10/501] Loss: 1.9381, Batch Acc: 0.1250\n",
      "[Batch 20/501] Loss: 1.6619, Batch Acc: 0.3750\n",
      "[Batch 30/501] Loss: 1.7598, Batch Acc: 0.2500\n",
      "[Batch 40/501] Loss: 1.5264, Batch Acc: 0.2500\n",
      "[Batch 50/501] Loss: 1.3445, Batch Acc: 0.3750\n",
      "[Batch 60/501] Loss: 1.7729, Batch Acc: 0.1250\n",
      "[Batch 70/501] Loss: 1.5781, Batch Acc: 0.7500\n",
      "[Batch 80/501] Loss: 1.5891, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.5531, Batch Acc: 0.2500\n",
      "[Batch 100/501] Loss: 1.5511, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 1.7372, Batch Acc: 0.3750\n",
      "[Batch 120/501] Loss: 1.6327, Batch Acc: 0.2500\n",
      "[Batch 130/501] Loss: 1.6307, Batch Acc: 0.1250\n",
      "[Batch 140/501] Loss: 1.7197, Batch Acc: 0.1250\n",
      "[Batch 150/501] Loss: 1.7729, Batch Acc: 0.2500\n",
      "[Batch 160/501] Loss: 1.7595, Batch Acc: 0.1250\n",
      "[Batch 170/501] Loss: 1.7077, Batch Acc: 0.2500\n",
      "[Batch 180/501] Loss: 1.7777, Batch Acc: 0.0000\n",
      "[Batch 190/501] Loss: 1.7820, Batch Acc: 0.0000\n",
      "[Batch 200/501] Loss: 1.6886, Batch Acc: 0.5000\n",
      "[Batch 210/501] Loss: 1.9168, Batch Acc: 0.0000\n",
      "[Batch 220/501] Loss: 1.6354, Batch Acc: 0.0000\n",
      "[Batch 230/501] Loss: 1.4369, Batch Acc: 0.3750\n",
      "[Batch 240/501] Loss: 1.5698, Batch Acc: 0.1250\n",
      "[Batch 250/501] Loss: 1.7423, Batch Acc: 0.1250\n",
      "[Batch 260/501] Loss: 1.6495, Batch Acc: 0.2500\n",
      "[Batch 270/501] Loss: 1.6149, Batch Acc: 0.2500\n",
      "[Batch 280/501] Loss: 1.7461, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 1.4856, Batch Acc: 0.2500\n",
      "[Batch 300/501] Loss: 1.4946, Batch Acc: 0.3750\n",
      "[Batch 310/501] Loss: 2.0178, Batch Acc: 0.0000\n",
      "[Batch 320/501] Loss: 1.8338, Batch Acc: 0.0000\n",
      "[Batch 330/501] Loss: 1.8968, Batch Acc: 0.0000\n",
      "[Batch 340/501] Loss: 1.6503, Batch Acc: 0.2500\n",
      "[Batch 350/501] Loss: 1.2162, Batch Acc: 0.6250\n",
      "[Batch 360/501] Loss: 1.5650, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 1.5554, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.6046, Batch Acc: 0.3750\n",
      "[Batch 390/501] Loss: 1.8596, Batch Acc: 0.1250\n",
      "[Batch 400/501] Loss: 1.4666, Batch Acc: 0.3750\n",
      "[Batch 410/501] Loss: 1.5129, Batch Acc: 0.3750\n",
      "[Batch 420/501] Loss: 1.5138, Batch Acc: 0.3750\n",
      "[Batch 430/501] Loss: 1.6807, Batch Acc: 0.2500\n",
      "[Batch 440/501] Loss: 1.5343, Batch Acc: 0.5000\n",
      "[Batch 450/501] Loss: 1.5809, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.5760, Batch Acc: 0.3750\n",
      "[Batch 470/501] Loss: 1.6598, Batch Acc: 0.3750\n",
      "[Batch 480/501] Loss: 1.3315, Batch Acc: 0.5000\n",
      "[Batch 490/501] Loss: 1.6140, Batch Acc: 0.2500\n",
      "[Batch 500/501] Loss: 1.6210, Batch Acc: 0.3750\n",
      "Epoch 5 Summary - Loss: 815.7310, Train Accuracy: 0.2517\n",
      "[Batch 10/501] Loss: 1.6921, Batch Acc: 0.1250\n",
      "[Batch 20/501] Loss: 1.6801, Batch Acc: 0.3750\n",
      "[Batch 30/501] Loss: 1.8738, Batch Acc: 0.2500\n",
      "[Batch 40/501] Loss: 1.5510, Batch Acc: 0.2500\n",
      "[Batch 50/501] Loss: 1.5590, Batch Acc: 0.3750\n",
      "[Batch 60/501] Loss: 1.4386, Batch Acc: 0.3750\n",
      "[Batch 70/501] Loss: 1.4823, Batch Acc: 0.3750\n",
      "[Batch 80/501] Loss: 1.5986, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.6366, Batch Acc: 0.2500\n",
      "[Batch 100/501] Loss: 1.8663, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 1.9893, Batch Acc: 0.1250\n",
      "[Batch 120/501] Loss: 1.8828, Batch Acc: 0.0000\n",
      "[Batch 130/501] Loss: 1.6743, Batch Acc: 0.2500\n",
      "[Batch 140/501] Loss: 1.3366, Batch Acc: 0.3750\n",
      "[Batch 150/501] Loss: 1.7393, Batch Acc: 0.0000\n",
      "[Batch 160/501] Loss: 1.7910, Batch Acc: 0.2500\n",
      "[Batch 170/501] Loss: 1.6679, Batch Acc: 0.2500\n",
      "[Batch 180/501] Loss: 1.4919, Batch Acc: 0.2500\n",
      "[Batch 190/501] Loss: 1.6285, Batch Acc: 0.1250\n",
      "[Batch 200/501] Loss: 1.8109, Batch Acc: 0.2500\n",
      "[Batch 210/501] Loss: 1.5257, Batch Acc: 0.1250\n",
      "[Batch 220/501] Loss: 1.6340, Batch Acc: 0.1250\n",
      "[Batch 230/501] Loss: 1.5417, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.7591, Batch Acc: 0.3750\n",
      "[Batch 250/501] Loss: 1.5609, Batch Acc: 0.0000\n",
      "[Batch 260/501] Loss: 1.2912, Batch Acc: 0.6250\n",
      "[Batch 270/501] Loss: 1.6190, Batch Acc: 0.2500\n",
      "[Batch 280/501] Loss: 1.3840, Batch Acc: 0.6250\n",
      "[Batch 290/501] Loss: 1.5532, Batch Acc: 0.5000\n",
      "[Batch 300/501] Loss: 1.4517, Batch Acc: 0.3750\n",
      "[Batch 310/501] Loss: 1.5909, Batch Acc: 0.5000\n",
      "[Batch 320/501] Loss: 1.4952, Batch Acc: 0.3750\n",
      "[Batch 330/501] Loss: 1.7529, Batch Acc: 0.2500\n",
      "[Batch 340/501] Loss: 1.7759, Batch Acc: 0.3750\n",
      "[Batch 350/501] Loss: 1.8612, Batch Acc: 0.1250\n",
      "[Batch 360/501] Loss: 1.5454, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 1.4404, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.6278, Batch Acc: 0.2500\n",
      "[Batch 390/501] Loss: 1.4796, Batch Acc: 0.3750\n",
      "[Batch 400/501] Loss: 1.7696, Batch Acc: 0.2500\n",
      "[Batch 410/501] Loss: 1.7804, Batch Acc: 0.1250\n",
      "[Batch 420/501] Loss: 1.6669, Batch Acc: 0.2500\n",
      "[Batch 430/501] Loss: 1.5959, Batch Acc: 0.3750\n",
      "[Batch 440/501] Loss: 1.7365, Batch Acc: 0.1250\n",
      "[Batch 450/501] Loss: 1.8182, Batch Acc: 0.1250\n",
      "[Batch 460/501] Loss: 1.6699, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.4984, Batch Acc: 0.3750\n",
      "[Batch 480/501] Loss: 1.7108, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.9780, Batch Acc: 0.1250\n",
      "[Batch 500/501] Loss: 1.8632, Batch Acc: 0.1250\n",
      "Epoch 6 Summary - Loss: 816.3847, Train Accuracy: 0.2647\n",
      "[Batch 10/501] Loss: 1.7015, Batch Acc: 0.2500\n",
      "[Batch 20/501] Loss: 1.5709, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.7374, Batch Acc: 0.3750\n",
      "[Batch 40/501] Loss: 1.6864, Batch Acc: 0.1250\n",
      "[Batch 50/501] Loss: 1.4712, Batch Acc: 0.5000\n",
      "[Batch 60/501] Loss: 1.5702, Batch Acc: 0.2500\n",
      "[Batch 70/501] Loss: 1.8289, Batch Acc: 0.0000\n",
      "[Batch 80/501] Loss: 1.3943, Batch Acc: 0.6250\n",
      "[Batch 90/501] Loss: 1.7429, Batch Acc: 0.0000\n",
      "[Batch 100/501] Loss: 1.7964, Batch Acc: 0.3750\n",
      "[Batch 110/501] Loss: 1.4375, Batch Acc: 0.3750\n",
      "[Batch 120/501] Loss: 1.7878, Batch Acc: 0.1250\n",
      "[Batch 130/501] Loss: 1.5463, Batch Acc: 0.3750\n",
      "[Batch 140/501] Loss: 1.5295, Batch Acc: 0.3750\n",
      "[Batch 150/501] Loss: 1.5442, Batch Acc: 0.3750\n",
      "[Batch 160/501] Loss: 1.5405, Batch Acc: 0.2500\n",
      "[Batch 170/501] Loss: 1.7720, Batch Acc: 0.2500\n",
      "[Batch 180/501] Loss: 1.7303, Batch Acc: 0.1250\n",
      "[Batch 190/501] Loss: 1.9242, Batch Acc: 0.1250\n",
      "[Batch 200/501] Loss: 1.6983, Batch Acc: 0.2500\n",
      "[Batch 210/501] Loss: 1.7816, Batch Acc: 0.2500\n",
      "[Batch 220/501] Loss: 1.4923, Batch Acc: 0.2500\n",
      "[Batch 230/501] Loss: 1.7637, Batch Acc: 0.1250\n",
      "[Batch 240/501] Loss: 1.5994, Batch Acc: 0.2500\n",
      "[Batch 250/501] Loss: 1.5972, Batch Acc: 0.3750\n",
      "[Batch 260/501] Loss: 1.4200, Batch Acc: 0.3750\n",
      "[Batch 270/501] Loss: 1.4650, Batch Acc: 0.3750\n",
      "[Batch 280/501] Loss: 1.6469, Batch Acc: 0.3750\n",
      "[Batch 290/501] Loss: 1.5537, Batch Acc: 0.2500\n",
      "[Batch 300/501] Loss: 1.8936, Batch Acc: 0.1250\n",
      "[Batch 310/501] Loss: 1.5455, Batch Acc: 0.5000\n",
      "[Batch 320/501] Loss: 1.6620, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 1.6686, Batch Acc: 0.1250\n",
      "[Batch 340/501] Loss: 1.6131, Batch Acc: 0.2500\n",
      "[Batch 350/501] Loss: 1.1802, Batch Acc: 0.6250\n",
      "[Batch 360/501] Loss: 1.6064, Batch Acc: 0.1250\n",
      "[Batch 370/501] Loss: 1.8204, Batch Acc: 0.0000\n",
      "[Batch 380/501] Loss: 1.6302, Batch Acc: 0.2500\n",
      "[Batch 390/501] Loss: 1.3544, Batch Acc: 0.3750\n",
      "[Batch 400/501] Loss: 1.6810, Batch Acc: 0.2500\n",
      "[Batch 410/501] Loss: 1.7333, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.6111, Batch Acc: 0.2500\n",
      "[Batch 430/501] Loss: 1.6565, Batch Acc: 0.3750\n",
      "[Batch 440/501] Loss: 1.4781, Batch Acc: 0.3750\n",
      "[Batch 450/501] Loss: 1.6257, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.5545, Batch Acc: 0.3750\n",
      "[Batch 470/501] Loss: 1.8145, Batch Acc: 0.2500\n",
      "[Batch 480/501] Loss: 1.8975, Batch Acc: 0.0000\n",
      "[Batch 490/501] Loss: 1.8398, Batch Acc: 0.1250\n",
      "[Batch 500/501] Loss: 1.8267, Batch Acc: 0.1250\n",
      "Epoch 7 Summary - Loss: 815.9869, Train Accuracy: 0.2637\n",
      "[Batch 10/501] Loss: 1.4065, Batch Acc: 0.5000\n",
      "[Batch 20/501] Loss: 1.6288, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 2.0216, Batch Acc: 0.0000\n",
      "[Batch 40/501] Loss: 1.7603, Batch Acc: 0.0000\n",
      "[Batch 50/501] Loss: 1.5376, Batch Acc: 0.3750\n",
      "[Batch 60/501] Loss: 1.4984, Batch Acc: 0.2500\n",
      "[Batch 70/501] Loss: 1.6927, Batch Acc: 0.1250\n",
      "[Batch 80/501] Loss: 1.5546, Batch Acc: 0.5000\n",
      "[Batch 90/501] Loss: 1.6740, Batch Acc: 0.1250\n",
      "[Batch 100/501] Loss: 2.0175, Batch Acc: 0.0000\n",
      "[Batch 110/501] Loss: 1.5941, Batch Acc: 0.0000\n",
      "[Batch 120/501] Loss: 1.5234, Batch Acc: 0.2500\n",
      "[Batch 130/501] Loss: 1.5750, Batch Acc: 0.2500\n",
      "[Batch 140/501] Loss: 1.5563, Batch Acc: 0.2500\n",
      "[Batch 150/501] Loss: 1.7807, Batch Acc: 0.1250\n",
      "[Batch 160/501] Loss: 1.7326, Batch Acc: 0.1250\n",
      "[Batch 170/501] Loss: 1.5314, Batch Acc: 0.2500\n",
      "[Batch 180/501] Loss: 1.5053, Batch Acc: 0.3750\n",
      "[Batch 190/501] Loss: 1.6897, Batch Acc: 0.1250\n",
      "[Batch 200/501] Loss: 1.7042, Batch Acc: 0.2500\n",
      "[Batch 210/501] Loss: 1.3772, Batch Acc: 0.5000\n",
      "[Batch 220/501] Loss: 1.4194, Batch Acc: 0.6250\n",
      "[Batch 230/501] Loss: 1.3327, Batch Acc: 0.3750\n",
      "[Batch 240/501] Loss: 1.6912, Batch Acc: 0.0000\n",
      "[Batch 250/501] Loss: 1.3424, Batch Acc: 0.6250\n",
      "[Batch 260/501] Loss: 1.5987, Batch Acc: 0.3750\n",
      "[Batch 270/501] Loss: 1.5294, Batch Acc: 0.3750\n",
      "[Batch 280/501] Loss: 1.7224, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 1.3570, Batch Acc: 0.3750\n",
      "[Batch 300/501] Loss: 1.4981, Batch Acc: 0.3750\n",
      "[Batch 310/501] Loss: 1.7232, Batch Acc: 0.3750\n",
      "[Batch 320/501] Loss: 1.3729, Batch Acc: 0.6250\n",
      "[Batch 330/501] Loss: 1.8082, Batch Acc: 0.2500\n",
      "[Batch 340/501] Loss: 1.5769, Batch Acc: 0.2500\n",
      "[Batch 350/501] Loss: 1.5622, Batch Acc: 0.2500\n",
      "[Batch 360/501] Loss: 1.8074, Batch Acc: 0.1250\n",
      "[Batch 370/501] Loss: 1.6179, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.7408, Batch Acc: 0.2500\n",
      "[Batch 390/501] Loss: 1.4693, Batch Acc: 0.5000\n",
      "[Batch 400/501] Loss: 1.7372, Batch Acc: 0.1250\n",
      "[Batch 410/501] Loss: 1.4323, Batch Acc: 0.3750\n",
      "[Batch 420/501] Loss: 1.4110, Batch Acc: 0.2500\n",
      "[Batch 430/501] Loss: 1.5516, Batch Acc: 0.3750\n",
      "[Batch 440/501] Loss: 1.7410, Batch Acc: 0.1250\n",
      "[Batch 450/501] Loss: 1.7314, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.5319, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.4568, Batch Acc: 0.6250\n",
      "[Batch 480/501] Loss: 1.5798, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.8433, Batch Acc: 0.1250\n",
      "[Batch 500/501] Loss: 1.3711, Batch Acc: 0.5000\n",
      "Epoch 8 Summary - Loss: 816.9015, Train Accuracy: 0.2620\n",
      "[Batch 10/501] Loss: 1.5071, Batch Acc: 0.2500\n",
      "[Batch 20/501] Loss: 1.4595, Batch Acc: 0.3750\n",
      "[Batch 30/501] Loss: 1.6898, Batch Acc: 0.2500\n",
      "[Batch 40/501] Loss: 1.6682, Batch Acc: 0.0000\n",
      "[Batch 50/501] Loss: 1.5334, Batch Acc: 0.0000\n",
      "[Batch 60/501] Loss: 1.5099, Batch Acc: 0.5000\n",
      "[Batch 70/501] Loss: 1.5902, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.6986, Batch Acc: 0.1250\n",
      "[Batch 90/501] Loss: 1.4380, Batch Acc: 0.3750\n",
      "[Batch 100/501] Loss: 1.4664, Batch Acc: 0.3750\n",
      "[Batch 110/501] Loss: 1.6494, Batch Acc: 0.0000\n",
      "[Batch 120/501] Loss: 1.6894, Batch Acc: 0.1250\n",
      "[Batch 130/501] Loss: 1.4385, Batch Acc: 0.3750\n",
      "[Batch 140/501] Loss: 1.7369, Batch Acc: 0.1250\n",
      "[Batch 150/501] Loss: 1.4073, Batch Acc: 0.5000\n",
      "[Batch 160/501] Loss: 1.7515, Batch Acc: 0.1250\n",
      "[Batch 170/501] Loss: 1.4871, Batch Acc: 0.2500\n",
      "[Batch 180/501] Loss: 1.5869, Batch Acc: 0.3750\n",
      "[Batch 190/501] Loss: 1.9581, Batch Acc: 0.1250\n",
      "[Batch 200/501] Loss: 1.6864, Batch Acc: 0.1250\n",
      "[Batch 210/501] Loss: 1.7354, Batch Acc: 0.2500\n",
      "[Batch 220/501] Loss: 1.5788, Batch Acc: 0.2500\n",
      "[Batch 230/501] Loss: 1.5609, Batch Acc: 0.3750\n",
      "[Batch 240/501] Loss: 1.6236, Batch Acc: 0.2500\n",
      "[Batch 250/501] Loss: 1.6474, Batch Acc: 0.2500\n",
      "[Batch 260/501] Loss: 1.8942, Batch Acc: 0.2500\n",
      "[Batch 270/501] Loss: 1.4919, Batch Acc: 0.5000\n",
      "[Batch 280/501] Loss: 1.6106, Batch Acc: 0.2500\n",
      "[Batch 290/501] Loss: 1.7421, Batch Acc: 0.3750\n",
      "[Batch 300/501] Loss: 1.6238, Batch Acc: 0.5000\n",
      "[Batch 310/501] Loss: 1.8025, Batch Acc: 0.0000\n",
      "[Batch 320/501] Loss: 1.9996, Batch Acc: 0.0000\n",
      "[Batch 330/501] Loss: 1.6451, Batch Acc: 0.3750\n",
      "[Batch 340/501] Loss: 1.7265, Batch Acc: 0.1250\n",
      "[Batch 350/501] Loss: 1.6559, Batch Acc: 0.2500\n",
      "[Batch 360/501] Loss: 1.4682, Batch Acc: 0.3750\n",
      "[Batch 370/501] Loss: 1.6754, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.3611, Batch Acc: 0.5000\n",
      "[Batch 390/501] Loss: 1.2981, Batch Acc: 0.3750\n",
      "[Batch 400/501] Loss: 1.6965, Batch Acc: 0.2500\n",
      "[Batch 410/501] Loss: 1.7679, Batch Acc: 0.0000\n",
      "[Batch 420/501] Loss: 1.5496, Batch Acc: 0.2500\n",
      "[Batch 430/501] Loss: 1.9886, Batch Acc: 0.0000\n",
      "[Batch 440/501] Loss: 1.7372, Batch Acc: 0.1250\n",
      "[Batch 450/501] Loss: 1.6865, Batch Acc: 0.1250\n",
      "[Batch 460/501] Loss: 1.8407, Batch Acc: 0.1250\n",
      "[Batch 470/501] Loss: 1.4448, Batch Acc: 0.2500\n",
      "[Batch 480/501] Loss: 1.5883, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.6669, Batch Acc: 0.0000\n",
      "[Batch 500/501] Loss: 1.6887, Batch Acc: 0.1250\n",
      "Epoch 9 Summary - Loss: 818.8081, Train Accuracy: 0.2535\n",
      "[Batch 10/501] Loss: 1.6311, Batch Acc: 0.2500\n",
      "[Batch 20/501] Loss: 1.5543, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.6105, Batch Acc: 0.1250\n",
      "[Batch 40/501] Loss: 1.5803, Batch Acc: 0.1250\n",
      "[Batch 50/501] Loss: 1.4006, Batch Acc: 0.5000\n",
      "[Batch 60/501] Loss: 1.7123, Batch Acc: 0.2500\n",
      "[Batch 70/501] Loss: 1.2994, Batch Acc: 0.5000\n",
      "[Batch 80/501] Loss: 1.4702, Batch Acc: 0.3750\n",
      "[Batch 90/501] Loss: 1.7144, Batch Acc: 0.3750\n",
      "[Batch 100/501] Loss: 1.6436, Batch Acc: 0.1250\n",
      "[Batch 110/501] Loss: 1.6378, Batch Acc: 0.2500\n",
      "[Batch 120/501] Loss: 1.4649, Batch Acc: 0.3750\n",
      "[Batch 130/501] Loss: 1.5134, Batch Acc: 0.2500\n",
      "[Batch 140/501] Loss: 1.5507, Batch Acc: 0.3750\n",
      "[Batch 150/501] Loss: 1.7366, Batch Acc: 0.2500\n",
      "[Batch 160/501] Loss: 1.4650, Batch Acc: 0.2500\n",
      "[Batch 170/501] Loss: 1.4840, Batch Acc: 0.5000\n",
      "[Batch 180/501] Loss: 1.5934, Batch Acc: 0.2500\n",
      "[Batch 190/501] Loss: 1.5490, Batch Acc: 0.3750\n",
      "[Batch 200/501] Loss: 1.6509, Batch Acc: 0.1250\n",
      "[Batch 210/501] Loss: 1.6538, Batch Acc: 0.2500\n",
      "[Batch 220/501] Loss: 1.5468, Batch Acc: 0.5000\n",
      "[Batch 230/501] Loss: 1.7455, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.8637, Batch Acc: 0.0000\n",
      "[Batch 250/501] Loss: 1.5190, Batch Acc: 0.3750\n",
      "[Batch 260/501] Loss: 1.7176, Batch Acc: 0.2500\n",
      "[Batch 270/501] Loss: 1.5101, Batch Acc: 0.3750\n",
      "[Batch 280/501] Loss: 1.2729, Batch Acc: 0.5000\n",
      "[Batch 290/501] Loss: 1.8845, Batch Acc: 0.0000\n",
      "[Batch 300/501] Loss: 1.5277, Batch Acc: 0.1250\n",
      "[Batch 310/501] Loss: 1.5364, Batch Acc: 0.0000\n",
      "[Batch 320/501] Loss: 1.4583, Batch Acc: 0.5000\n",
      "[Batch 330/501] Loss: 1.4155, Batch Acc: 0.1250\n",
      "[Batch 340/501] Loss: 1.0479, Batch Acc: 0.7500\n",
      "[Batch 350/501] Loss: 1.7164, Batch Acc: 0.5000\n",
      "[Batch 360/501] Loss: 1.6904, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 1.7074, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.6604, Batch Acc: 0.2500\n",
      "[Batch 390/501] Loss: 1.5383, Batch Acc: 0.1250\n",
      "[Batch 400/501] Loss: 1.6244, Batch Acc: 0.1250\n",
      "[Batch 410/501] Loss: 1.7793, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.4850, Batch Acc: 0.3750\n",
      "[Batch 430/501] Loss: 1.6340, Batch Acc: 0.5000\n",
      "[Batch 440/501] Loss: 1.5788, Batch Acc: 0.3750\n",
      "[Batch 450/501] Loss: 1.3140, Batch Acc: 0.6250\n",
      "[Batch 460/501] Loss: 1.6169, Batch Acc: 0.3750\n",
      "[Batch 470/501] Loss: 1.5675, Batch Acc: 0.3750\n",
      "[Batch 480/501] Loss: 1.7338, Batch Acc: 0.3750\n",
      "[Batch 490/501] Loss: 1.8833, Batch Acc: 0.1250\n",
      "[Batch 500/501] Loss: 1.6729, Batch Acc: 0.1250\n",
      "Epoch 10 Summary - Loss: 816.1901, Train Accuracy: 0.2567\n",
      "Test Accuracy: 0.2552\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_accuracy</td><td>█▅▁▅▁▇▇▆▂▃</td></tr><tr><td>train_loss</td><td>▆▄▁▅▁▃▂▄█▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>test_accuracy</td><td>0.25519</td></tr><tr><td>test_loss</td><td>205.99341</td></tr><tr><td>train_accuracy</td><td>0.25674</td></tr><tr><td>train_loss</td><td>816.19011</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">stellar-sweep-5</strong> at: <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/4srb7kkk' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/4srb7kkk</a><br> View project at: <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250615_140459-4srb7kkk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5q9f0pwo with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_nodes: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ajit.kumar4@happiestminds.com/Documents/Drackula/ComputerVisionFromScratch/wandb/run-20250615_140806-5q9f0pwo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/5q9f0pwo' target=\"_blank\">quiet-sweep-6</a></strong> to <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/5q9f0pwo' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/5q9f0pwo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 10/501] Loss: 1.8049, Batch Acc: 0.1250\n",
      "[Batch 20/501] Loss: 1.7502, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.6504, Batch Acc: 0.2500\n",
      "[Batch 40/501] Loss: 1.4986, Batch Acc: 0.1250\n",
      "[Batch 50/501] Loss: 1.7095, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.7779, Batch Acc: 0.2500\n",
      "[Batch 70/501] Loss: 1.5666, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.6604, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.6126, Batch Acc: 0.1250\n",
      "[Batch 100/501] Loss: 1.9952, Batch Acc: 0.1250\n",
      "[Batch 110/501] Loss: 1.8866, Batch Acc: 0.0000\n",
      "[Batch 120/501] Loss: 1.7429, Batch Acc: 0.1250\n",
      "[Batch 130/501] Loss: 1.5445, Batch Acc: 0.1250\n",
      "[Batch 140/501] Loss: 1.8936, Batch Acc: 0.2500\n",
      "[Batch 150/501] Loss: 1.3541, Batch Acc: 0.6250\n",
      "[Batch 160/501] Loss: 1.8114, Batch Acc: 0.1250\n",
      "[Batch 170/501] Loss: 1.9769, Batch Acc: 0.0000\n",
      "[Batch 180/501] Loss: 1.8138, Batch Acc: 0.3750\n",
      "[Batch 190/501] Loss: 1.7856, Batch Acc: 0.1250\n",
      "[Batch 200/501] Loss: 1.7110, Batch Acc: 0.1250\n",
      "[Batch 210/501] Loss: 1.3559, Batch Acc: 0.5000\n",
      "[Batch 220/501] Loss: 1.5126, Batch Acc: 0.5000\n",
      "[Batch 230/501] Loss: 1.7770, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.6407, Batch Acc: 0.1250\n",
      "[Batch 250/501] Loss: 1.7612, Batch Acc: 0.1250\n",
      "[Batch 260/501] Loss: 1.5952, Batch Acc: 0.3750\n",
      "[Batch 270/501] Loss: 1.3253, Batch Acc: 0.6250\n",
      "[Batch 280/501] Loss: 1.7944, Batch Acc: 0.0000\n",
      "[Batch 290/501] Loss: 1.7570, Batch Acc: 0.1250\n",
      "[Batch 300/501] Loss: 1.5730, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 1.6728, Batch Acc: 0.1250\n",
      "[Batch 320/501] Loss: 1.6500, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 1.5780, Batch Acc: 0.1250\n",
      "[Batch 340/501] Loss: 1.5273, Batch Acc: 0.2500\n",
      "[Batch 350/501] Loss: 1.7563, Batch Acc: 0.0000\n",
      "[Batch 360/501] Loss: 1.6734, Batch Acc: 0.1250\n",
      "[Batch 370/501] Loss: 1.5246, Batch Acc: 0.3750\n",
      "[Batch 380/501] Loss: 1.3411, Batch Acc: 0.5000\n",
      "[Batch 390/501] Loss: 1.7491, Batch Acc: 0.2500\n",
      "[Batch 400/501] Loss: 1.5926, Batch Acc: 0.2500\n",
      "[Batch 410/501] Loss: 1.5775, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.7942, Batch Acc: 0.2500\n",
      "[Batch 430/501] Loss: 1.8013, Batch Acc: 0.1250\n",
      "[Batch 440/501] Loss: 1.6339, Batch Acc: 0.1250\n",
      "[Batch 450/501] Loss: 1.7690, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.7217, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.4998, Batch Acc: 0.3750\n",
      "[Batch 480/501] Loss: 1.7851, Batch Acc: 0.1250\n",
      "[Batch 490/501] Loss: 1.8996, Batch Acc: 0.0000\n",
      "[Batch 500/501] Loss: 1.6436, Batch Acc: 0.2500\n",
      "Epoch 1 Summary - Loss: 834.5202, Train Accuracy: 0.2201\n",
      "[Batch 10/501] Loss: 1.7582, Batch Acc: 0.2500\n",
      "[Batch 20/501] Loss: 1.4481, Batch Acc: 0.5000\n",
      "[Batch 30/501] Loss: 1.5182, Batch Acc: 0.2500\n",
      "[Batch 40/501] Loss: 1.5399, Batch Acc: 0.5000\n",
      "[Batch 50/501] Loss: 1.8048, Batch Acc: 0.1250\n",
      "[Batch 60/501] Loss: 1.7172, Batch Acc: 0.1250\n",
      "[Batch 70/501] Loss: 1.6313, Batch Acc: 0.0000\n",
      "[Batch 80/501] Loss: 1.5719, Batch Acc: 0.3750\n",
      "[Batch 90/501] Loss: 1.8318, Batch Acc: 0.2500\n",
      "[Batch 100/501] Loss: 1.6266, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 1.5785, Batch Acc: 0.0000\n",
      "[Batch 120/501] Loss: 1.8858, Batch Acc: 0.1250\n",
      "[Batch 130/501] Loss: 1.6432, Batch Acc: 0.1250\n",
      "[Batch 140/501] Loss: 1.4746, Batch Acc: 0.3750\n",
      "[Batch 150/501] Loss: 1.7776, Batch Acc: 0.1250\n",
      "[Batch 160/501] Loss: 1.5021, Batch Acc: 0.3750\n",
      "[Batch 170/501] Loss: 1.5793, Batch Acc: 0.1250\n",
      "[Batch 180/501] Loss: 1.8580, Batch Acc: 0.1250\n",
      "[Batch 190/501] Loss: 1.5594, Batch Acc: 0.3750\n",
      "[Batch 200/501] Loss: 1.7418, Batch Acc: 0.1250\n",
      "[Batch 210/501] Loss: 1.7163, Batch Acc: 0.1250\n",
      "[Batch 220/501] Loss: 1.6080, Batch Acc: 0.3750\n",
      "[Batch 230/501] Loss: 1.5861, Batch Acc: 0.5000\n",
      "[Batch 240/501] Loss: 1.5805, Batch Acc: 0.1250\n",
      "[Batch 250/501] Loss: 1.5941, Batch Acc: 0.1250\n",
      "[Batch 260/501] Loss: 1.4472, Batch Acc: 0.2500\n",
      "[Batch 270/501] Loss: 1.7892, Batch Acc: 0.0000\n",
      "[Batch 280/501] Loss: 1.5379, Batch Acc: 0.2500\n",
      "[Batch 290/501] Loss: 1.5791, Batch Acc: 0.0000\n",
      "[Batch 300/501] Loss: 1.8821, Batch Acc: 0.1250\n",
      "[Batch 310/501] Loss: 1.4859, Batch Acc: 0.2500\n",
      "[Batch 320/501] Loss: 1.5983, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 1.4932, Batch Acc: 0.3750\n",
      "[Batch 340/501] Loss: 1.9125, Batch Acc: 0.0000\n",
      "[Batch 350/501] Loss: 1.6500, Batch Acc: 0.2500\n",
      "[Batch 360/501] Loss: 1.8515, Batch Acc: 0.1250\n",
      "[Batch 370/501] Loss: 1.8309, Batch Acc: 0.0000\n",
      "[Batch 380/501] Loss: 1.3791, Batch Acc: 0.3750\n",
      "[Batch 390/501] Loss: 1.8095, Batch Acc: 0.0000\n",
      "[Batch 400/501] Loss: 1.5621, Batch Acc: 0.2500\n",
      "[Batch 410/501] Loss: 1.7670, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.7015, Batch Acc: 0.1250\n",
      "[Batch 430/501] Loss: 1.5706, Batch Acc: 0.2500\n",
      "[Batch 440/501] Loss: 1.6556, Batch Acc: 0.1250\n",
      "[Batch 450/501] Loss: 1.6940, Batch Acc: 0.1250\n",
      "[Batch 460/501] Loss: 1.6178, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.7565, Batch Acc: 0.1250\n",
      "[Batch 480/501] Loss: 1.9674, Batch Acc: 0.1250\n",
      "[Batch 490/501] Loss: 1.5390, Batch Acc: 0.5000\n",
      "[Batch 500/501] Loss: 1.7464, Batch Acc: 0.2500\n",
      "Epoch 2 Summary - Loss: 833.8177, Train Accuracy: 0.2211\n",
      "[Batch 10/501] Loss: 1.5290, Batch Acc: 0.2500\n",
      "[Batch 20/501] Loss: 1.8076, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.5083, Batch Acc: 0.6250\n",
      "[Batch 40/501] Loss: 1.9035, Batch Acc: 0.1250\n",
      "[Batch 50/501] Loss: 1.4783, Batch Acc: 0.3750\n",
      "[Batch 60/501] Loss: 1.4034, Batch Acc: 0.3750\n",
      "[Batch 70/501] Loss: 1.5094, Batch Acc: 0.5000\n",
      "[Batch 80/501] Loss: 1.6570, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.7108, Batch Acc: 0.2500\n",
      "[Batch 100/501] Loss: 1.4999, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 1.5713, Batch Acc: 0.3750\n",
      "[Batch 120/501] Loss: 1.8707, Batch Acc: 0.1250\n",
      "[Batch 130/501] Loss: 1.6828, Batch Acc: 0.2500\n",
      "[Batch 140/501] Loss: 1.6435, Batch Acc: 0.0000\n",
      "[Batch 150/501] Loss: 1.4889, Batch Acc: 0.3750\n",
      "[Batch 160/501] Loss: 1.6628, Batch Acc: 0.2500\n",
      "[Batch 170/501] Loss: 1.6301, Batch Acc: 0.2500\n",
      "[Batch 180/501] Loss: 1.8818, Batch Acc: 0.1250\n",
      "[Batch 190/501] Loss: 1.7533, Batch Acc: 0.1250\n",
      "[Batch 200/501] Loss: 1.5590, Batch Acc: 0.2500\n",
      "[Batch 210/501] Loss: 1.7126, Batch Acc: 0.1250\n",
      "[Batch 220/501] Loss: 1.6334, Batch Acc: 0.2500\n",
      "[Batch 230/501] Loss: 1.6497, Batch Acc: 0.1250\n",
      "[Batch 240/501] Loss: 1.4348, Batch Acc: 0.5000\n",
      "[Batch 250/501] Loss: 1.6064, Batch Acc: 0.3750\n",
      "[Batch 260/501] Loss: 1.7260, Batch Acc: 0.1250\n",
      "[Batch 270/501] Loss: 1.7257, Batch Acc: 0.1250\n",
      "[Batch 280/501] Loss: 1.5322, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 1.4450, Batch Acc: 0.5000\n",
      "[Batch 300/501] Loss: 1.7128, Batch Acc: 0.1250\n",
      "[Batch 310/501] Loss: 1.5964, Batch Acc: 0.3750\n",
      "[Batch 320/501] Loss: 1.5757, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 1.4692, Batch Acc: 0.3750\n",
      "[Batch 340/501] Loss: 1.6642, Batch Acc: 0.1250\n",
      "[Batch 350/501] Loss: 1.5094, Batch Acc: 0.5000\n",
      "[Batch 360/501] Loss: 1.7262, Batch Acc: 0.1250\n",
      "[Batch 370/501] Loss: 1.7233, Batch Acc: 0.0000\n",
      "[Batch 380/501] Loss: 1.4512, Batch Acc: 0.6250\n",
      "[Batch 390/501] Loss: 1.7886, Batch Acc: 0.2500\n",
      "[Batch 400/501] Loss: 1.8406, Batch Acc: 0.1250\n",
      "[Batch 410/501] Loss: 1.5759, Batch Acc: 0.1250\n",
      "[Batch 420/501] Loss: 1.7842, Batch Acc: 0.2500\n",
      "[Batch 430/501] Loss: 1.6353, Batch Acc: 0.1250\n",
      "[Batch 440/501] Loss: 1.6287, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.9213, Batch Acc: 0.0000\n",
      "[Batch 460/501] Loss: 1.4904, Batch Acc: 0.6250\n",
      "[Batch 470/501] Loss: 1.8232, Batch Acc: 0.2500\n",
      "[Batch 480/501] Loss: 1.7273, Batch Acc: 0.1250\n",
      "[Batch 490/501] Loss: 1.8398, Batch Acc: 0.1250\n",
      "[Batch 500/501] Loss: 1.5528, Batch Acc: 0.5000\n",
      "Epoch 3 Summary - Loss: 834.5352, Train Accuracy: 0.2265\n",
      "[Batch 10/501] Loss: 1.7315, Batch Acc: 0.2500\n",
      "[Batch 20/501] Loss: 1.5797, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.7683, Batch Acc: 0.2500\n",
      "[Batch 40/501] Loss: 1.8539, Batch Acc: 0.2500\n",
      "[Batch 50/501] Loss: 1.8191, Batch Acc: 0.1250\n",
      "[Batch 60/501] Loss: 1.8660, Batch Acc: 0.1250\n",
      "[Batch 70/501] Loss: 1.6495, Batch Acc: 0.0000\n",
      "[Batch 80/501] Loss: 1.7214, Batch Acc: 0.1250\n",
      "[Batch 90/501] Loss: 1.3795, Batch Acc: 0.5000\n",
      "[Batch 100/501] Loss: 1.5686, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 1.7284, Batch Acc: 0.1250\n",
      "[Batch 120/501] Loss: 1.5135, Batch Acc: 0.3750\n",
      "[Batch 130/501] Loss: 1.8247, Batch Acc: 0.2500\n",
      "[Batch 140/501] Loss: 1.5415, Batch Acc: 0.0000\n",
      "[Batch 150/501] Loss: 1.8202, Batch Acc: 0.1250\n",
      "[Batch 160/501] Loss: 1.6925, Batch Acc: 0.1250\n",
      "[Batch 170/501] Loss: 1.8354, Batch Acc: 0.0000\n",
      "[Batch 180/501] Loss: 1.8559, Batch Acc: 0.2500\n",
      "[Batch 190/501] Loss: 1.2433, Batch Acc: 0.7500\n",
      "[Batch 200/501] Loss: 1.7316, Batch Acc: 0.1250\n",
      "[Batch 210/501] Loss: 1.8078, Batch Acc: 0.1250\n",
      "[Batch 220/501] Loss: 1.4078, Batch Acc: 0.3750\n",
      "[Batch 230/501] Loss: 1.6176, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.5148, Batch Acc: 0.3750\n",
      "[Batch 250/501] Loss: 1.3770, Batch Acc: 0.5000\n",
      "[Batch 260/501] Loss: 1.8238, Batch Acc: 0.0000\n",
      "[Batch 270/501] Loss: 1.9767, Batch Acc: 0.0000\n",
      "[Batch 280/501] Loss: 1.6460, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 1.6955, Batch Acc: 0.3750\n",
      "[Batch 300/501] Loss: 1.5852, Batch Acc: 0.1250\n",
      "[Batch 310/501] Loss: 1.7020, Batch Acc: 0.2500\n",
      "[Batch 320/501] Loss: 1.8347, Batch Acc: 0.0000\n",
      "[Batch 330/501] Loss: 1.6524, Batch Acc: 0.2500\n",
      "[Batch 340/501] Loss: 1.6898, Batch Acc: 0.2500\n",
      "[Batch 350/501] Loss: 1.1755, Batch Acc: 0.8750\n",
      "[Batch 360/501] Loss: 1.6521, Batch Acc: 0.1250\n",
      "[Batch 370/501] Loss: 1.8054, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.9785, Batch Acc: 0.1250\n",
      "[Batch 390/501] Loss: 1.6057, Batch Acc: 0.3750\n",
      "[Batch 400/501] Loss: 1.5763, Batch Acc: 0.1250\n",
      "[Batch 410/501] Loss: 1.6355, Batch Acc: 0.1250\n",
      "[Batch 420/501] Loss: 1.6893, Batch Acc: 0.3750\n",
      "[Batch 430/501] Loss: 1.8496, Batch Acc: 0.1250\n",
      "[Batch 440/501] Loss: 1.8443, Batch Acc: 0.0000\n",
      "[Batch 450/501] Loss: 1.4405, Batch Acc: 0.6250\n",
      "[Batch 460/501] Loss: 1.7141, Batch Acc: 0.1250\n",
      "[Batch 470/501] Loss: 1.7202, Batch Acc: 0.1250\n",
      "[Batch 480/501] Loss: 1.3560, Batch Acc: 0.5000\n",
      "[Batch 490/501] Loss: 1.3515, Batch Acc: 0.5000\n",
      "[Batch 500/501] Loss: 1.6143, Batch Acc: 0.3750\n",
      "Epoch 4 Summary - Loss: 833.5105, Train Accuracy: 0.2178\n",
      "[Batch 10/501] Loss: 1.5370, Batch Acc: 0.2500\n",
      "[Batch 20/501] Loss: 1.6521, Batch Acc: 0.1250\n",
      "[Batch 30/501] Loss: 1.7946, Batch Acc: 0.0000\n",
      "[Batch 40/501] Loss: 1.7997, Batch Acc: 0.3750\n",
      "[Batch 50/501] Loss: 1.6274, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 2.0201, Batch Acc: 0.1250\n",
      "[Batch 70/501] Loss: 1.8876, Batch Acc: 0.0000\n",
      "[Batch 80/501] Loss: 1.7325, Batch Acc: 0.1250\n",
      "[Batch 90/501] Loss: 1.6005, Batch Acc: 0.0000\n",
      "[Batch 100/501] Loss: 1.5241, Batch Acc: 0.5000\n",
      "[Batch 110/501] Loss: 1.6568, Batch Acc: 0.3750\n",
      "[Batch 120/501] Loss: 1.7193, Batch Acc: 0.1250\n",
      "[Batch 130/501] Loss: 1.5351, Batch Acc: 0.2500\n",
      "[Batch 140/501] Loss: 1.6182, Batch Acc: 0.1250\n",
      "[Batch 150/501] Loss: 1.7414, Batch Acc: 0.3750\n",
      "[Batch 160/501] Loss: 1.7965, Batch Acc: 0.0000\n",
      "[Batch 170/501] Loss: 1.7154, Batch Acc: 0.3750\n",
      "[Batch 180/501] Loss: 1.8778, Batch Acc: 0.1250\n",
      "[Batch 190/501] Loss: 1.5205, Batch Acc: 0.3750\n",
      "[Batch 200/501] Loss: 1.5195, Batch Acc: 0.5000\n",
      "[Batch 210/501] Loss: 1.6339, Batch Acc: 0.3750\n",
      "[Batch 220/501] Loss: 1.6233, Batch Acc: 0.2500\n",
      "[Batch 230/501] Loss: 1.7969, Batch Acc: 0.1250\n",
      "[Batch 240/501] Loss: 1.3471, Batch Acc: 0.6250\n",
      "[Batch 250/501] Loss: 1.7773, Batch Acc: 0.1250\n",
      "[Batch 260/501] Loss: 1.7164, Batch Acc: 0.2500\n",
      "[Batch 270/501] Loss: 1.4841, Batch Acc: 0.2500\n",
      "[Batch 280/501] Loss: 1.7371, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 1.7938, Batch Acc: 0.2500\n",
      "[Batch 300/501] Loss: 1.6956, Batch Acc: 0.1250\n",
      "[Batch 310/501] Loss: 1.5854, Batch Acc: 0.1250\n",
      "[Batch 320/501] Loss: 1.8809, Batch Acc: 0.0000\n",
      "[Batch 330/501] Loss: 1.8198, Batch Acc: 0.3750\n",
      "[Batch 340/501] Loss: 1.8356, Batch Acc: 0.0000\n",
      "[Batch 350/501] Loss: 1.6077, Batch Acc: 0.5000\n",
      "[Batch 360/501] Loss: 1.7581, Batch Acc: 0.1250\n",
      "[Batch 370/501] Loss: 1.7845, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.7340, Batch Acc: 0.1250\n",
      "[Batch 390/501] Loss: 1.6927, Batch Acc: 0.2500\n",
      "[Batch 400/501] Loss: 1.3364, Batch Acc: 0.5000\n",
      "[Batch 410/501] Loss: 1.7182, Batch Acc: 0.1250\n",
      "[Batch 420/501] Loss: 1.7790, Batch Acc: 0.2500\n",
      "[Batch 430/501] Loss: 1.5484, Batch Acc: 0.3750\n",
      "[Batch 440/501] Loss: 1.7068, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.4261, Batch Acc: 0.3750\n",
      "[Batch 460/501] Loss: 1.8395, Batch Acc: 0.1250\n",
      "[Batch 470/501] Loss: 1.6464, Batch Acc: 0.1250\n",
      "[Batch 480/501] Loss: 1.8089, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.4907, Batch Acc: 0.3750\n",
      "[Batch 500/501] Loss: 1.6110, Batch Acc: 0.2500\n",
      "Epoch 5 Summary - Loss: 830.7695, Train Accuracy: 0.2295\n",
      "[Batch 10/501] Loss: 1.6666, Batch Acc: 0.2500\n",
      "[Batch 20/501] Loss: 1.7876, Batch Acc: 0.1250\n",
      "[Batch 30/501] Loss: 1.8226, Batch Acc: 0.2500\n",
      "[Batch 40/501] Loss: 1.5194, Batch Acc: 0.2500\n",
      "[Batch 50/501] Loss: 1.4442, Batch Acc: 0.3750\n",
      "[Batch 60/501] Loss: 1.5657, Batch Acc: 0.2500\n",
      "[Batch 70/501] Loss: 1.4134, Batch Acc: 0.3750\n",
      "[Batch 80/501] Loss: 1.8870, Batch Acc: 0.1250\n",
      "[Batch 90/501] Loss: 1.8158, Batch Acc: 0.1250\n",
      "[Batch 100/501] Loss: 1.6204, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 1.5522, Batch Acc: 0.5000\n",
      "[Batch 120/501] Loss: 1.8193, Batch Acc: 0.1250\n",
      "[Batch 130/501] Loss: 1.6442, Batch Acc: 0.3750\n",
      "[Batch 140/501] Loss: 1.8223, Batch Acc: 0.1250\n",
      "[Batch 150/501] Loss: 1.6162, Batch Acc: 0.1250\n",
      "[Batch 160/501] Loss: 1.6678, Batch Acc: 0.2500\n",
      "[Batch 170/501] Loss: 1.4995, Batch Acc: 0.2500\n",
      "[Batch 180/501] Loss: 1.7688, Batch Acc: 0.2500\n",
      "[Batch 190/501] Loss: 1.5592, Batch Acc: 0.0000\n",
      "[Batch 200/501] Loss: 1.7021, Batch Acc: 0.1250\n",
      "[Batch 210/501] Loss: 1.7114, Batch Acc: 0.3750\n",
      "[Batch 220/501] Loss: 1.5884, Batch Acc: 0.2500\n",
      "[Batch 230/501] Loss: 1.8960, Batch Acc: 0.1250\n",
      "[Batch 240/501] Loss: 1.6264, Batch Acc: 0.2500\n",
      "[Batch 250/501] Loss: 1.5908, Batch Acc: 0.3750\n",
      "[Batch 260/501] Loss: 1.7272, Batch Acc: 0.2500\n",
      "[Batch 270/501] Loss: 1.7277, Batch Acc: 0.2500\n",
      "[Batch 280/501] Loss: 1.8417, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 1.6282, Batch Acc: 0.1250\n",
      "[Batch 300/501] Loss: 1.7369, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 1.6756, Batch Acc: 0.0000\n",
      "[Batch 320/501] Loss: 1.6860, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 1.7275, Batch Acc: 0.1250\n",
      "[Batch 340/501] Loss: 1.5734, Batch Acc: 0.3750\n",
      "[Batch 350/501] Loss: 1.6508, Batch Acc: 0.2500\n",
      "[Batch 360/501] Loss: 1.5554, Batch Acc: 0.1250\n",
      "[Batch 370/501] Loss: 1.7190, Batch Acc: 0.0000\n",
      "[Batch 380/501] Loss: 1.5648, Batch Acc: 0.2500\n",
      "[Batch 390/501] Loss: 1.5632, Batch Acc: 0.3750\n",
      "[Batch 400/501] Loss: 1.8697, Batch Acc: 0.1250\n",
      "[Batch 410/501] Loss: 1.6079, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.5531, Batch Acc: 0.2500\n",
      "[Batch 430/501] Loss: 1.9371, Batch Acc: 0.1250\n",
      "[Batch 440/501] Loss: 1.7260, Batch Acc: 0.1250\n",
      "[Batch 450/501] Loss: 1.6133, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.6132, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.6510, Batch Acc: 0.1250\n",
      "[Batch 480/501] Loss: 1.8418, Batch Acc: 0.0000\n",
      "[Batch 490/501] Loss: 1.4301, Batch Acc: 0.5000\n",
      "[Batch 500/501] Loss: 1.5202, Batch Acc: 0.2500\n",
      "Epoch 6 Summary - Loss: 834.1758, Train Accuracy: 0.2221\n",
      "[Batch 10/501] Loss: 1.6577, Batch Acc: 0.2500\n",
      "[Batch 20/501] Loss: 1.6637, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.7019, Batch Acc: 0.1250\n",
      "[Batch 40/501] Loss: 1.6582, Batch Acc: 0.1250\n",
      "[Batch 50/501] Loss: 1.7800, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.7918, Batch Acc: 0.0000\n",
      "[Batch 70/501] Loss: 1.5981, Batch Acc: 0.3750\n",
      "[Batch 80/501] Loss: 1.5258, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.5668, Batch Acc: 0.3750\n",
      "[Batch 100/501] Loss: 1.6875, Batch Acc: 0.3750\n",
      "[Batch 110/501] Loss: 1.4194, Batch Acc: 0.3750\n",
      "[Batch 120/501] Loss: 1.8894, Batch Acc: 0.0000\n",
      "[Batch 130/501] Loss: 1.5210, Batch Acc: 0.3750\n",
      "[Batch 140/501] Loss: 1.5917, Batch Acc: 0.3750\n",
      "[Batch 150/501] Loss: 1.4882, Batch Acc: 0.2500\n",
      "[Batch 160/501] Loss: 1.8951, Batch Acc: 0.2500\n",
      "[Batch 170/501] Loss: 1.6252, Batch Acc: 0.3750\n",
      "[Batch 180/501] Loss: 1.6429, Batch Acc: 0.2500\n",
      "[Batch 190/501] Loss: 1.5841, Batch Acc: 0.2500\n",
      "[Batch 200/501] Loss: 1.7150, Batch Acc: 0.1250\n",
      "[Batch 210/501] Loss: 1.5693, Batch Acc: 0.3750\n",
      "[Batch 220/501] Loss: 1.7875, Batch Acc: 0.1250\n",
      "[Batch 230/501] Loss: 1.9480, Batch Acc: 0.1250\n",
      "[Batch 240/501] Loss: 1.5812, Batch Acc: 0.2500\n",
      "[Batch 250/501] Loss: 1.5539, Batch Acc: 0.3750\n",
      "[Batch 260/501] Loss: 1.5428, Batch Acc: 0.3750\n",
      "[Batch 270/501] Loss: 1.4862, Batch Acc: 0.2500\n",
      "[Batch 280/501] Loss: 1.6955, Batch Acc: 0.3750\n",
      "[Batch 290/501] Loss: 1.6202, Batch Acc: 0.2500\n",
      "[Batch 300/501] Loss: 1.8150, Batch Acc: 0.1250\n",
      "[Batch 310/501] Loss: 1.7641, Batch Acc: 0.2500\n",
      "[Batch 320/501] Loss: 1.6376, Batch Acc: 0.0000\n",
      "[Batch 330/501] Loss: 1.6667, Batch Acc: 0.1250\n",
      "[Batch 340/501] Loss: 1.7113, Batch Acc: 0.5000\n",
      "[Batch 350/501] Loss: 1.5796, Batch Acc: 0.1250\n",
      "[Batch 360/501] Loss: 1.7297, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 1.6538, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.6901, Batch Acc: 0.0000\n",
      "[Batch 390/501] Loss: 1.5938, Batch Acc: 0.2500\n",
      "[Batch 400/501] Loss: 1.5403, Batch Acc: 0.2500\n",
      "[Batch 410/501] Loss: 1.7886, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.7176, Batch Acc: 0.2500\n",
      "[Batch 430/501] Loss: 1.8359, Batch Acc: 0.1250\n",
      "[Batch 440/501] Loss: 1.4215, Batch Acc: 0.5000\n",
      "[Batch 450/501] Loss: 1.5474, Batch Acc: 0.1250\n",
      "[Batch 460/501] Loss: 1.6280, Batch Acc: 0.1250\n",
      "[Batch 470/501] Loss: 1.5230, Batch Acc: 0.5000\n",
      "[Batch 480/501] Loss: 1.9556, Batch Acc: 0.0000\n",
      "[Batch 490/501] Loss: 1.4427, Batch Acc: 0.5000\n",
      "[Batch 500/501] Loss: 1.8336, Batch Acc: 0.0000\n",
      "Epoch 7 Summary - Loss: 833.3030, Train Accuracy: 0.2293\n",
      "[Batch 10/501] Loss: 1.6020, Batch Acc: 0.1250\n",
      "[Batch 20/501] Loss: 1.7858, Batch Acc: 0.1250\n",
      "[Batch 30/501] Loss: 1.8230, Batch Acc: 0.1250\n",
      "[Batch 40/501] Loss: 1.3381, Batch Acc: 0.8750\n",
      "[Batch 50/501] Loss: 1.5309, Batch Acc: 0.3750\n",
      "[Batch 60/501] Loss: 1.9913, Batch Acc: 0.0000\n",
      "[Batch 70/501] Loss: 1.6960, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.8334, Batch Acc: 0.1250\n",
      "[Batch 90/501] Loss: 1.6576, Batch Acc: 0.1250\n",
      "[Batch 100/501] Loss: 1.5599, Batch Acc: 0.3750\n",
      "[Batch 110/501] Loss: 1.4891, Batch Acc: 0.2500\n",
      "[Batch 120/501] Loss: 1.5059, Batch Acc: 0.5000\n",
      "[Batch 130/501] Loss: 1.6975, Batch Acc: 0.1250\n",
      "[Batch 140/501] Loss: 1.9645, Batch Acc: 0.1250\n",
      "[Batch 150/501] Loss: 1.7328, Batch Acc: 0.3750\n",
      "[Batch 160/501] Loss: 1.6278, Batch Acc: 0.3750\n",
      "[Batch 170/501] Loss: 1.7526, Batch Acc: 0.0000\n",
      "[Batch 180/501] Loss: 1.6679, Batch Acc: 0.2500\n",
      "[Batch 190/501] Loss: 1.7030, Batch Acc: 0.2500\n",
      "[Batch 200/501] Loss: 1.4643, Batch Acc: 0.5000\n",
      "[Batch 210/501] Loss: 1.4186, Batch Acc: 0.3750\n",
      "[Batch 220/501] Loss: 1.6821, Batch Acc: 0.3750\n",
      "[Batch 230/501] Loss: 1.7149, Batch Acc: 0.1250\n",
      "[Batch 240/501] Loss: 1.7973, Batch Acc: 0.0000\n",
      "[Batch 250/501] Loss: 1.6319, Batch Acc: 0.3750\n",
      "[Batch 260/501] Loss: 1.3326, Batch Acc: 0.6250\n",
      "[Batch 270/501] Loss: 1.7551, Batch Acc: 0.2500\n",
      "[Batch 280/501] Loss: 1.8239, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 1.5308, Batch Acc: 0.2500\n",
      "[Batch 300/501] Loss: 1.7562, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 1.6754, Batch Acc: 0.2500\n",
      "[Batch 320/501] Loss: 1.6427, Batch Acc: 0.1250\n",
      "[Batch 330/501] Loss: 1.8866, Batch Acc: 0.0000\n",
      "[Batch 340/501] Loss: 1.8289, Batch Acc: 0.0000\n",
      "[Batch 350/501] Loss: 1.5441, Batch Acc: 0.2500\n",
      "[Batch 360/501] Loss: 1.6499, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 1.6776, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.6193, Batch Acc: 0.2500\n",
      "[Batch 390/501] Loss: 1.6300, Batch Acc: 0.3750\n",
      "[Batch 400/501] Loss: 1.6747, Batch Acc: 0.2500\n",
      "[Batch 410/501] Loss: 1.4061, Batch Acc: 0.6250\n",
      "[Batch 420/501] Loss: 1.7588, Batch Acc: 0.0000\n",
      "[Batch 430/501] Loss: 1.9082, Batch Acc: 0.0000\n",
      "[Batch 440/501] Loss: 1.7121, Batch Acc: 0.1250\n",
      "[Batch 450/501] Loss: 1.7050, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.6294, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.6166, Batch Acc: 0.2500\n",
      "[Batch 480/501] Loss: 1.6331, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.7504, Batch Acc: 0.0000\n",
      "[Batch 500/501] Loss: 1.4260, Batch Acc: 0.3750\n",
      "Epoch 8 Summary - Loss: 828.6765, Train Accuracy: 0.2323\n",
      "[Batch 10/501] Loss: 1.5065, Batch Acc: 0.5000\n",
      "[Batch 20/501] Loss: 1.7163, Batch Acc: 0.1250\n",
      "[Batch 30/501] Loss: 1.7872, Batch Acc: 0.0000\n",
      "[Batch 40/501] Loss: 1.6369, Batch Acc: 0.2500\n",
      "[Batch 50/501] Loss: 1.6839, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.6982, Batch Acc: 0.2500\n",
      "[Batch 70/501] Loss: 1.6793, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.4208, Batch Acc: 0.3750\n",
      "[Batch 90/501] Loss: 1.6339, Batch Acc: 0.1250\n",
      "[Batch 100/501] Loss: 1.7514, Batch Acc: 0.1250\n",
      "[Batch 110/501] Loss: 1.6350, Batch Acc: 0.3750\n",
      "[Batch 120/501] Loss: 1.6309, Batch Acc: 0.5000\n",
      "[Batch 130/501] Loss: 1.7979, Batch Acc: 0.1250\n",
      "[Batch 140/501] Loss: 1.7397, Batch Acc: 0.2500\n",
      "[Batch 150/501] Loss: 1.8208, Batch Acc: 0.0000\n",
      "[Batch 160/501] Loss: 1.6305, Batch Acc: 0.2500\n",
      "[Batch 170/501] Loss: 1.6736, Batch Acc: 0.2500\n",
      "[Batch 180/501] Loss: 1.5099, Batch Acc: 0.1250\n",
      "[Batch 190/501] Loss: 1.7333, Batch Acc: 0.2500\n",
      "[Batch 200/501] Loss: 1.6258, Batch Acc: 0.0000\n",
      "[Batch 210/501] Loss: 1.6967, Batch Acc: 0.3750\n",
      "[Batch 220/501] Loss: 1.4328, Batch Acc: 0.6250\n",
      "[Batch 230/501] Loss: 1.7989, Batch Acc: 0.1250\n",
      "[Batch 240/501] Loss: 1.5621, Batch Acc: 0.5000\n",
      "[Batch 250/501] Loss: 1.6960, Batch Acc: 0.2500\n",
      "[Batch 260/501] Loss: 1.6709, Batch Acc: 0.2500\n",
      "[Batch 270/501] Loss: 1.4420, Batch Acc: 0.1250\n",
      "[Batch 280/501] Loss: 1.6208, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 1.5750, Batch Acc: 0.3750\n",
      "[Batch 300/501] Loss: 1.8139, Batch Acc: 0.1250\n",
      "[Batch 310/501] Loss: 1.8218, Batch Acc: 0.1250\n",
      "[Batch 320/501] Loss: 1.5747, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 1.6907, Batch Acc: 0.2500\n",
      "[Batch 340/501] Loss: 1.6359, Batch Acc: 0.3750\n",
      "[Batch 350/501] Loss: 1.7316, Batch Acc: 0.2500\n",
      "[Batch 360/501] Loss: 1.6822, Batch Acc: 0.1250\n",
      "[Batch 370/501] Loss: 1.8285, Batch Acc: 0.0000\n",
      "[Batch 380/501] Loss: 1.9123, Batch Acc: 0.0000\n",
      "[Batch 390/501] Loss: 1.5808, Batch Acc: 0.0000\n",
      "[Batch 400/501] Loss: 1.5625, Batch Acc: 0.3750\n",
      "[Batch 410/501] Loss: 1.7033, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.4904, Batch Acc: 0.3750\n",
      "[Batch 430/501] Loss: 1.6827, Batch Acc: 0.0000\n",
      "[Batch 440/501] Loss: 1.4368, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.9024, Batch Acc: 0.0000\n",
      "[Batch 460/501] Loss: 1.7290, Batch Acc: 0.3750\n",
      "[Batch 470/501] Loss: 1.7118, Batch Acc: 0.0000\n",
      "[Batch 480/501] Loss: 1.7401, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.8567, Batch Acc: 0.1250\n",
      "[Batch 500/501] Loss: 1.7002, Batch Acc: 0.0000\n",
      "Epoch 9 Summary - Loss: 831.8051, Train Accuracy: 0.2308\n",
      "[Batch 10/501] Loss: 1.7708, Batch Acc: 0.1250\n",
      "[Batch 20/501] Loss: 1.8965, Batch Acc: 0.1250\n",
      "[Batch 30/501] Loss: 1.8773, Batch Acc: 0.1250\n",
      "[Batch 40/501] Loss: 1.5165, Batch Acc: 0.3750\n",
      "[Batch 50/501] Loss: 1.6930, Batch Acc: 0.3750\n",
      "[Batch 60/501] Loss: 1.4971, Batch Acc: 0.1250\n",
      "[Batch 70/501] Loss: 1.3916, Batch Acc: 0.5000\n",
      "[Batch 80/501] Loss: 1.6351, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.4986, Batch Acc: 0.3750\n",
      "[Batch 100/501] Loss: 1.7227, Batch Acc: 0.0000\n",
      "[Batch 110/501] Loss: 1.6904, Batch Acc: 0.1250\n",
      "[Batch 120/501] Loss: 1.6532, Batch Acc: 0.3750\n",
      "[Batch 130/501] Loss: 1.6863, Batch Acc: 0.3750\n",
      "[Batch 140/501] Loss: 1.4538, Batch Acc: 0.5000\n",
      "[Batch 150/501] Loss: 1.8504, Batch Acc: 0.1250\n",
      "[Batch 160/501] Loss: 1.5955, Batch Acc: 0.3750\n",
      "[Batch 170/501] Loss: 1.8962, Batch Acc: 0.1250\n",
      "[Batch 180/501] Loss: 1.6321, Batch Acc: 0.1250\n",
      "[Batch 190/501] Loss: 1.6001, Batch Acc: 0.1250\n",
      "[Batch 200/501] Loss: 2.0478, Batch Acc: 0.0000\n",
      "[Batch 210/501] Loss: 1.7402, Batch Acc: 0.1250\n",
      "[Batch 220/501] Loss: 1.8979, Batch Acc: 0.2500\n",
      "[Batch 230/501] Loss: 1.3770, Batch Acc: 0.6250\n",
      "[Batch 240/501] Loss: 1.7172, Batch Acc: 0.1250\n",
      "[Batch 250/501] Loss: 1.4809, Batch Acc: 0.3750\n",
      "[Batch 260/501] Loss: 1.7881, Batch Acc: 0.1250\n",
      "[Batch 270/501] Loss: 1.6004, Batch Acc: 0.2500\n",
      "[Batch 280/501] Loss: 1.5889, Batch Acc: 0.3750\n",
      "[Batch 290/501] Loss: 1.7131, Batch Acc: 0.2500\n",
      "[Batch 300/501] Loss: 1.5125, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 1.9163, Batch Acc: 0.1250\n",
      "[Batch 320/501] Loss: 1.6199, Batch Acc: 0.1250\n",
      "[Batch 330/501] Loss: 1.5566, Batch Acc: 0.2500\n",
      "[Batch 340/501] Loss: 1.4972, Batch Acc: 0.2500\n",
      "[Batch 350/501] Loss: 1.4688, Batch Acc: 0.3750\n",
      "[Batch 360/501] Loss: 1.8978, Batch Acc: 0.1250\n",
      "[Batch 370/501] Loss: 1.6298, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.6067, Batch Acc: 0.1250\n",
      "[Batch 390/501] Loss: 1.5359, Batch Acc: 0.2500\n",
      "[Batch 400/501] Loss: 1.4213, Batch Acc: 0.6250\n",
      "[Batch 410/501] Loss: 1.3802, Batch Acc: 0.1250\n",
      "[Batch 420/501] Loss: 1.4557, Batch Acc: 0.5000\n",
      "[Batch 430/501] Loss: 1.4514, Batch Acc: 0.3750\n",
      "[Batch 440/501] Loss: 1.6442, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.7579, Batch Acc: 0.1250\n",
      "[Batch 460/501] Loss: 1.8746, Batch Acc: 0.0000\n",
      "[Batch 470/501] Loss: 1.7251, Batch Acc: 0.1250\n",
      "[Batch 480/501] Loss: 1.6799, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.6554, Batch Acc: 0.2500\n",
      "[Batch 500/501] Loss: 1.6224, Batch Acc: 0.2500\n",
      "Epoch 10 Summary - Loss: 831.7294, Train Accuracy: 0.2223\n",
      "Test Accuracy: 0.2433\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_accuracy</td><td>▂▃▅▁▇▃▇█▇▃</td></tr><tr><td>train_loss</td><td>█▇█▇▄█▇▁▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>test_accuracy</td><td>0.24332</td></tr><tr><td>test_loss</td><td>208.91686</td></tr><tr><td>train_accuracy</td><td>0.22231</td></tr><tr><td>train_loss</td><td>831.72943</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">quiet-sweep-6</strong> at: <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/5q9f0pwo' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/5q9f0pwo</a><br> View project at: <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250615_140806-5q9f0pwo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0c8o4rum with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_nodes: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ajit.kumar4@happiestminds.com/Documents/Drackula/ComputerVisionFromScratch/wandb/run-20250615_141108-0c8o4rum</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/0c8o4rum' target=\"_blank\">jumping-sweep-7</a></strong> to <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/0c8o4rum' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/0c8o4rum</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 10/501] Loss: 1.8658, Batch Acc: 0.2500\n",
      "[Batch 20/501] Loss: 1.5969, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.4422, Batch Acc: 0.5000\n",
      "[Batch 40/501] Loss: 1.6701, Batch Acc: 0.2500\n",
      "[Batch 50/501] Loss: 1.7235, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.3953, Batch Acc: 0.2500\n",
      "[Batch 70/501] Loss: 1.6133, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.9461, Batch Acc: 0.0000\n",
      "[Batch 90/501] Loss: 1.2482, Batch Acc: 0.6250\n",
      "[Batch 100/501] Loss: 1.6397, Batch Acc: 0.1250\n",
      "[Batch 110/501] Loss: 1.4551, Batch Acc: 0.3750\n",
      "[Batch 120/501] Loss: 1.5584, Batch Acc: 0.2500\n",
      "[Batch 130/501] Loss: 1.7730, Batch Acc: 0.1250\n",
      "[Batch 140/501] Loss: 1.7071, Batch Acc: 0.2500\n",
      "[Batch 150/501] Loss: 1.8392, Batch Acc: 0.1250\n",
      "[Batch 160/501] Loss: 1.3060, Batch Acc: 0.6250\n",
      "[Batch 170/501] Loss: 1.7075, Batch Acc: 0.2500\n",
      "[Batch 180/501] Loss: 2.0029, Batch Acc: 0.2500\n",
      "[Batch 190/501] Loss: 1.7516, Batch Acc: 0.0000\n",
      "[Batch 200/501] Loss: 1.6336, Batch Acc: 0.1250\n",
      "[Batch 210/501] Loss: 1.6694, Batch Acc: 0.1250\n",
      "[Batch 220/501] Loss: 1.5928, Batch Acc: 0.3750\n",
      "[Batch 230/501] Loss: 1.4619, Batch Acc: 0.5000\n",
      "[Batch 240/501] Loss: 1.2575, Batch Acc: 0.3750\n",
      "[Batch 250/501] Loss: 2.0250, Batch Acc: 0.0000\n",
      "[Batch 260/501] Loss: 1.4080, Batch Acc: 0.5000\n",
      "[Batch 270/501] Loss: 1.8527, Batch Acc: 0.2500\n",
      "[Batch 280/501] Loss: 1.7639, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 1.4344, Batch Acc: 0.5000\n",
      "[Batch 300/501] Loss: 1.7623, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 1.3682, Batch Acc: 0.5000\n",
      "[Batch 320/501] Loss: 1.6675, Batch Acc: 0.1250\n",
      "[Batch 330/501] Loss: 1.6334, Batch Acc: 0.3750\n",
      "[Batch 340/501] Loss: 1.6227, Batch Acc: 0.3750\n",
      "[Batch 350/501] Loss: 1.4249, Batch Acc: 0.6250\n",
      "[Batch 360/501] Loss: 1.7233, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 1.7337, Batch Acc: 0.2500\n",
      "[Batch 380/501] Loss: 1.7111, Batch Acc: 0.5000\n",
      "[Batch 390/501] Loss: 1.7111, Batch Acc: 0.2500\n",
      "[Batch 400/501] Loss: 1.8066, Batch Acc: 0.1250\n",
      "[Batch 410/501] Loss: 1.7344, Batch Acc: 0.1250\n",
      "[Batch 420/501] Loss: 1.7301, Batch Acc: 0.5000\n",
      "[Batch 430/501] Loss: 1.5798, Batch Acc: 0.2500\n",
      "[Batch 440/501] Loss: 1.2536, Batch Acc: 0.6250\n",
      "[Batch 450/501] Loss: 1.7534, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.7722, Batch Acc: 0.3750\n",
      "[Batch 470/501] Loss: 1.6034, Batch Acc: 0.3750\n",
      "[Batch 480/501] Loss: 1.8519, Batch Acc: 0.1250\n",
      "[Batch 490/501] Loss: 1.6142, Batch Acc: 0.2500\n",
      "[Batch 500/501] Loss: 1.9919, Batch Acc: 0.1250\n",
      "Epoch 1 Summary - Loss: 823.0265, Train Accuracy: 0.2929\n",
      "[Batch 10/501] Loss: 1.4521, Batch Acc: 0.3750\n",
      "[Batch 20/501] Loss: 1.7846, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.7039, Batch Acc: 0.3750\n",
      "[Batch 40/501] Loss: 1.5694, Batch Acc: 0.1250\n",
      "[Batch 50/501] Loss: 1.3821, Batch Acc: 0.3750\n",
      "[Batch 60/501] Loss: 1.7125, Batch Acc: 0.2500\n",
      "[Batch 70/501] Loss: 1.6299, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.7789, Batch Acc: 0.1250\n",
      "[Batch 90/501] Loss: 1.6027, Batch Acc: 0.3750\n",
      "[Batch 100/501] Loss: 1.4194, Batch Acc: 0.3750\n",
      "[Batch 110/501] Loss: 1.2272, Batch Acc: 0.6250\n",
      "[Batch 120/501] Loss: 1.2726, Batch Acc: 0.3750\n",
      "[Batch 130/501] Loss: 1.3853, Batch Acc: 0.3750\n",
      "[Batch 140/501] Loss: 1.5301, Batch Acc: 0.3750\n",
      "[Batch 150/501] Loss: 1.2494, Batch Acc: 0.5000\n",
      "[Batch 160/501] Loss: 1.6919, Batch Acc: 0.3750\n",
      "[Batch 170/501] Loss: 1.7035, Batch Acc: 0.5000\n",
      "[Batch 180/501] Loss: 1.6052, Batch Acc: 0.2500\n",
      "[Batch 190/501] Loss: 1.6752, Batch Acc: 0.3750\n",
      "[Batch 200/501] Loss: 1.3192, Batch Acc: 0.5000\n",
      "[Batch 210/501] Loss: 1.2566, Batch Acc: 0.6250\n",
      "[Batch 220/501] Loss: 1.5605, Batch Acc: 0.3750\n",
      "[Batch 230/501] Loss: 1.7072, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.5910, Batch Acc: 0.2500\n",
      "[Batch 250/501] Loss: 1.4573, Batch Acc: 0.5000\n",
      "[Batch 260/501] Loss: 1.1529, Batch Acc: 0.7500\n",
      "[Batch 270/501] Loss: 1.8230, Batch Acc: 0.2500\n",
      "[Batch 280/501] Loss: 1.5988, Batch Acc: 0.2500\n",
      "[Batch 290/501] Loss: 1.6383, Batch Acc: 0.5000\n",
      "[Batch 300/501] Loss: 1.3697, Batch Acc: 0.3750\n",
      "[Batch 310/501] Loss: 1.6081, Batch Acc: 0.2500\n",
      "[Batch 320/501] Loss: 1.4917, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 1.4172, Batch Acc: 0.3750\n",
      "[Batch 340/501] Loss: 1.5608, Batch Acc: 0.3750\n",
      "[Batch 350/501] Loss: 1.3804, Batch Acc: 0.3750\n",
      "[Batch 360/501] Loss: 1.7787, Batch Acc: 0.0000\n",
      "[Batch 370/501] Loss: 1.5614, Batch Acc: 0.3750\n",
      "[Batch 380/501] Loss: 1.3247, Batch Acc: 0.5000\n",
      "[Batch 390/501] Loss: 1.9927, Batch Acc: 0.2500\n",
      "[Batch 400/501] Loss: 1.4709, Batch Acc: 0.2500\n",
      "[Batch 410/501] Loss: 1.6001, Batch Acc: 0.5000\n",
      "[Batch 420/501] Loss: 1.6571, Batch Acc: 0.2500\n",
      "[Batch 430/501] Loss: 1.6244, Batch Acc: 0.2500\n",
      "[Batch 440/501] Loss: 1.5157, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.4366, Batch Acc: 0.3750\n",
      "[Batch 460/501] Loss: 1.5308, Batch Acc: 0.3750\n",
      "[Batch 470/501] Loss: 1.4498, Batch Acc: 0.5000\n",
      "[Batch 480/501] Loss: 1.8035, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.6433, Batch Acc: 0.2500\n",
      "[Batch 500/501] Loss: 1.2705, Batch Acc: 0.5000\n",
      "Epoch 2 Summary - Loss: 824.9925, Train Accuracy: 0.2887\n",
      "[Batch 10/501] Loss: 1.9868, Batch Acc: 0.1250\n",
      "[Batch 20/501] Loss: 1.8888, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.4172, Batch Acc: 0.3750\n",
      "[Batch 40/501] Loss: 1.9464, Batch Acc: 0.5000\n",
      "[Batch 50/501] Loss: 1.6021, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.3985, Batch Acc: 0.3750\n",
      "[Batch 70/501] Loss: 1.8142, Batch Acc: 0.0000\n",
      "[Batch 80/501] Loss: 1.7926, Batch Acc: 0.1250\n",
      "[Batch 90/501] Loss: 1.7721, Batch Acc: 0.2500\n",
      "[Batch 100/501] Loss: 1.1987, Batch Acc: 0.5000\n",
      "[Batch 110/501] Loss: 1.6526, Batch Acc: 0.1250\n",
      "[Batch 120/501] Loss: 1.2940, Batch Acc: 0.5000\n",
      "[Batch 130/501] Loss: 1.6167, Batch Acc: 0.1250\n",
      "[Batch 140/501] Loss: 1.6269, Batch Acc: 0.2500\n",
      "[Batch 150/501] Loss: 1.6349, Batch Acc: 0.1250\n",
      "[Batch 160/501] Loss: 1.3813, Batch Acc: 0.3750\n",
      "[Batch 170/501] Loss: 1.6776, Batch Acc: 0.3750\n",
      "[Batch 180/501] Loss: 1.8313, Batch Acc: 0.0000\n",
      "[Batch 190/501] Loss: 1.4346, Batch Acc: 0.2500\n",
      "[Batch 200/501] Loss: 1.4473, Batch Acc: 0.3750\n",
      "[Batch 210/501] Loss: 1.4994, Batch Acc: 0.3750\n",
      "[Batch 220/501] Loss: 2.0619, Batch Acc: 0.0000\n",
      "[Batch 230/501] Loss: 1.7678, Batch Acc: 0.1250\n",
      "[Batch 240/501] Loss: 1.7762, Batch Acc: 0.1250\n",
      "[Batch 250/501] Loss: 1.6549, Batch Acc: 0.1250\n",
      "[Batch 260/501] Loss: 1.8846, Batch Acc: 0.0000\n",
      "[Batch 270/501] Loss: 1.8620, Batch Acc: 0.2500\n",
      "[Batch 280/501] Loss: 1.7752, Batch Acc: 0.3750\n",
      "[Batch 290/501] Loss: 1.4127, Batch Acc: 0.3750\n",
      "[Batch 300/501] Loss: 1.3013, Batch Acc: 0.5000\n",
      "[Batch 310/501] Loss: 1.3587, Batch Acc: 0.3750\n",
      "[Batch 320/501] Loss: 1.8186, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 1.4164, Batch Acc: 0.3750\n",
      "[Batch 340/501] Loss: 1.6021, Batch Acc: 0.1250\n",
      "[Batch 350/501] Loss: 1.8550, Batch Acc: 0.1250\n",
      "[Batch 360/501] Loss: 2.1070, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 1.6256, Batch Acc: 0.2500\n",
      "[Batch 380/501] Loss: 1.2284, Batch Acc: 0.5000\n",
      "[Batch 390/501] Loss: 1.4775, Batch Acc: 0.3750\n",
      "[Batch 400/501] Loss: 1.6908, Batch Acc: 0.0000\n",
      "[Batch 410/501] Loss: 1.4984, Batch Acc: 0.3750\n",
      "[Batch 420/501] Loss: 2.0791, Batch Acc: 0.1250\n",
      "[Batch 430/501] Loss: 1.7716, Batch Acc: 0.1250\n",
      "[Batch 440/501] Loss: 1.4802, Batch Acc: 0.6250\n",
      "[Batch 450/501] Loss: 1.6198, Batch Acc: 0.3750\n",
      "[Batch 460/501] Loss: 1.2320, Batch Acc: 0.6250\n",
      "[Batch 470/501] Loss: 1.5249, Batch Acc: 0.5000\n",
      "[Batch 480/501] Loss: 1.5340, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.7879, Batch Acc: 0.1250\n",
      "[Batch 500/501] Loss: 1.8951, Batch Acc: 0.1250\n",
      "Epoch 3 Summary - Loss: 816.6252, Train Accuracy: 0.2884\n",
      "[Batch 10/501] Loss: 1.3383, Batch Acc: 0.5000\n",
      "[Batch 20/501] Loss: 1.8961, Batch Acc: 0.1250\n",
      "[Batch 30/501] Loss: 1.4713, Batch Acc: 0.2500\n",
      "[Batch 40/501] Loss: 1.4566, Batch Acc: 0.3750\n",
      "[Batch 50/501] Loss: 1.6031, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.8760, Batch Acc: 0.1250\n",
      "[Batch 70/501] Loss: 1.9456, Batch Acc: 0.1250\n",
      "[Batch 80/501] Loss: 1.9477, Batch Acc: 0.0000\n",
      "[Batch 90/501] Loss: 1.7585, Batch Acc: 0.1250\n",
      "[Batch 100/501] Loss: 1.8071, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 1.6854, Batch Acc: 0.2500\n",
      "[Batch 120/501] Loss: 1.8219, Batch Acc: 0.1250\n",
      "[Batch 130/501] Loss: 1.3795, Batch Acc: 0.3750\n",
      "[Batch 140/501] Loss: 1.8415, Batch Acc: 0.3750\n",
      "[Batch 150/501] Loss: 1.8751, Batch Acc: 0.1250\n",
      "[Batch 160/501] Loss: 1.8783, Batch Acc: 0.1250\n",
      "[Batch 170/501] Loss: 1.7159, Batch Acc: 0.1250\n",
      "[Batch 180/501] Loss: 2.2018, Batch Acc: 0.1250\n",
      "[Batch 190/501] Loss: 1.8133, Batch Acc: 0.2500\n",
      "[Batch 200/501] Loss: 1.7675, Batch Acc: 0.2500\n",
      "[Batch 210/501] Loss: 2.0409, Batch Acc: 0.2500\n",
      "[Batch 220/501] Loss: 1.6722, Batch Acc: 0.2500\n",
      "[Batch 230/501] Loss: 1.3502, Batch Acc: 0.5000\n",
      "[Batch 240/501] Loss: 1.5834, Batch Acc: 0.2500\n",
      "[Batch 250/501] Loss: 1.3808, Batch Acc: 0.3750\n",
      "[Batch 260/501] Loss: 1.6975, Batch Acc: 0.2500\n",
      "[Batch 270/501] Loss: 1.9715, Batch Acc: 0.1250\n",
      "[Batch 280/501] Loss: 1.6806, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 1.9105, Batch Acc: 0.2500\n",
      "[Batch 300/501] Loss: 1.7047, Batch Acc: 0.1250\n",
      "[Batch 310/501] Loss: 1.6096, Batch Acc: 0.2500\n",
      "[Batch 320/501] Loss: 1.7033, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 1.6812, Batch Acc: 0.2500\n",
      "[Batch 340/501] Loss: 1.9893, Batch Acc: 0.1250\n",
      "[Batch 350/501] Loss: 1.5186, Batch Acc: 0.3750\n",
      "[Batch 360/501] Loss: 1.9467, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 1.9222, Batch Acc: 0.2500\n",
      "[Batch 380/501] Loss: 1.8367, Batch Acc: 0.1250\n",
      "[Batch 390/501] Loss: 1.7113, Batch Acc: 0.2500\n",
      "[Batch 400/501] Loss: 2.1165, Batch Acc: 0.1250\n",
      "[Batch 410/501] Loss: 1.7483, Batch Acc: 0.3750\n",
      "[Batch 420/501] Loss: 1.7718, Batch Acc: 0.2500\n",
      "[Batch 430/501] Loss: 1.4023, Batch Acc: 0.3750\n",
      "[Batch 440/501] Loss: 1.6326, Batch Acc: 0.3750\n",
      "[Batch 450/501] Loss: 2.1035, Batch Acc: 0.0000\n",
      "[Batch 460/501] Loss: 1.7969, Batch Acc: 0.1250\n",
      "[Batch 470/501] Loss: 2.1491, Batch Acc: 0.1250\n",
      "[Batch 480/501] Loss: 1.5717, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.2839, Batch Acc: 0.3750\n",
      "[Batch 500/501] Loss: 2.0099, Batch Acc: 0.0000\n",
      "Epoch 4 Summary - Loss: 823.0620, Train Accuracy: 0.2859\n",
      "[Batch 10/501] Loss: 1.6747, Batch Acc: 0.3750\n",
      "[Batch 20/501] Loss: 1.7549, Batch Acc: 0.1250\n",
      "[Batch 30/501] Loss: 1.4992, Batch Acc: 0.3750\n",
      "[Batch 40/501] Loss: 1.4753, Batch Acc: 0.3750\n",
      "[Batch 50/501] Loss: 1.7671, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.6296, Batch Acc: 0.3750\n",
      "[Batch 70/501] Loss: 1.2075, Batch Acc: 0.5000\n",
      "[Batch 80/501] Loss: 1.6770, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.6381, Batch Acc: 0.5000\n",
      "[Batch 100/501] Loss: 1.7417, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 2.1005, Batch Acc: 0.1250\n",
      "[Batch 120/501] Loss: 1.5447, Batch Acc: 0.3750\n",
      "[Batch 130/501] Loss: 1.7264, Batch Acc: 0.1250\n",
      "[Batch 140/501] Loss: 1.7053, Batch Acc: 0.3750\n",
      "[Batch 150/501] Loss: 1.5405, Batch Acc: 0.5000\n",
      "[Batch 160/501] Loss: 1.6430, Batch Acc: 0.3750\n",
      "[Batch 170/501] Loss: 1.5125, Batch Acc: 0.2500\n",
      "[Batch 180/501] Loss: 1.9260, Batch Acc: 0.0000\n",
      "[Batch 190/501] Loss: 1.6310, Batch Acc: 0.1250\n",
      "[Batch 200/501] Loss: 1.6575, Batch Acc: 0.3750\n",
      "[Batch 210/501] Loss: 1.7576, Batch Acc: 0.1250\n",
      "[Batch 220/501] Loss: 1.6373, Batch Acc: 0.3750\n",
      "[Batch 230/501] Loss: 1.6464, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.4088, Batch Acc: 0.5000\n",
      "[Batch 250/501] Loss: 1.5796, Batch Acc: 0.2500\n",
      "[Batch 260/501] Loss: 2.0700, Batch Acc: 0.3750\n",
      "[Batch 270/501] Loss: 1.6497, Batch Acc: 0.2500\n",
      "[Batch 280/501] Loss: 1.5139, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 1.6256, Batch Acc: 0.2500\n",
      "[Batch 300/501] Loss: 1.4622, Batch Acc: 0.5000\n",
      "[Batch 310/501] Loss: 1.4417, Batch Acc: 0.5000\n",
      "[Batch 320/501] Loss: 1.7407, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 1.7353, Batch Acc: 0.2500\n",
      "[Batch 340/501] Loss: 1.5644, Batch Acc: 0.5000\n",
      "[Batch 350/501] Loss: 1.4272, Batch Acc: 0.2500\n",
      "[Batch 360/501] Loss: 1.4601, Batch Acc: 0.5000\n",
      "[Batch 370/501] Loss: 2.0094, Batch Acc: 0.3750\n",
      "[Batch 380/501] Loss: 1.3334, Batch Acc: 0.2500\n",
      "[Batch 390/501] Loss: 1.4411, Batch Acc: 0.3750\n",
      "[Batch 400/501] Loss: 1.7166, Batch Acc: 0.5000\n",
      "[Batch 410/501] Loss: 1.7825, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.5679, Batch Acc: 0.1250\n",
      "[Batch 430/501] Loss: 1.7551, Batch Acc: 0.2500\n",
      "[Batch 440/501] Loss: 1.5392, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.3501, Batch Acc: 0.3750\n",
      "[Batch 460/501] Loss: 1.3086, Batch Acc: 0.6250\n",
      "[Batch 470/501] Loss: 1.5431, Batch Acc: 0.1250\n",
      "[Batch 480/501] Loss: 1.8036, Batch Acc: 0.1250\n",
      "[Batch 490/501] Loss: 1.7697, Batch Acc: 0.3750\n",
      "[Batch 500/501] Loss: 1.5543, Batch Acc: 0.5000\n",
      "Epoch 5 Summary - Loss: 820.8562, Train Accuracy: 0.2934\n",
      "[Batch 10/501] Loss: 1.9853, Batch Acc: 0.1250\n",
      "[Batch 20/501] Loss: 1.7391, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.6410, Batch Acc: 0.2500\n",
      "[Batch 40/501] Loss: 1.7440, Batch Acc: 0.1250\n",
      "[Batch 50/501] Loss: 1.8620, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.4803, Batch Acc: 0.3750\n",
      "[Batch 70/501] Loss: 1.4672, Batch Acc: 0.3750\n",
      "[Batch 80/501] Loss: 2.0155, Batch Acc: 0.3750\n",
      "[Batch 90/501] Loss: 1.5904, Batch Acc: 0.2500\n",
      "[Batch 100/501] Loss: 1.3656, Batch Acc: 0.5000\n",
      "[Batch 110/501] Loss: 1.6962, Batch Acc: 0.2500\n",
      "[Batch 120/501] Loss: 1.6198, Batch Acc: 0.2500\n",
      "[Batch 130/501] Loss: 1.8497, Batch Acc: 0.1250\n",
      "[Batch 140/501] Loss: 1.7440, Batch Acc: 0.1250\n",
      "[Batch 150/501] Loss: 1.3657, Batch Acc: 0.2500\n",
      "[Batch 160/501] Loss: 1.6914, Batch Acc: 0.1250\n",
      "[Batch 170/501] Loss: 1.7795, Batch Acc: 0.1250\n",
      "[Batch 180/501] Loss: 1.7905, Batch Acc: 0.1250\n",
      "[Batch 190/501] Loss: 1.9401, Batch Acc: 0.2500\n",
      "[Batch 200/501] Loss: 1.5308, Batch Acc: 0.2500\n",
      "[Batch 210/501] Loss: 1.4091, Batch Acc: 0.3750\n",
      "[Batch 220/501] Loss: 1.7348, Batch Acc: 0.1250\n",
      "[Batch 230/501] Loss: 1.3976, Batch Acc: 0.3750\n",
      "[Batch 240/501] Loss: 1.7512, Batch Acc: 0.1250\n",
      "[Batch 250/501] Loss: 1.6391, Batch Acc: 0.5000\n",
      "[Batch 260/501] Loss: 1.8509, Batch Acc: 0.0000\n",
      "[Batch 270/501] Loss: 1.6713, Batch Acc: 0.2500\n",
      "[Batch 280/501] Loss: 1.7037, Batch Acc: 0.3750\n",
      "[Batch 290/501] Loss: 1.6706, Batch Acc: 0.1250\n",
      "[Batch 300/501] Loss: 1.2441, Batch Acc: 0.5000\n",
      "[Batch 310/501] Loss: 1.8858, Batch Acc: 0.1250\n",
      "[Batch 320/501] Loss: 1.7920, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 1.7658, Batch Acc: 0.1250\n",
      "[Batch 340/501] Loss: 1.9323, Batch Acc: 0.0000\n",
      "[Batch 350/501] Loss: 1.6602, Batch Acc: 0.2500\n",
      "[Batch 360/501] Loss: 1.8199, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 1.3027, Batch Acc: 0.5000\n",
      "[Batch 380/501] Loss: 1.6595, Batch Acc: 0.2500\n",
      "[Batch 390/501] Loss: 1.8264, Batch Acc: 0.0000\n",
      "[Batch 400/501] Loss: 1.6368, Batch Acc: 0.3750\n",
      "[Batch 410/501] Loss: 1.9505, Batch Acc: 0.1250\n",
      "[Batch 420/501] Loss: 1.5313, Batch Acc: 0.5000\n",
      "[Batch 430/501] Loss: 1.2767, Batch Acc: 0.3750\n",
      "[Batch 440/501] Loss: 1.2555, Batch Acc: 0.5000\n",
      "[Batch 450/501] Loss: 1.7311, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.2732, Batch Acc: 0.5000\n",
      "[Batch 470/501] Loss: 1.2475, Batch Acc: 0.6250\n",
      "[Batch 480/501] Loss: 1.8870, Batch Acc: 0.1250\n",
      "[Batch 490/501] Loss: 1.7926, Batch Acc: 0.1250\n",
      "[Batch 500/501] Loss: 1.7437, Batch Acc: 0.2500\n",
      "Epoch 6 Summary - Loss: 824.6105, Train Accuracy: 0.2762\n",
      "[Batch 10/501] Loss: 2.1861, Batch Acc: 0.0000\n",
      "[Batch 20/501] Loss: 1.4558, Batch Acc: 0.5000\n",
      "[Batch 30/501] Loss: 1.9992, Batch Acc: 0.1250\n",
      "[Batch 40/501] Loss: 1.7510, Batch Acc: 0.2500\n",
      "[Batch 50/501] Loss: 1.9326, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.5910, Batch Acc: 0.2500\n",
      "[Batch 70/501] Loss: 2.1349, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.3557, Batch Acc: 0.3750\n",
      "[Batch 90/501] Loss: 1.1904, Batch Acc: 0.5000\n",
      "[Batch 100/501] Loss: 1.3942, Batch Acc: 0.6250\n",
      "[Batch 110/501] Loss: 1.5060, Batch Acc: 0.2500\n",
      "[Batch 120/501] Loss: 1.8380, Batch Acc: 0.2500\n",
      "[Batch 130/501] Loss: 1.3113, Batch Acc: 0.3750\n",
      "[Batch 140/501] Loss: 1.6747, Batch Acc: 0.2500\n",
      "[Batch 150/501] Loss: 1.1276, Batch Acc: 0.5000\n",
      "[Batch 160/501] Loss: 2.0479, Batch Acc: 0.0000\n",
      "[Batch 170/501] Loss: 1.6798, Batch Acc: 0.3750\n",
      "[Batch 180/501] Loss: 1.6165, Batch Acc: 0.3750\n",
      "[Batch 190/501] Loss: 1.8660, Batch Acc: 0.2500\n",
      "[Batch 200/501] Loss: 1.7017, Batch Acc: 0.3750\n",
      "[Batch 210/501] Loss: 1.9797, Batch Acc: 0.1250\n",
      "[Batch 220/501] Loss: 1.4576, Batch Acc: 0.5000\n",
      "[Batch 230/501] Loss: 1.6488, Batch Acc: 0.1250\n",
      "[Batch 240/501] Loss: 1.4423, Batch Acc: 0.3750\n",
      "[Batch 250/501] Loss: 1.5673, Batch Acc: 0.1250\n",
      "[Batch 260/501] Loss: 1.7251, Batch Acc: 0.1250\n",
      "[Batch 270/501] Loss: 1.6796, Batch Acc: 0.1250\n",
      "[Batch 280/501] Loss: 1.7487, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 1.6250, Batch Acc: 0.2500\n",
      "[Batch 300/501] Loss: 1.5055, Batch Acc: 0.3750\n",
      "[Batch 310/501] Loss: 1.4179, Batch Acc: 0.5000\n",
      "[Batch 320/501] Loss: 1.1806, Batch Acc: 0.6250\n",
      "[Batch 330/501] Loss: 1.9598, Batch Acc: 0.1250\n",
      "[Batch 340/501] Loss: 1.7402, Batch Acc: 0.1250\n",
      "[Batch 350/501] Loss: 1.3294, Batch Acc: 0.2500\n",
      "[Batch 360/501] Loss: 1.3936, Batch Acc: 0.3750\n",
      "[Batch 370/501] Loss: 1.6036, Batch Acc: 0.3750\n",
      "[Batch 380/501] Loss: 1.9263, Batch Acc: 0.3750\n",
      "[Batch 390/501] Loss: 1.6329, Batch Acc: 0.1250\n",
      "[Batch 400/501] Loss: 1.7198, Batch Acc: 0.1250\n",
      "[Batch 410/501] Loss: 1.7385, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.8242, Batch Acc: 0.3750\n",
      "[Batch 430/501] Loss: 1.6230, Batch Acc: 0.1250\n",
      "[Batch 440/501] Loss: 1.4404, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.7856, Batch Acc: 0.1250\n",
      "[Batch 460/501] Loss: 1.3252, Batch Acc: 0.6250\n",
      "[Batch 470/501] Loss: 1.8232, Batch Acc: 0.0000\n",
      "[Batch 480/501] Loss: 1.4580, Batch Acc: 0.5000\n",
      "[Batch 490/501] Loss: 1.6410, Batch Acc: 0.5000\n",
      "[Batch 500/501] Loss: 1.4926, Batch Acc: 0.2500\n",
      "Epoch 7 Summary - Loss: 822.6560, Train Accuracy: 0.2924\n",
      "[Batch 10/501] Loss: 1.8108, Batch Acc: 0.1250\n",
      "[Batch 20/501] Loss: 1.3319, Batch Acc: 0.6250\n",
      "[Batch 30/501] Loss: 1.8276, Batch Acc: 0.3750\n",
      "[Batch 40/501] Loss: 1.4376, Batch Acc: 0.2500\n",
      "[Batch 50/501] Loss: 1.7423, Batch Acc: 0.1250\n",
      "[Batch 60/501] Loss: 1.5489, Batch Acc: 0.3750\n",
      "[Batch 70/501] Loss: 1.9984, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.5531, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.5686, Batch Acc: 0.2500\n",
      "[Batch 100/501] Loss: 1.6247, Batch Acc: 0.3750\n",
      "[Batch 110/501] Loss: 1.9117, Batch Acc: 0.1250\n",
      "[Batch 120/501] Loss: 1.3925, Batch Acc: 0.5000\n",
      "[Batch 130/501] Loss: 1.9434, Batch Acc: 0.0000\n",
      "[Batch 140/501] Loss: 1.8339, Batch Acc: 0.2500\n",
      "[Batch 150/501] Loss: 1.4461, Batch Acc: 0.3750\n",
      "[Batch 160/501] Loss: 1.6164, Batch Acc: 0.2500\n",
      "[Batch 170/501] Loss: 1.8034, Batch Acc: 0.2500\n",
      "[Batch 180/501] Loss: 1.5869, Batch Acc: 0.2500\n",
      "[Batch 190/501] Loss: 1.5726, Batch Acc: 0.2500\n",
      "[Batch 200/501] Loss: 1.9754, Batch Acc: 0.1250\n",
      "[Batch 210/501] Loss: 1.7834, Batch Acc: 0.2500\n",
      "[Batch 220/501] Loss: 1.6139, Batch Acc: 0.2500\n",
      "[Batch 230/501] Loss: 1.4581, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.2146, Batch Acc: 0.3750\n",
      "[Batch 250/501] Loss: 1.4670, Batch Acc: 0.2500\n",
      "[Batch 260/501] Loss: 1.4962, Batch Acc: 0.2500\n",
      "[Batch 270/501] Loss: 1.4510, Batch Acc: 0.5000\n",
      "[Batch 280/501] Loss: 1.6527, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 2.1102, Batch Acc: 0.0000\n",
      "[Batch 300/501] Loss: 1.8878, Batch Acc: 0.0000\n",
      "[Batch 310/501] Loss: 1.5821, Batch Acc: 0.2500\n",
      "[Batch 320/501] Loss: 1.7827, Batch Acc: 0.1250\n",
      "[Batch 330/501] Loss: 1.3733, Batch Acc: 0.3750\n",
      "[Batch 340/501] Loss: 1.5222, Batch Acc: 0.7500\n",
      "[Batch 350/501] Loss: 1.6446, Batch Acc: 0.2500\n",
      "[Batch 360/501] Loss: 1.3123, Batch Acc: 0.5000\n",
      "[Batch 370/501] Loss: 1.6679, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.5556, Batch Acc: 0.1250\n",
      "[Batch 390/501] Loss: 1.3608, Batch Acc: 0.3750\n",
      "[Batch 400/501] Loss: 1.3204, Batch Acc: 0.5000\n",
      "[Batch 410/501] Loss: 1.4069, Batch Acc: 0.3750\n",
      "[Batch 420/501] Loss: 1.3494, Batch Acc: 0.5000\n",
      "[Batch 430/501] Loss: 1.4619, Batch Acc: 0.3750\n",
      "[Batch 440/501] Loss: 1.5574, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.6233, Batch Acc: 0.3750\n",
      "[Batch 460/501] Loss: 1.6296, Batch Acc: 0.5000\n",
      "[Batch 470/501] Loss: 1.7991, Batch Acc: 0.3750\n",
      "[Batch 480/501] Loss: 1.3315, Batch Acc: 0.5000\n",
      "[Batch 490/501] Loss: 1.8078, Batch Acc: 0.2500\n",
      "[Batch 500/501] Loss: 1.3679, Batch Acc: 0.3750\n",
      "Epoch 8 Summary - Loss: 820.5850, Train Accuracy: 0.2887\n",
      "[Batch 10/501] Loss: 1.8871, Batch Acc: 0.3750\n",
      "[Batch 20/501] Loss: 1.4365, Batch Acc: 0.5000\n",
      "[Batch 30/501] Loss: 1.3605, Batch Acc: 0.3750\n",
      "[Batch 40/501] Loss: 1.6376, Batch Acc: 0.2500\n",
      "[Batch 50/501] Loss: 1.6709, Batch Acc: 0.1250\n",
      "[Batch 60/501] Loss: 1.3975, Batch Acc: 0.7500\n",
      "[Batch 70/501] Loss: 1.7269, Batch Acc: 0.5000\n",
      "[Batch 80/501] Loss: 1.6666, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.7130, Batch Acc: 0.1250\n",
      "[Batch 100/501] Loss: 1.3130, Batch Acc: 0.3750\n",
      "[Batch 110/501] Loss: 1.7249, Batch Acc: 0.2500\n",
      "[Batch 120/501] Loss: 1.6484, Batch Acc: 0.3750\n",
      "[Batch 130/501] Loss: 1.5490, Batch Acc: 0.6250\n",
      "[Batch 140/501] Loss: 1.6265, Batch Acc: 0.1250\n",
      "[Batch 150/501] Loss: 1.7094, Batch Acc: 0.2500\n",
      "[Batch 160/501] Loss: 1.2903, Batch Acc: 0.6250\n",
      "[Batch 170/501] Loss: 2.0254, Batch Acc: 0.2500\n",
      "[Batch 180/501] Loss: 1.6977, Batch Acc: 0.5000\n",
      "[Batch 190/501] Loss: 1.4469, Batch Acc: 0.3750\n",
      "[Batch 200/501] Loss: 1.4606, Batch Acc: 0.5000\n",
      "[Batch 210/501] Loss: 1.6817, Batch Acc: 0.3750\n",
      "[Batch 220/501] Loss: 1.7430, Batch Acc: 0.3750\n",
      "[Batch 230/501] Loss: 1.9787, Batch Acc: 0.1250\n",
      "[Batch 240/501] Loss: 1.4103, Batch Acc: 0.6250\n",
      "[Batch 250/501] Loss: 1.7998, Batch Acc: 0.2500\n",
      "[Batch 260/501] Loss: 1.5983, Batch Acc: 0.2500\n",
      "[Batch 270/501] Loss: 1.7847, Batch Acc: 0.0000\n",
      "[Batch 280/501] Loss: 1.7370, Batch Acc: 0.2500\n",
      "[Batch 290/501] Loss: 1.3999, Batch Acc: 0.3750\n",
      "[Batch 300/501] Loss: 1.8282, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 1.3618, Batch Acc: 0.6250\n",
      "[Batch 320/501] Loss: 2.2077, Batch Acc: 0.1250\n",
      "[Batch 330/501] Loss: 1.5992, Batch Acc: 0.5000\n",
      "[Batch 340/501] Loss: 1.3929, Batch Acc: 0.3750\n",
      "[Batch 350/501] Loss: 1.2398, Batch Acc: 0.6250\n",
      "[Batch 360/501] Loss: 1.6529, Batch Acc: 0.1250\n",
      "[Batch 370/501] Loss: 1.4437, Batch Acc: 0.3750\n",
      "[Batch 380/501] Loss: 1.7603, Batch Acc: 0.1250\n",
      "[Batch 390/501] Loss: 1.4935, Batch Acc: 0.3750\n",
      "[Batch 400/501] Loss: 1.4968, Batch Acc: 0.5000\n",
      "[Batch 410/501] Loss: 1.9501, Batch Acc: 0.1250\n",
      "[Batch 420/501] Loss: 1.7011, Batch Acc: 0.1250\n",
      "[Batch 430/501] Loss: 1.9156, Batch Acc: 0.0000\n",
      "[Batch 440/501] Loss: 1.2856, Batch Acc: 0.3750\n",
      "[Batch 450/501] Loss: 1.4861, Batch Acc: 0.5000\n",
      "[Batch 460/501] Loss: 1.7408, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.7133, Batch Acc: 0.2500\n",
      "[Batch 480/501] Loss: 1.4054, Batch Acc: 0.3750\n",
      "[Batch 490/501] Loss: 1.6642, Batch Acc: 0.2500\n",
      "[Batch 500/501] Loss: 1.2772, Batch Acc: 0.6250\n",
      "Epoch 9 Summary - Loss: 822.9937, Train Accuracy: 0.2909\n",
      "[Batch 10/501] Loss: 1.4193, Batch Acc: 0.3750\n",
      "[Batch 20/501] Loss: 1.5924, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.5480, Batch Acc: 0.2500\n",
      "[Batch 40/501] Loss: 1.6538, Batch Acc: 0.1250\n",
      "[Batch 50/501] Loss: 1.4938, Batch Acc: 0.5000\n",
      "[Batch 60/501] Loss: 1.6310, Batch Acc: 0.2500\n",
      "[Batch 70/501] Loss: 1.5102, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.5850, Batch Acc: 0.3750\n",
      "[Batch 90/501] Loss: 1.2971, Batch Acc: 0.5000\n",
      "[Batch 100/501] Loss: 1.5798, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 1.3908, Batch Acc: 0.3750\n",
      "[Batch 120/501] Loss: 1.8318, Batch Acc: 0.1250\n",
      "[Batch 130/501] Loss: 1.6148, Batch Acc: 0.1250\n",
      "[Batch 140/501] Loss: 1.6309, Batch Acc: 0.2500\n",
      "[Batch 150/501] Loss: 1.9286, Batch Acc: 0.2500\n",
      "[Batch 160/501] Loss: 2.0740, Batch Acc: 0.1250\n",
      "[Batch 170/501] Loss: 1.6630, Batch Acc: 0.2500\n",
      "[Batch 180/501] Loss: 1.6046, Batch Acc: 0.3750\n",
      "[Batch 190/501] Loss: 1.5246, Batch Acc: 0.2500\n",
      "[Batch 200/501] Loss: 1.9968, Batch Acc: 0.2500\n",
      "[Batch 210/501] Loss: 1.4517, Batch Acc: 0.3750\n",
      "[Batch 220/501] Loss: 1.3662, Batch Acc: 0.5000\n",
      "[Batch 230/501] Loss: 1.7257, Batch Acc: 0.0000\n",
      "[Batch 240/501] Loss: 1.3589, Batch Acc: 0.5000\n",
      "[Batch 250/501] Loss: 1.6791, Batch Acc: 0.0000\n",
      "[Batch 260/501] Loss: 1.4864, Batch Acc: 0.6250\n",
      "[Batch 270/501] Loss: 1.7164, Batch Acc: 0.3750\n",
      "[Batch 280/501] Loss: 1.3860, Batch Acc: 0.5000\n",
      "[Batch 290/501] Loss: 1.4873, Batch Acc: 0.3750\n",
      "[Batch 300/501] Loss: 1.3810, Batch Acc: 0.3750\n",
      "[Batch 310/501] Loss: 1.6436, Batch Acc: 0.2500\n",
      "[Batch 320/501] Loss: 1.5181, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 2.0374, Batch Acc: 0.5000\n",
      "[Batch 340/501] Loss: 1.7122, Batch Acc: 0.2500\n",
      "[Batch 350/501] Loss: 1.4179, Batch Acc: 0.5000\n",
      "[Batch 360/501] Loss: 1.4298, Batch Acc: 0.3750\n",
      "[Batch 370/501] Loss: 1.9414, Batch Acc: 0.2500\n",
      "[Batch 380/501] Loss: 1.8731, Batch Acc: 0.1250\n",
      "[Batch 390/501] Loss: 1.7750, Batch Acc: 0.1250\n",
      "[Batch 400/501] Loss: 1.4301, Batch Acc: 0.2500\n",
      "[Batch 410/501] Loss: 1.3329, Batch Acc: 0.5000\n",
      "[Batch 420/501] Loss: 1.3921, Batch Acc: 0.5000\n",
      "[Batch 430/501] Loss: 1.9354, Batch Acc: 0.1250\n",
      "[Batch 440/501] Loss: 1.6100, Batch Acc: 0.1250\n",
      "[Batch 450/501] Loss: 1.5950, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.1071, Batch Acc: 0.5000\n",
      "[Batch 470/501] Loss: 1.8293, Batch Acc: 0.0000\n",
      "[Batch 480/501] Loss: 1.9131, Batch Acc: 0.3750\n",
      "[Batch 490/501] Loss: 1.6179, Batch Acc: 0.3750\n",
      "[Batch 500/501] Loss: 1.8262, Batch Acc: 0.3750\n",
      "Epoch 10 Summary - Loss: 821.9675, Train Accuracy: 0.2979\n",
      "Test Accuracy: 0.2868\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_accuracy</td><td>▆▅▅▄▇▁▆▅▆█</td></tr><tr><td>train_loss</td><td>▆█▁▆▅█▆▄▆▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>test_accuracy</td><td>0.28684</td></tr><tr><td>test_loss</td><td>207.6542</td></tr><tr><td>train_accuracy</td><td>0.2979</td></tr><tr><td>train_loss</td><td>821.96754</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">jumping-sweep-7</strong> at: <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/0c8o4rum' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/0c8o4rum</a><br> View project at: <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250615_141108-0c8o4rum/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: l5bgo6ha with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_nodes: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ajit.kumar4@happiestminds.com/Documents/Drackula/ComputerVisionFromScratch/wandb/run-20250615_141453-l5bgo6ha</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/l5bgo6ha' target=\"_blank\">helpful-sweep-8</a></strong> to <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/l5bgo6ha' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/l5bgo6ha</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 10/501] Loss: 1.7293, Batch Acc: 0.1250\n",
      "[Batch 20/501] Loss: 1.5028, Batch Acc: 0.7500\n",
      "[Batch 30/501] Loss: 1.5889, Batch Acc: 0.1250\n",
      "[Batch 40/501] Loss: 1.7117, Batch Acc: 0.1250\n",
      "[Batch 50/501] Loss: 1.7259, Batch Acc: 0.0000\n",
      "[Batch 60/501] Loss: 2.1188, Batch Acc: 0.0000\n",
      "[Batch 70/501] Loss: 1.4473, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.5359, Batch Acc: 0.3750\n",
      "[Batch 90/501] Loss: 1.8403, Batch Acc: 0.2500\n",
      "[Batch 100/501] Loss: 1.5895, Batch Acc: 0.5000\n",
      "[Batch 110/501] Loss: 1.3643, Batch Acc: 0.5000\n",
      "[Batch 120/501] Loss: 1.6537, Batch Acc: 0.1250\n",
      "[Batch 130/501] Loss: 1.6345, Batch Acc: 0.1250\n",
      "[Batch 140/501] Loss: 1.7847, Batch Acc: 0.2500\n",
      "[Batch 150/501] Loss: 1.7660, Batch Acc: 0.1250\n",
      "[Batch 160/501] Loss: 1.8327, Batch Acc: 0.1250\n",
      "[Batch 170/501] Loss: 2.0176, Batch Acc: 0.1250\n",
      "[Batch 180/501] Loss: 1.7129, Batch Acc: 0.2500\n",
      "[Batch 190/501] Loss: 1.9015, Batch Acc: 0.0000\n",
      "[Batch 200/501] Loss: 1.5614, Batch Acc: 0.1250\n",
      "[Batch 210/501] Loss: 1.7603, Batch Acc: 0.2500\n",
      "[Batch 220/501] Loss: 1.6184, Batch Acc: 0.1250\n",
      "[Batch 230/501] Loss: 1.5294, Batch Acc: 0.1250\n",
      "[Batch 240/501] Loss: 1.9003, Batch Acc: 0.1250\n",
      "[Batch 250/501] Loss: 1.6959, Batch Acc: 0.2500\n",
      "[Batch 260/501] Loss: 1.4843, Batch Acc: 0.2500\n",
      "[Batch 270/501] Loss: 1.8835, Batch Acc: 0.1250\n",
      "[Batch 280/501] Loss: 1.7782, Batch Acc: 0.5000\n",
      "[Batch 290/501] Loss: 1.7972, Batch Acc: 0.1250\n",
      "[Batch 300/501] Loss: 1.7864, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 1.7471, Batch Acc: 0.1250\n",
      "[Batch 320/501] Loss: 1.7679, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 1.7038, Batch Acc: 0.1250\n",
      "[Batch 340/501] Loss: 1.8186, Batch Acc: 0.2500\n",
      "[Batch 350/501] Loss: 2.1744, Batch Acc: 0.0000\n",
      "[Batch 360/501] Loss: 1.8052, Batch Acc: 0.0000\n",
      "[Batch 370/501] Loss: 1.7915, Batch Acc: 0.2500\n",
      "[Batch 380/501] Loss: 1.5432, Batch Acc: 0.2500\n",
      "[Batch 390/501] Loss: 1.8303, Batch Acc: 0.2500\n",
      "[Batch 400/501] Loss: 1.6575, Batch Acc: 0.2500\n",
      "[Batch 410/501] Loss: 1.6615, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.6002, Batch Acc: 0.5000\n",
      "[Batch 430/501] Loss: 1.3678, Batch Acc: 0.2500\n",
      "[Batch 440/501] Loss: 1.5205, Batch Acc: 0.3750\n",
      "[Batch 450/501] Loss: 1.5434, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.4177, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.7211, Batch Acc: 0.1250\n",
      "[Batch 480/501] Loss: 1.4897, Batch Acc: 0.3750\n",
      "[Batch 490/501] Loss: 1.5527, Batch Acc: 0.2500\n",
      "[Batch 500/501] Loss: 1.7141, Batch Acc: 0.1250\n",
      "Epoch 1 Summary - Loss: 841.3475, Train Accuracy: 0.2146\n",
      "[Batch 10/501] Loss: 1.8189, Batch Acc: 0.2500\n",
      "[Batch 20/501] Loss: 1.6666, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.5672, Batch Acc: 0.3750\n",
      "[Batch 40/501] Loss: 1.6470, Batch Acc: 0.2500\n",
      "[Batch 50/501] Loss: 1.6683, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.7690, Batch Acc: 0.1250\n",
      "[Batch 70/501] Loss: 1.7867, Batch Acc: 0.1250\n",
      "[Batch 80/501] Loss: 1.6134, Batch Acc: 0.1250\n",
      "[Batch 90/501] Loss: 1.5062, Batch Acc: 0.2500\n",
      "[Batch 100/501] Loss: 1.6979, Batch Acc: 0.1250\n",
      "[Batch 110/501] Loss: 1.7412, Batch Acc: 0.2500\n",
      "[Batch 120/501] Loss: 1.9478, Batch Acc: 0.0000\n",
      "[Batch 130/501] Loss: 1.8915, Batch Acc: 0.1250\n",
      "[Batch 140/501] Loss: 1.5349, Batch Acc: 0.1250\n",
      "[Batch 150/501] Loss: 1.9586, Batch Acc: 0.0000\n",
      "[Batch 160/501] Loss: 1.5408, Batch Acc: 0.3750\n",
      "[Batch 170/501] Loss: 1.4074, Batch Acc: 0.3750\n",
      "[Batch 180/501] Loss: 1.5065, Batch Acc: 0.6250\n",
      "[Batch 190/501] Loss: 1.5819, Batch Acc: 0.3750\n",
      "[Batch 200/501] Loss: 2.0625, Batch Acc: 0.0000\n",
      "[Batch 210/501] Loss: 2.0213, Batch Acc: 0.0000\n",
      "[Batch 220/501] Loss: 1.5775, Batch Acc: 0.2500\n",
      "[Batch 230/501] Loss: 1.8956, Batch Acc: 0.1250\n",
      "[Batch 240/501] Loss: 1.3809, Batch Acc: 0.3750\n",
      "[Batch 250/501] Loss: 1.6628, Batch Acc: 0.1250\n",
      "[Batch 260/501] Loss: 1.7946, Batch Acc: 0.0000\n",
      "[Batch 270/501] Loss: 1.8160, Batch Acc: 0.2500\n",
      "[Batch 280/501] Loss: 1.8834, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 1.7242, Batch Acc: 0.2500\n",
      "[Batch 300/501] Loss: 1.5475, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 1.6426, Batch Acc: 0.2500\n",
      "[Batch 320/501] Loss: 1.7864, Batch Acc: 0.1250\n",
      "[Batch 330/501] Loss: 1.7290, Batch Acc: 0.2500\n",
      "[Batch 340/501] Loss: 1.7741, Batch Acc: 0.1250\n",
      "[Batch 350/501] Loss: 1.8796, Batch Acc: 0.1250\n",
      "[Batch 360/501] Loss: 2.0778, Batch Acc: 0.0000\n",
      "[Batch 370/501] Loss: 1.9959, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.8633, Batch Acc: 0.1250\n",
      "[Batch 390/501] Loss: 1.5586, Batch Acc: 0.2500\n",
      "[Batch 400/501] Loss: 1.6496, Batch Acc: 0.2500\n",
      "[Batch 410/501] Loss: 1.9181, Batch Acc: 0.1250\n",
      "[Batch 420/501] Loss: 1.6715, Batch Acc: 0.2500\n",
      "[Batch 430/501] Loss: 1.7779, Batch Acc: 0.0000\n",
      "[Batch 440/501] Loss: 2.1181, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.6333, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.5540, Batch Acc: 0.1250\n",
      "[Batch 470/501] Loss: 1.4857, Batch Acc: 0.1250\n",
      "[Batch 480/501] Loss: 1.6986, Batch Acc: 0.3750\n",
      "[Batch 490/501] Loss: 1.8321, Batch Acc: 0.2500\n",
      "[Batch 500/501] Loss: 1.7788, Batch Acc: 0.1250\n",
      "Epoch 2 Summary - Loss: 841.7604, Train Accuracy: 0.2108\n",
      "[Batch 10/501] Loss: 1.8194, Batch Acc: 0.1250\n",
      "[Batch 20/501] Loss: 1.7836, Batch Acc: 0.1250\n",
      "[Batch 30/501] Loss: 1.9094, Batch Acc: 0.1250\n",
      "[Batch 40/501] Loss: 2.0286, Batch Acc: 0.0000\n",
      "[Batch 50/501] Loss: 1.5596, Batch Acc: 0.3750\n",
      "[Batch 60/501] Loss: 1.6345, Batch Acc: 0.1250\n",
      "[Batch 70/501] Loss: 1.8288, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.9053, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.5704, Batch Acc: 0.3750\n",
      "[Batch 100/501] Loss: 1.4665, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 1.7403, Batch Acc: 0.3750\n",
      "[Batch 120/501] Loss: 1.5998, Batch Acc: 0.2500\n",
      "[Batch 130/501] Loss: 1.5629, Batch Acc: 0.3750\n",
      "[Batch 140/501] Loss: 1.6928, Batch Acc: 0.2500\n",
      "[Batch 150/501] Loss: 1.8168, Batch Acc: 0.0000\n",
      "[Batch 160/501] Loss: 1.7333, Batch Acc: 0.0000\n",
      "[Batch 170/501] Loss: 1.5746, Batch Acc: 0.1250\n",
      "[Batch 180/501] Loss: 2.1036, Batch Acc: 0.1250\n",
      "[Batch 190/501] Loss: 1.6512, Batch Acc: 0.2500\n",
      "[Batch 200/501] Loss: 1.7349, Batch Acc: 0.0000\n",
      "[Batch 210/501] Loss: 1.6136, Batch Acc: 0.2500\n",
      "[Batch 220/501] Loss: 1.5679, Batch Acc: 0.2500\n",
      "[Batch 230/501] Loss: 1.5062, Batch Acc: 0.3750\n",
      "[Batch 240/501] Loss: 1.7926, Batch Acc: 0.0000\n",
      "[Batch 250/501] Loss: 1.6152, Batch Acc: 0.3750\n",
      "[Batch 260/501] Loss: 1.5335, Batch Acc: 0.3750\n",
      "[Batch 270/501] Loss: 1.5161, Batch Acc: 0.3750\n",
      "[Batch 280/501] Loss: 1.7479, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 1.6096, Batch Acc: 0.3750\n",
      "[Batch 300/501] Loss: 1.8596, Batch Acc: 0.1250\n",
      "[Batch 310/501] Loss: 1.6732, Batch Acc: 0.2500\n",
      "[Batch 320/501] Loss: 1.5957, Batch Acc: 0.1250\n",
      "[Batch 330/501] Loss: 1.5933, Batch Acc: 0.3750\n",
      "[Batch 340/501] Loss: 1.8122, Batch Acc: 0.1250\n",
      "[Batch 350/501] Loss: 1.8883, Batch Acc: 0.1250\n",
      "[Batch 360/501] Loss: 1.9696, Batch Acc: 0.1250\n",
      "[Batch 370/501] Loss: 1.4147, Batch Acc: 0.5000\n",
      "[Batch 380/501] Loss: 1.5819, Batch Acc: 0.3750\n",
      "[Batch 390/501] Loss: 1.9686, Batch Acc: 0.0000\n",
      "[Batch 400/501] Loss: 1.6619, Batch Acc: 0.0000\n",
      "[Batch 410/501] Loss: 1.7956, Batch Acc: 0.1250\n",
      "[Batch 420/501] Loss: 1.4627, Batch Acc: 0.2500\n",
      "[Batch 430/501] Loss: 1.3265, Batch Acc: 0.5000\n",
      "[Batch 440/501] Loss: 1.5301, Batch Acc: 0.1250\n",
      "[Batch 450/501] Loss: 1.6619, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.6072, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.7764, Batch Acc: 0.0000\n",
      "[Batch 480/501] Loss: 1.6507, Batch Acc: 0.1250\n",
      "[Batch 490/501] Loss: 1.7684, Batch Acc: 0.1250\n",
      "[Batch 500/501] Loss: 1.7228, Batch Acc: 0.2500\n",
      "Epoch 3 Summary - Loss: 836.0507, Train Accuracy: 0.2116\n",
      "[Batch 10/501] Loss: 1.5173, Batch Acc: 0.0000\n",
      "[Batch 20/501] Loss: 1.6237, Batch Acc: 0.3750\n",
      "[Batch 30/501] Loss: 1.5412, Batch Acc: 0.2500\n",
      "[Batch 40/501] Loss: 1.6441, Batch Acc: 0.0000\n",
      "[Batch 50/501] Loss: 2.0487, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.6201, Batch Acc: 0.2500\n",
      "[Batch 70/501] Loss: 2.0841, Batch Acc: 0.1250\n",
      "[Batch 80/501] Loss: 1.6675, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.6044, Batch Acc: 0.2500\n",
      "[Batch 100/501] Loss: 1.4552, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 1.5912, Batch Acc: 0.2500\n",
      "[Batch 120/501] Loss: 1.4097, Batch Acc: 0.3750\n",
      "[Batch 130/501] Loss: 1.5929, Batch Acc: 0.2500\n",
      "[Batch 140/501] Loss: 1.5917, Batch Acc: 0.3750\n",
      "[Batch 150/501] Loss: 1.7636, Batch Acc: 0.0000\n",
      "[Batch 160/501] Loss: 1.6042, Batch Acc: 0.3750\n",
      "[Batch 170/501] Loss: 2.0060, Batch Acc: 0.1250\n",
      "[Batch 180/501] Loss: 1.9248, Batch Acc: 0.0000\n",
      "[Batch 190/501] Loss: 1.5381, Batch Acc: 0.2500\n",
      "[Batch 200/501] Loss: 1.6122, Batch Acc: 0.2500\n",
      "[Batch 210/501] Loss: 1.8971, Batch Acc: 0.0000\n",
      "[Batch 220/501] Loss: 1.8491, Batch Acc: 0.2500\n",
      "[Batch 230/501] Loss: 1.4210, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.5845, Batch Acc: 0.2500\n",
      "[Batch 250/501] Loss: 1.9903, Batch Acc: 0.2500\n",
      "[Batch 260/501] Loss: 1.5049, Batch Acc: 0.3750\n",
      "[Batch 270/501] Loss: 1.6683, Batch Acc: 0.2500\n",
      "[Batch 280/501] Loss: 1.6583, Batch Acc: 0.5000\n",
      "[Batch 290/501] Loss: 1.5017, Batch Acc: 0.1250\n",
      "[Batch 300/501] Loss: 1.5483, Batch Acc: 0.3750\n",
      "[Batch 310/501] Loss: 1.6585, Batch Acc: 0.1250\n",
      "[Batch 320/501] Loss: 1.4705, Batch Acc: 0.3750\n",
      "[Batch 330/501] Loss: 1.5016, Batch Acc: 0.2500\n",
      "[Batch 340/501] Loss: 1.6612, Batch Acc: 0.2500\n",
      "[Batch 350/501] Loss: 1.6095, Batch Acc: 0.3750\n",
      "[Batch 360/501] Loss: 1.6558, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 1.6371, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.5528, Batch Acc: 0.2500\n",
      "[Batch 390/501] Loss: 1.4961, Batch Acc: 0.2500\n",
      "[Batch 400/501] Loss: 1.6226, Batch Acc: 0.1250\n",
      "[Batch 410/501] Loss: 1.7695, Batch Acc: 0.1250\n",
      "[Batch 420/501] Loss: 1.7777, Batch Acc: 0.1250\n",
      "[Batch 430/501] Loss: 1.6240, Batch Acc: 0.2500\n",
      "[Batch 440/501] Loss: 1.5327, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.4311, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.6999, Batch Acc: 0.1250\n",
      "[Batch 470/501] Loss: 1.7218, Batch Acc: 0.2500\n",
      "[Batch 480/501] Loss: 1.5655, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.9214, Batch Acc: 0.1250\n",
      "[Batch 500/501] Loss: 1.7663, Batch Acc: 0.2500\n",
      "Epoch 4 Summary - Loss: 839.7822, Train Accuracy: 0.2163\n",
      "[Batch 10/501] Loss: 1.6622, Batch Acc: 0.2500\n",
      "[Batch 20/501] Loss: 1.5127, Batch Acc: 0.1250\n",
      "[Batch 30/501] Loss: 1.7662, Batch Acc: 0.2500\n",
      "[Batch 40/501] Loss: 1.6228, Batch Acc: 0.1250\n",
      "[Batch 50/501] Loss: 1.6474, Batch Acc: 0.3750\n",
      "[Batch 60/501] Loss: 1.5461, Batch Acc: 0.6250\n",
      "[Batch 70/501] Loss: 1.7578, Batch Acc: 0.0000\n",
      "[Batch 80/501] Loss: 1.3917, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.5471, Batch Acc: 0.1250\n",
      "[Batch 100/501] Loss: 1.5733, Batch Acc: 0.1250\n",
      "[Batch 110/501] Loss: 1.6585, Batch Acc: 0.1250\n",
      "[Batch 120/501] Loss: 1.5678, Batch Acc: 0.3750\n",
      "[Batch 130/501] Loss: 1.8434, Batch Acc: 0.0000\n",
      "[Batch 140/501] Loss: 1.5977, Batch Acc: 0.0000\n",
      "[Batch 150/501] Loss: 1.4767, Batch Acc: 0.3750\n",
      "[Batch 160/501] Loss: 1.6998, Batch Acc: 0.1250\n",
      "[Batch 170/501] Loss: 1.7706, Batch Acc: 0.2500\n",
      "[Batch 180/501] Loss: 1.6669, Batch Acc: 0.2500\n",
      "[Batch 190/501] Loss: 1.5889, Batch Acc: 0.2500\n",
      "[Batch 200/501] Loss: 1.8401, Batch Acc: 0.2500\n",
      "[Batch 210/501] Loss: 1.8507, Batch Acc: 0.1250\n",
      "[Batch 220/501] Loss: 1.8798, Batch Acc: 0.0000\n",
      "[Batch 230/501] Loss: 1.6090, Batch Acc: 0.1250\n",
      "[Batch 240/501] Loss: 1.8027, Batch Acc: 0.1250\n",
      "[Batch 250/501] Loss: 1.8954, Batch Acc: 0.0000\n",
      "[Batch 260/501] Loss: 1.5458, Batch Acc: 0.3750\n",
      "[Batch 270/501] Loss: 1.7719, Batch Acc: 0.0000\n",
      "[Batch 280/501] Loss: 1.7862, Batch Acc: 0.2500\n",
      "[Batch 290/501] Loss: 1.5445, Batch Acc: 0.5000\n",
      "[Batch 300/501] Loss: 1.6070, Batch Acc: 0.0000\n",
      "[Batch 310/501] Loss: 1.8252, Batch Acc: 0.1250\n",
      "[Batch 320/501] Loss: 1.7986, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 1.7937, Batch Acc: 0.2500\n",
      "[Batch 340/501] Loss: 1.6473, Batch Acc: 0.2500\n",
      "[Batch 350/501] Loss: 1.4401, Batch Acc: 0.3750\n",
      "[Batch 360/501] Loss: 1.9452, Batch Acc: 0.0000\n",
      "[Batch 370/501] Loss: 1.7630, Batch Acc: 0.2500\n",
      "[Batch 380/501] Loss: 1.9515, Batch Acc: 0.1250\n",
      "[Batch 390/501] Loss: 1.5982, Batch Acc: 0.2500\n",
      "[Batch 400/501] Loss: 1.6882, Batch Acc: 0.2500\n",
      "[Batch 410/501] Loss: 1.4199, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.7393, Batch Acc: 0.1250\n",
      "[Batch 430/501] Loss: 1.6127, Batch Acc: 0.1250\n",
      "[Batch 440/501] Loss: 1.4491, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.6805, Batch Acc: 0.1250\n",
      "[Batch 460/501] Loss: 1.5098, Batch Acc: 0.3750\n",
      "[Batch 470/501] Loss: 1.5926, Batch Acc: 0.3750\n",
      "[Batch 480/501] Loss: 1.4080, Batch Acc: 0.1250\n",
      "[Batch 490/501] Loss: 1.6558, Batch Acc: 0.2500\n",
      "[Batch 500/501] Loss: 1.6255, Batch Acc: 0.2500\n",
      "Epoch 5 Summary - Loss: 839.4431, Train Accuracy: 0.2046\n",
      "[Batch 10/501] Loss: 1.8440, Batch Acc: 0.1250\n",
      "[Batch 20/501] Loss: 1.7354, Batch Acc: 0.1250\n",
      "[Batch 30/501] Loss: 1.7633, Batch Acc: 0.3750\n",
      "[Batch 40/501] Loss: 1.9232, Batch Acc: 0.0000\n",
      "[Batch 50/501] Loss: 1.5609, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.6540, Batch Acc: 0.1250\n",
      "[Batch 70/501] Loss: 1.9203, Batch Acc: 0.0000\n",
      "[Batch 80/501] Loss: 1.8464, Batch Acc: 0.0000\n",
      "[Batch 90/501] Loss: 1.4943, Batch Acc: 0.3750\n",
      "[Batch 100/501] Loss: 1.5130, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 1.6431, Batch Acc: 0.2500\n",
      "[Batch 120/501] Loss: 1.5794, Batch Acc: 0.1250\n",
      "[Batch 130/501] Loss: 1.2520, Batch Acc: 0.5000\n",
      "[Batch 140/501] Loss: 1.6479, Batch Acc: 0.2500\n",
      "[Batch 150/501] Loss: 1.6815, Batch Acc: 0.0000\n",
      "[Batch 160/501] Loss: 1.5949, Batch Acc: 0.2500\n",
      "[Batch 170/501] Loss: 1.5874, Batch Acc: 0.2500\n",
      "[Batch 180/501] Loss: 1.7676, Batch Acc: 0.0000\n",
      "[Batch 190/501] Loss: 1.3998, Batch Acc: 0.5000\n",
      "[Batch 200/501] Loss: 1.9084, Batch Acc: 0.0000\n",
      "[Batch 210/501] Loss: 1.7473, Batch Acc: 0.2500\n",
      "[Batch 220/501] Loss: 1.4758, Batch Acc: 0.3750\n",
      "[Batch 230/501] Loss: 1.6944, Batch Acc: 0.1250\n",
      "[Batch 240/501] Loss: 1.4928, Batch Acc: 0.2500\n",
      "[Batch 250/501] Loss: 1.7564, Batch Acc: 0.1250\n",
      "[Batch 260/501] Loss: 1.9455, Batch Acc: 0.1250\n",
      "[Batch 270/501] Loss: 1.6671, Batch Acc: 0.0000\n",
      "[Batch 280/501] Loss: 1.7084, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 1.5654, Batch Acc: 0.2500\n",
      "[Batch 300/501] Loss: 1.7262, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 1.8732, Batch Acc: 0.0000\n",
      "[Batch 320/501] Loss: 1.7071, Batch Acc: 0.1250\n",
      "[Batch 330/501] Loss: 1.5157, Batch Acc: 0.2500\n",
      "[Batch 340/501] Loss: 2.0107, Batch Acc: 0.1250\n",
      "[Batch 350/501] Loss: 1.5649, Batch Acc: 0.2500\n",
      "[Batch 360/501] Loss: 1.6468, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 1.8805, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.4362, Batch Acc: 0.3750\n",
      "[Batch 390/501] Loss: 1.6781, Batch Acc: 0.2500\n",
      "[Batch 400/501] Loss: 1.6388, Batch Acc: 0.1250\n",
      "[Batch 410/501] Loss: 1.5179, Batch Acc: 0.3750\n",
      "[Batch 420/501] Loss: 1.5771, Batch Acc: 0.3750\n",
      "[Batch 430/501] Loss: 1.3478, Batch Acc: 0.3750\n",
      "[Batch 440/501] Loss: 1.5822, Batch Acc: 0.1250\n",
      "[Batch 450/501] Loss: 1.6793, Batch Acc: 0.0000\n",
      "[Batch 460/501] Loss: 1.4104, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.5538, Batch Acc: 0.3750\n",
      "[Batch 480/501] Loss: 1.6276, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 2.2307, Batch Acc: 0.0000\n",
      "[Batch 500/501] Loss: 1.4919, Batch Acc: 0.5000\n",
      "Epoch 6 Summary - Loss: 838.5622, Train Accuracy: 0.2086\n",
      "[Batch 10/501] Loss: 1.5491, Batch Acc: 0.2500\n",
      "[Batch 20/501] Loss: 1.5962, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.5669, Batch Acc: 0.2500\n",
      "[Batch 40/501] Loss: 1.7821, Batch Acc: 0.1250\n",
      "[Batch 50/501] Loss: 1.5075, Batch Acc: 0.3750\n",
      "[Batch 60/501] Loss: 1.6632, Batch Acc: 0.1250\n",
      "[Batch 70/501] Loss: 1.5342, Batch Acc: 0.3750\n",
      "[Batch 80/501] Loss: 1.6396, Batch Acc: 0.1250\n",
      "[Batch 90/501] Loss: 1.4334, Batch Acc: 0.3750\n",
      "[Batch 100/501] Loss: 1.7281, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 1.6748, Batch Acc: 0.1250\n",
      "[Batch 120/501] Loss: 1.7256, Batch Acc: 0.2500\n",
      "[Batch 130/501] Loss: 1.9255, Batch Acc: 0.0000\n",
      "[Batch 140/501] Loss: 1.8400, Batch Acc: 0.2500\n",
      "[Batch 150/501] Loss: 1.8019, Batch Acc: 0.1250\n",
      "[Batch 160/501] Loss: 1.7025, Batch Acc: 0.1250\n",
      "[Batch 170/501] Loss: 1.7446, Batch Acc: 0.3750\n",
      "[Batch 180/501] Loss: 1.4803, Batch Acc: 0.0000\n",
      "[Batch 190/501] Loss: 1.6694, Batch Acc: 0.2500\n",
      "[Batch 200/501] Loss: 1.5516, Batch Acc: 0.2500\n",
      "[Batch 210/501] Loss: 1.5151, Batch Acc: 0.5000\n",
      "[Batch 220/501] Loss: 1.6046, Batch Acc: 0.1250\n",
      "[Batch 230/501] Loss: 1.6750, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.5374, Batch Acc: 0.3750\n",
      "[Batch 250/501] Loss: 1.6241, Batch Acc: 0.1250\n",
      "[Batch 260/501] Loss: 1.7038, Batch Acc: 0.2500\n",
      "[Batch 270/501] Loss: 1.4534, Batch Acc: 0.3750\n",
      "[Batch 280/501] Loss: 1.5952, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 1.9188, Batch Acc: 0.1250\n",
      "[Batch 300/501] Loss: 1.9464, Batch Acc: 0.1250\n",
      "[Batch 310/501] Loss: 1.7217, Batch Acc: 0.1250\n",
      "[Batch 320/501] Loss: 1.6314, Batch Acc: 0.3750\n",
      "[Batch 330/501] Loss: 1.7498, Batch Acc: 0.0000\n",
      "[Batch 340/501] Loss: 1.8314, Batch Acc: 0.0000\n",
      "[Batch 350/501] Loss: 1.5813, Batch Acc: 0.0000\n",
      "[Batch 360/501] Loss: 1.8863, Batch Acc: 0.1250\n",
      "[Batch 370/501] Loss: 1.8216, Batch Acc: 0.0000\n",
      "[Batch 380/501] Loss: 1.6420, Batch Acc: 0.3750\n",
      "[Batch 390/501] Loss: 1.7353, Batch Acc: 0.2500\n",
      "[Batch 400/501] Loss: 1.7693, Batch Acc: 0.0000\n",
      "[Batch 410/501] Loss: 1.6489, Batch Acc: 0.1250\n",
      "[Batch 420/501] Loss: 1.9754, Batch Acc: 0.1250\n",
      "[Batch 430/501] Loss: 1.5715, Batch Acc: 0.1250\n",
      "[Batch 440/501] Loss: 1.5408, Batch Acc: 0.5000\n",
      "[Batch 450/501] Loss: 1.8275, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.7686, Batch Acc: 0.1250\n",
      "[Batch 470/501] Loss: 1.9058, Batch Acc: 0.2500\n",
      "[Batch 480/501] Loss: 2.0882, Batch Acc: 0.1250\n",
      "[Batch 490/501] Loss: 1.7628, Batch Acc: 0.1250\n",
      "[Batch 500/501] Loss: 1.7970, Batch Acc: 0.3750\n",
      "Epoch 7 Summary - Loss: 839.5309, Train Accuracy: 0.2188\n",
      "[Batch 10/501] Loss: 1.8960, Batch Acc: 0.1250\n",
      "[Batch 20/501] Loss: 1.7011, Batch Acc: 0.1250\n",
      "[Batch 30/501] Loss: 1.5181, Batch Acc: 0.2500\n",
      "[Batch 40/501] Loss: 1.9137, Batch Acc: 0.0000\n",
      "[Batch 50/501] Loss: 1.4290, Batch Acc: 0.5000\n",
      "[Batch 60/501] Loss: 1.6454, Batch Acc: 0.2500\n",
      "[Batch 70/501] Loss: 1.9149, Batch Acc: 0.0000\n",
      "[Batch 80/501] Loss: 1.5168, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.7523, Batch Acc: 0.1250\n",
      "[Batch 100/501] Loss: 1.7634, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 1.9333, Batch Acc: 0.1250\n",
      "[Batch 120/501] Loss: 1.8189, Batch Acc: 0.1250\n",
      "[Batch 130/501] Loss: 1.8157, Batch Acc: 0.1250\n",
      "[Batch 140/501] Loss: 1.5145, Batch Acc: 0.2500\n",
      "[Batch 150/501] Loss: 1.4540, Batch Acc: 0.3750\n",
      "[Batch 160/501] Loss: 1.7664, Batch Acc: 0.2500\n",
      "[Batch 170/501] Loss: 1.7017, Batch Acc: 0.0000\n",
      "[Batch 180/501] Loss: 1.8814, Batch Acc: 0.1250\n",
      "[Batch 190/501] Loss: 1.7324, Batch Acc: 0.0000\n",
      "[Batch 200/501] Loss: 1.8897, Batch Acc: 0.1250\n",
      "[Batch 210/501] Loss: 1.7761, Batch Acc: 0.2500\n",
      "[Batch 220/501] Loss: 1.9212, Batch Acc: 0.1250\n",
      "[Batch 230/501] Loss: 1.4381, Batch Acc: 0.3750\n",
      "[Batch 240/501] Loss: 1.8079, Batch Acc: 0.1250\n",
      "[Batch 250/501] Loss: 1.7444, Batch Acc: 0.1250\n",
      "[Batch 260/501] Loss: 1.5664, Batch Acc: 0.3750\n",
      "[Batch 270/501] Loss: 1.7745, Batch Acc: 0.2500\n",
      "[Batch 280/501] Loss: 1.6290, Batch Acc: 0.2500\n",
      "[Batch 290/501] Loss: 1.4528, Batch Acc: 0.5000\n",
      "[Batch 300/501] Loss: 1.4837, Batch Acc: 0.3750\n",
      "[Batch 310/501] Loss: 1.6892, Batch Acc: 0.2500\n",
      "[Batch 320/501] Loss: 1.5363, Batch Acc: 0.1250\n",
      "[Batch 330/501] Loss: 1.5954, Batch Acc: 0.2500\n",
      "[Batch 340/501] Loss: 1.8079, Batch Acc: 0.0000\n",
      "[Batch 350/501] Loss: 1.9963, Batch Acc: 0.1250\n",
      "[Batch 360/501] Loss: 1.9458, Batch Acc: 0.0000\n",
      "[Batch 370/501] Loss: 1.2873, Batch Acc: 0.6250\n",
      "[Batch 380/501] Loss: 1.9930, Batch Acc: 0.0000\n",
      "[Batch 390/501] Loss: 1.9242, Batch Acc: 0.1250\n",
      "[Batch 400/501] Loss: 1.6585, Batch Acc: 0.1250\n",
      "[Batch 410/501] Loss: 1.5092, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.9947, Batch Acc: 0.1250\n",
      "[Batch 430/501] Loss: 1.6873, Batch Acc: 0.0000\n",
      "[Batch 440/501] Loss: 1.8372, Batch Acc: 0.0000\n",
      "[Batch 450/501] Loss: 1.7241, Batch Acc: 0.3750\n",
      "[Batch 460/501] Loss: 1.8176, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.7156, Batch Acc: 0.2500\n",
      "[Batch 480/501] Loss: 1.6521, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.6285, Batch Acc: 0.1250\n",
      "[Batch 500/501] Loss: 1.7282, Batch Acc: 0.1250\n",
      "Epoch 8 Summary - Loss: 837.9519, Train Accuracy: 0.2133\n",
      "[Batch 10/501] Loss: 1.5790, Batch Acc: 0.2500\n",
      "[Batch 20/501] Loss: 1.6607, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.5536, Batch Acc: 0.2500\n",
      "[Batch 40/501] Loss: 1.5522, Batch Acc: 0.2500\n",
      "[Batch 50/501] Loss: 1.7843, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.9579, Batch Acc: 0.0000\n",
      "[Batch 70/501] Loss: 1.4642, Batch Acc: 0.3750\n",
      "[Batch 80/501] Loss: 1.6361, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.5491, Batch Acc: 0.1250\n",
      "[Batch 100/501] Loss: 1.9302, Batch Acc: 0.1250\n",
      "[Batch 110/501] Loss: 1.6385, Batch Acc: 0.1250\n",
      "[Batch 120/501] Loss: 1.5182, Batch Acc: 0.3750\n",
      "[Batch 130/501] Loss: 1.3384, Batch Acc: 0.5000\n",
      "[Batch 140/501] Loss: 1.3789, Batch Acc: 0.2500\n",
      "[Batch 150/501] Loss: 1.8974, Batch Acc: 0.2500\n",
      "[Batch 160/501] Loss: 1.7044, Batch Acc: 0.3750\n",
      "[Batch 170/501] Loss: 1.6675, Batch Acc: 0.1250\n",
      "[Batch 180/501] Loss: 1.5369, Batch Acc: 0.1250\n",
      "[Batch 190/501] Loss: 1.6224, Batch Acc: 0.1250\n",
      "[Batch 200/501] Loss: 1.9675, Batch Acc: 0.3750\n",
      "[Batch 210/501] Loss: 1.6460, Batch Acc: 0.1250\n",
      "[Batch 220/501] Loss: 1.7512, Batch Acc: 0.1250\n",
      "[Batch 230/501] Loss: 1.6042, Batch Acc: 0.3750\n",
      "[Batch 240/501] Loss: 1.5761, Batch Acc: 0.2500\n",
      "[Batch 250/501] Loss: 1.7774, Batch Acc: 0.2500\n",
      "[Batch 260/501] Loss: 1.8133, Batch Acc: 0.0000\n",
      "[Batch 270/501] Loss: 1.7419, Batch Acc: 0.1250\n",
      "[Batch 280/501] Loss: 1.5549, Batch Acc: 0.2500\n",
      "[Batch 290/501] Loss: 1.3560, Batch Acc: 0.5000\n",
      "[Batch 300/501] Loss: 1.6303, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 1.6535, Batch Acc: 0.6250\n",
      "[Batch 320/501] Loss: 1.3247, Batch Acc: 0.3750\n",
      "[Batch 330/501] Loss: 1.8922, Batch Acc: 0.0000\n",
      "[Batch 340/501] Loss: 1.5743, Batch Acc: 0.2500\n",
      "[Batch 350/501] Loss: 1.3758, Batch Acc: 0.5000\n",
      "[Batch 360/501] Loss: 1.5937, Batch Acc: 0.3750\n",
      "[Batch 370/501] Loss: 1.4272, Batch Acc: 0.3750\n",
      "[Batch 380/501] Loss: 1.8560, Batch Acc: 0.2500\n",
      "[Batch 390/501] Loss: 1.6476, Batch Acc: 0.1250\n",
      "[Batch 400/501] Loss: 1.6991, Batch Acc: 0.2500\n",
      "[Batch 410/501] Loss: 1.6924, Batch Acc: 0.1250\n",
      "[Batch 420/501] Loss: 1.5752, Batch Acc: 0.5000\n",
      "[Batch 430/501] Loss: 1.5185, Batch Acc: 0.3750\n",
      "[Batch 440/501] Loss: 1.8665, Batch Acc: 0.1250\n",
      "[Batch 450/501] Loss: 1.7480, Batch Acc: 0.1250\n",
      "[Batch 460/501] Loss: 1.6361, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.5270, Batch Acc: 0.5000\n",
      "[Batch 480/501] Loss: 1.5942, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.5116, Batch Acc: 0.2500\n",
      "[Batch 500/501] Loss: 1.9641, Batch Acc: 0.0000\n",
      "Epoch 9 Summary - Loss: 839.8750, Train Accuracy: 0.2088\n",
      "[Batch 10/501] Loss: 1.7331, Batch Acc: 0.1250\n",
      "[Batch 20/501] Loss: 1.7758, Batch Acc: 0.1250\n",
      "[Batch 30/501] Loss: 1.9505, Batch Acc: 0.2500\n",
      "[Batch 40/501] Loss: 1.5758, Batch Acc: 0.1250\n",
      "[Batch 50/501] Loss: 1.7290, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.6792, Batch Acc: 0.1250\n",
      "[Batch 70/501] Loss: 1.7558, Batch Acc: 0.1250\n",
      "[Batch 80/501] Loss: 1.6763, Batch Acc: 0.1250\n",
      "[Batch 90/501] Loss: 1.4768, Batch Acc: 0.2500\n",
      "[Batch 100/501] Loss: 1.5238, Batch Acc: 0.3750\n",
      "[Batch 110/501] Loss: 1.9549, Batch Acc: 0.0000\n",
      "[Batch 120/501] Loss: 1.7720, Batch Acc: 0.1250\n",
      "[Batch 130/501] Loss: 1.5826, Batch Acc: 0.2500\n",
      "[Batch 140/501] Loss: 2.0277, Batch Acc: 0.0000\n",
      "[Batch 150/501] Loss: 1.3581, Batch Acc: 0.2500\n",
      "[Batch 160/501] Loss: 1.5879, Batch Acc: 0.3750\n",
      "[Batch 170/501] Loss: 1.3888, Batch Acc: 0.2500\n",
      "[Batch 180/501] Loss: 1.7512, Batch Acc: 0.1250\n",
      "[Batch 190/501] Loss: 2.0144, Batch Acc: 0.2500\n",
      "[Batch 200/501] Loss: 1.7287, Batch Acc: 0.1250\n",
      "[Batch 210/501] Loss: 1.8934, Batch Acc: 0.3750\n",
      "[Batch 220/501] Loss: 1.7235, Batch Acc: 0.2500\n",
      "[Batch 230/501] Loss: 1.3516, Batch Acc: 0.5000\n",
      "[Batch 240/501] Loss: 2.0178, Batch Acc: 0.1250\n",
      "[Batch 250/501] Loss: 1.5626, Batch Acc: 0.3750\n",
      "[Batch 260/501] Loss: 1.8292, Batch Acc: 0.0000\n",
      "[Batch 270/501] Loss: 1.5292, Batch Acc: 0.1250\n",
      "[Batch 280/501] Loss: 1.6530, Batch Acc: 0.3750\n",
      "[Batch 290/501] Loss: 1.6229, Batch Acc: 0.2500\n",
      "[Batch 300/501] Loss: 1.5998, Batch Acc: 0.1250\n",
      "[Batch 310/501] Loss: 1.7324, Batch Acc: 0.0000\n",
      "[Batch 320/501] Loss: 1.9245, Batch Acc: 0.0000\n",
      "[Batch 330/501] Loss: 1.5447, Batch Acc: 0.1250\n",
      "[Batch 340/501] Loss: 1.6786, Batch Acc: 0.2500\n",
      "[Batch 350/501] Loss: 1.5005, Batch Acc: 0.5000\n",
      "[Batch 360/501] Loss: 1.6036, Batch Acc: 0.1250\n",
      "[Batch 370/501] Loss: 1.6454, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.7013, Batch Acc: 0.1250\n",
      "[Batch 390/501] Loss: 1.9131, Batch Acc: 0.1250\n",
      "[Batch 400/501] Loss: 1.9050, Batch Acc: 0.1250\n",
      "[Batch 410/501] Loss: 1.9245, Batch Acc: 0.0000\n",
      "[Batch 420/501] Loss: 1.7675, Batch Acc: 0.3750\n",
      "[Batch 430/501] Loss: 1.6313, Batch Acc: 0.2500\n",
      "[Batch 440/501] Loss: 1.5415, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.6678, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.5173, Batch Acc: 0.3750\n",
      "[Batch 470/501] Loss: 1.6563, Batch Acc: 0.2500\n",
      "[Batch 480/501] Loss: 1.9128, Batch Acc: 0.1250\n",
      "[Batch 490/501] Loss: 1.8031, Batch Acc: 0.3750\n",
      "[Batch 500/501] Loss: 1.5954, Batch Acc: 0.2500\n",
      "Epoch 10 Summary - Loss: 840.8917, Train Accuracy: 0.2176\n",
      "Test Accuracy: 0.2087\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_accuracy</td><td>▆▄▄▇▁▃█▅▃▇</td></tr><tr><td>train_loss</td><td>▇█▁▆▅▄▅▃▆▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>test_accuracy</td><td>0.2087</td></tr><tr><td>test_loss</td><td>209.5431</td></tr><tr><td>train_accuracy</td><td>0.21756</td></tr><tr><td>train_loss</td><td>840.89168</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">helpful-sweep-8</strong> at: <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/l5bgo6ha' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/l5bgo6ha</a><br> View project at: <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250615_141453-l5bgo6ha/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: l4r42d7q with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_nodes: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ajit.kumar4@happiestminds.com/Documents/Drackula/ComputerVisionFromScratch/wandb/run-20250615_141833-l4r42d7q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/l4r42d7q' target=\"_blank\">visionary-sweep-9</a></strong> to <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/l4r42d7q' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/l4r42d7q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 10/501] Loss: 1.3313, Batch Acc: 0.3750\n",
      "[Batch 20/501] Loss: 1.6538, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.5207, Batch Acc: 0.3750\n",
      "[Batch 40/501] Loss: 1.4907, Batch Acc: 0.3750\n",
      "[Batch 50/501] Loss: 1.5980, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.7337, Batch Acc: 0.2500\n",
      "[Batch 70/501] Loss: 1.5127, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.4460, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.6377, Batch Acc: 0.2500\n",
      "[Batch 100/501] Loss: 1.6560, Batch Acc: 0.0000\n",
      "[Batch 110/501] Loss: 1.5014, Batch Acc: 0.2500\n",
      "[Batch 120/501] Loss: 1.6931, Batch Acc: 0.2500\n",
      "[Batch 130/501] Loss: 1.3134, Batch Acc: 0.5000\n",
      "[Batch 140/501] Loss: 1.6391, Batch Acc: 0.3750\n",
      "[Batch 150/501] Loss: 1.6804, Batch Acc: 0.1250\n",
      "[Batch 160/501] Loss: 1.5279, Batch Acc: 0.2500\n",
      "[Batch 170/501] Loss: 1.4047, Batch Acc: 0.3750\n",
      "[Batch 180/501] Loss: 1.8031, Batch Acc: 0.2500\n",
      "[Batch 190/501] Loss: 1.5704, Batch Acc: 0.1250\n",
      "[Batch 200/501] Loss: 1.7460, Batch Acc: 0.1250\n",
      "[Batch 210/501] Loss: 1.3507, Batch Acc: 0.6250\n",
      "[Batch 220/501] Loss: 1.2125, Batch Acc: 0.6250\n",
      "[Batch 230/501] Loss: 1.8526, Batch Acc: 0.1250\n",
      "[Batch 240/501] Loss: 1.5126, Batch Acc: 0.5000\n",
      "[Batch 250/501] Loss: 1.7080, Batch Acc: 0.2500\n",
      "[Batch 260/501] Loss: 1.8214, Batch Acc: 0.1250\n",
      "[Batch 270/501] Loss: 1.4246, Batch Acc: 0.3750\n",
      "[Batch 280/501] Loss: 1.7853, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 1.4616, Batch Acc: 0.3750\n",
      "[Batch 300/501] Loss: 1.5465, Batch Acc: 0.3750\n",
      "[Batch 310/501] Loss: 1.6257, Batch Acc: 0.3750\n",
      "[Batch 320/501] Loss: 1.2825, Batch Acc: 0.3750\n",
      "[Batch 330/501] Loss: 1.7912, Batch Acc: 0.2500\n",
      "[Batch 340/501] Loss: 1.5480, Batch Acc: 0.1250\n",
      "[Batch 350/501] Loss: 1.6698, Batch Acc: 0.1250\n",
      "[Batch 360/501] Loss: 1.7240, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 1.6136, Batch Acc: 0.3750\n",
      "[Batch 380/501] Loss: 1.6685, Batch Acc: 0.3750\n",
      "[Batch 390/501] Loss: 1.5469, Batch Acc: 0.2500\n",
      "[Batch 400/501] Loss: 1.4796, Batch Acc: 0.5000\n",
      "[Batch 410/501] Loss: 1.5562, Batch Acc: 0.5000\n",
      "[Batch 420/501] Loss: 1.7229, Batch Acc: 0.2500\n",
      "[Batch 430/501] Loss: 1.4112, Batch Acc: 0.3750\n",
      "[Batch 440/501] Loss: 1.5862, Batch Acc: 0.3750\n",
      "[Batch 450/501] Loss: 1.5751, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.5641, Batch Acc: 0.3750\n",
      "[Batch 470/501] Loss: 1.6894, Batch Acc: 0.3750\n",
      "[Batch 480/501] Loss: 1.6957, Batch Acc: 0.1250\n",
      "[Batch 490/501] Loss: 1.3965, Batch Acc: 0.3750\n",
      "[Batch 500/501] Loss: 1.7887, Batch Acc: 0.1250\n",
      "Epoch 1 Summary - Loss: 804.4660, Train Accuracy: 0.2682\n",
      "[Batch 10/501] Loss: 1.6383, Batch Acc: 0.2500\n",
      "[Batch 20/501] Loss: 1.3256, Batch Acc: 0.5000\n",
      "[Batch 30/501] Loss: 1.4571, Batch Acc: 0.3750\n",
      "[Batch 40/501] Loss: 1.7857, Batch Acc: 0.1250\n",
      "[Batch 50/501] Loss: 1.7019, Batch Acc: 0.1250\n",
      "[Batch 60/501] Loss: 1.5603, Batch Acc: 0.3750\n",
      "[Batch 70/501] Loss: 1.6724, Batch Acc: 0.1250\n",
      "[Batch 80/501] Loss: 1.4900, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.6380, Batch Acc: 0.2500\n",
      "[Batch 100/501] Loss: 1.4336, Batch Acc: 0.3750\n",
      "[Batch 110/501] Loss: 1.5624, Batch Acc: 0.2500\n",
      "[Batch 120/501] Loss: 1.4091, Batch Acc: 0.6250\n",
      "[Batch 130/501] Loss: 1.4280, Batch Acc: 0.5000\n",
      "[Batch 140/501] Loss: 1.2731, Batch Acc: 0.6250\n",
      "[Batch 150/501] Loss: 1.8435, Batch Acc: 0.0000\n",
      "[Batch 160/501] Loss: 1.5195, Batch Acc: 0.2500\n",
      "[Batch 170/501] Loss: 1.5300, Batch Acc: 0.2500\n",
      "[Batch 180/501] Loss: 1.5731, Batch Acc: 0.2500\n",
      "[Batch 190/501] Loss: 1.3643, Batch Acc: 0.3750\n",
      "[Batch 200/501] Loss: 1.4760, Batch Acc: 0.2500\n",
      "[Batch 210/501] Loss: 1.3485, Batch Acc: 0.3750\n",
      "[Batch 220/501] Loss: 1.7521, Batch Acc: 0.2500\n",
      "[Batch 230/501] Loss: 1.8856, Batch Acc: 0.1250\n",
      "[Batch 240/501] Loss: 1.4310, Batch Acc: 0.6250\n",
      "[Batch 250/501] Loss: 1.6128, Batch Acc: 0.3750\n",
      "[Batch 260/501] Loss: 1.3497, Batch Acc: 0.3750\n",
      "[Batch 270/501] Loss: 1.7285, Batch Acc: 0.2500\n",
      "[Batch 280/501] Loss: 1.8279, Batch Acc: 0.2500\n",
      "[Batch 290/501] Loss: 1.7794, Batch Acc: 0.1250\n",
      "[Batch 300/501] Loss: 1.6980, Batch Acc: 0.1250\n",
      "[Batch 310/501] Loss: 1.4635, Batch Acc: 0.3750\n",
      "[Batch 320/501] Loss: 1.7891, Batch Acc: 0.0000\n",
      "[Batch 330/501] Loss: 1.7329, Batch Acc: 0.2500\n",
      "[Batch 340/501] Loss: 1.6723, Batch Acc: 0.2500\n",
      "[Batch 350/501] Loss: 1.3642, Batch Acc: 0.2500\n",
      "[Batch 360/501] Loss: 1.5974, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 1.8281, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.4987, Batch Acc: 0.5000\n",
      "[Batch 390/501] Loss: 1.8668, Batch Acc: 0.1250\n",
      "[Batch 400/501] Loss: 1.4456, Batch Acc: 0.3750\n",
      "[Batch 410/501] Loss: 1.4017, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.7558, Batch Acc: 0.2500\n",
      "[Batch 430/501] Loss: 1.4955, Batch Acc: 0.6250\n",
      "[Batch 440/501] Loss: 1.4566, Batch Acc: 0.5000\n",
      "[Batch 450/501] Loss: 1.7800, Batch Acc: 0.0000\n",
      "[Batch 460/501] Loss: 1.5896, Batch Acc: 0.1250\n",
      "[Batch 470/501] Loss: 1.8221, Batch Acc: 0.1250\n",
      "[Batch 480/501] Loss: 1.2369, Batch Acc: 0.3750\n",
      "[Batch 490/501] Loss: 1.2576, Batch Acc: 0.2500\n",
      "[Batch 500/501] Loss: 1.6245, Batch Acc: 0.3750\n",
      "Epoch 2 Summary - Loss: 801.5868, Train Accuracy: 0.2702\n",
      "[Batch 10/501] Loss: 1.5345, Batch Acc: 0.3750\n",
      "[Batch 20/501] Loss: 1.4512, Batch Acc: 0.3750\n",
      "[Batch 30/501] Loss: 1.6033, Batch Acc: 0.2500\n",
      "[Batch 40/501] Loss: 1.7310, Batch Acc: 0.1250\n",
      "[Batch 50/501] Loss: 1.6063, Batch Acc: 0.3750\n",
      "[Batch 60/501] Loss: 1.4774, Batch Acc: 0.3750\n",
      "[Batch 70/501] Loss: 1.5791, Batch Acc: 0.3750\n",
      "[Batch 80/501] Loss: 1.5179, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.7231, Batch Acc: 0.1250\n",
      "[Batch 100/501] Loss: 1.2512, Batch Acc: 0.3750\n",
      "[Batch 110/501] Loss: 1.4776, Batch Acc: 0.2500\n",
      "[Batch 120/501] Loss: 1.5274, Batch Acc: 0.1250\n",
      "[Batch 130/501] Loss: 1.4625, Batch Acc: 0.5000\n",
      "[Batch 140/501] Loss: 1.7138, Batch Acc: 0.1250\n",
      "[Batch 150/501] Loss: 1.9568, Batch Acc: 0.0000\n",
      "[Batch 160/501] Loss: 1.8600, Batch Acc: 0.1250\n",
      "[Batch 170/501] Loss: 1.3225, Batch Acc: 0.5000\n",
      "[Batch 180/501] Loss: 1.5949, Batch Acc: 0.2500\n",
      "[Batch 190/501] Loss: 1.5799, Batch Acc: 0.1250\n",
      "[Batch 200/501] Loss: 1.6626, Batch Acc: 0.0000\n",
      "[Batch 210/501] Loss: 1.6958, Batch Acc: 0.2500\n",
      "[Batch 220/501] Loss: 1.4200, Batch Acc: 0.3750\n",
      "[Batch 230/501] Loss: 1.7140, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.6270, Batch Acc: 0.3750\n",
      "[Batch 250/501] Loss: 1.3559, Batch Acc: 0.3750\n",
      "[Batch 260/501] Loss: 1.4541, Batch Acc: 0.5000\n",
      "[Batch 270/501] Loss: 1.7905, Batch Acc: 0.1250\n",
      "[Batch 280/501] Loss: 1.4953, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 1.5642, Batch Acc: 0.1250\n",
      "[Batch 300/501] Loss: 1.6130, Batch Acc: 0.1250\n",
      "[Batch 310/501] Loss: 1.4002, Batch Acc: 0.3750\n",
      "[Batch 320/501] Loss: 1.4252, Batch Acc: 0.3750\n",
      "[Batch 330/501] Loss: 1.3520, Batch Acc: 0.5000\n",
      "[Batch 340/501] Loss: 1.6341, Batch Acc: 0.2500\n",
      "[Batch 350/501] Loss: 1.7821, Batch Acc: 0.5000\n",
      "[Batch 360/501] Loss: 1.7119, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 1.8027, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.3320, Batch Acc: 0.5000\n",
      "[Batch 390/501] Loss: 1.5099, Batch Acc: 0.3750\n",
      "[Batch 400/501] Loss: 1.5291, Batch Acc: 0.2500\n",
      "[Batch 410/501] Loss: 1.4653, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.5927, Batch Acc: 0.2500\n",
      "[Batch 430/501] Loss: 1.5956, Batch Acc: 0.2500\n",
      "[Batch 440/501] Loss: 1.5533, Batch Acc: 0.3750\n",
      "[Batch 450/501] Loss: 1.3222, Batch Acc: 0.3750\n",
      "[Batch 460/501] Loss: 1.6257, Batch Acc: 0.3750\n",
      "[Batch 470/501] Loss: 1.1978, Batch Acc: 0.8750\n",
      "[Batch 480/501] Loss: 1.4552, Batch Acc: 0.3750\n",
      "[Batch 490/501] Loss: 1.4808, Batch Acc: 0.3750\n",
      "[Batch 500/501] Loss: 1.7157, Batch Acc: 0.3750\n",
      "Epoch 3 Summary - Loss: 800.2182, Train Accuracy: 0.2799\n",
      "[Batch 10/501] Loss: 1.5359, Batch Acc: 0.2500\n",
      "[Batch 20/501] Loss: 1.7319, Batch Acc: 0.1250\n",
      "[Batch 30/501] Loss: 1.6644, Batch Acc: 0.2500\n",
      "[Batch 40/501] Loss: 1.8001, Batch Acc: 0.0000\n",
      "[Batch 50/501] Loss: 1.4524, Batch Acc: 0.3750\n",
      "[Batch 60/501] Loss: 1.5959, Batch Acc: 0.3750\n",
      "[Batch 70/501] Loss: 1.6252, Batch Acc: 0.5000\n",
      "[Batch 80/501] Loss: 1.6232, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.5161, Batch Acc: 0.2500\n",
      "[Batch 100/501] Loss: 1.5945, Batch Acc: 0.1250\n",
      "[Batch 110/501] Loss: 1.3488, Batch Acc: 0.5000\n",
      "[Batch 120/501] Loss: 1.7379, Batch Acc: 0.0000\n",
      "[Batch 130/501] Loss: 1.3275, Batch Acc: 0.6250\n",
      "[Batch 140/501] Loss: 1.7442, Batch Acc: 0.0000\n",
      "[Batch 150/501] Loss: 1.4300, Batch Acc: 0.5000\n",
      "[Batch 160/501] Loss: 1.5673, Batch Acc: 0.3750\n",
      "[Batch 170/501] Loss: 1.7737, Batch Acc: 0.1250\n",
      "[Batch 180/501] Loss: 1.5542, Batch Acc: 0.3750\n",
      "[Batch 190/501] Loss: 1.8626, Batch Acc: 0.1250\n",
      "[Batch 200/501] Loss: 1.5061, Batch Acc: 0.2500\n",
      "[Batch 210/501] Loss: 1.6929, Batch Acc: 0.0000\n",
      "[Batch 220/501] Loss: 1.4273, Batch Acc: 0.5000\n",
      "[Batch 230/501] Loss: 1.4526, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.7070, Batch Acc: 0.2500\n",
      "[Batch 250/501] Loss: 1.6568, Batch Acc: 0.1250\n",
      "[Batch 260/501] Loss: 1.5547, Batch Acc: 0.3750\n",
      "[Batch 270/501] Loss: 1.3435, Batch Acc: 0.6250\n",
      "[Batch 280/501] Loss: 1.7390, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 1.9578, Batch Acc: 0.0000\n",
      "[Batch 300/501] Loss: 1.6950, Batch Acc: 0.1250\n",
      "[Batch 310/501] Loss: 1.6551, Batch Acc: 0.1250\n",
      "[Batch 320/501] Loss: 1.4982, Batch Acc: 0.3750\n",
      "[Batch 330/501] Loss: 2.0214, Batch Acc: 0.0000\n",
      "[Batch 340/501] Loss: 1.4505, Batch Acc: 0.3750\n",
      "[Batch 350/501] Loss: 1.8835, Batch Acc: 0.3750\n",
      "[Batch 360/501] Loss: 1.7637, Batch Acc: 0.1250\n",
      "[Batch 370/501] Loss: 1.8000, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.8594, Batch Acc: 0.1250\n",
      "[Batch 390/501] Loss: 1.5851, Batch Acc: 0.5000\n",
      "[Batch 400/501] Loss: 1.8110, Batch Acc: 0.0000\n",
      "[Batch 410/501] Loss: 1.2352, Batch Acc: 0.6250\n",
      "[Batch 420/501] Loss: 1.5238, Batch Acc: 0.3750\n",
      "[Batch 430/501] Loss: 1.4599, Batch Acc: 0.2500\n",
      "[Batch 440/501] Loss: 1.3408, Batch Acc: 0.6250\n",
      "[Batch 450/501] Loss: 1.6765, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.6402, Batch Acc: 0.1250\n",
      "[Batch 470/501] Loss: 1.3721, Batch Acc: 0.3750\n",
      "[Batch 480/501] Loss: 1.5757, Batch Acc: 0.1250\n",
      "[Batch 490/501] Loss: 1.4055, Batch Acc: 0.3750\n",
      "[Batch 500/501] Loss: 1.2293, Batch Acc: 0.7500\n",
      "Epoch 4 Summary - Loss: 800.7936, Train Accuracy: 0.2680\n",
      "[Batch 10/501] Loss: 1.9128, Batch Acc: 0.0000\n",
      "[Batch 20/501] Loss: 1.5836, Batch Acc: 0.3750\n",
      "[Batch 30/501] Loss: 1.6919, Batch Acc: 0.2500\n",
      "[Batch 40/501] Loss: 1.7373, Batch Acc: 0.1250\n",
      "[Batch 50/501] Loss: 1.8040, Batch Acc: 0.3750\n",
      "[Batch 60/501] Loss: 1.2346, Batch Acc: 0.7500\n",
      "[Batch 70/501] Loss: 1.3722, Batch Acc: 0.5000\n",
      "[Batch 80/501] Loss: 1.8915, Batch Acc: 0.0000\n",
      "[Batch 90/501] Loss: 1.4754, Batch Acc: 0.5000\n",
      "[Batch 100/501] Loss: 1.6306, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 1.3286, Batch Acc: 0.6250\n",
      "[Batch 120/501] Loss: 1.6892, Batch Acc: 0.1250\n",
      "[Batch 130/501] Loss: 1.5998, Batch Acc: 0.1250\n",
      "[Batch 140/501] Loss: 1.3334, Batch Acc: 0.5000\n",
      "[Batch 150/501] Loss: 1.4136, Batch Acc: 0.5000\n",
      "[Batch 160/501] Loss: 1.5907, Batch Acc: 0.2500\n",
      "[Batch 170/501] Loss: 1.4808, Batch Acc: 0.3750\n",
      "[Batch 180/501] Loss: 1.6260, Batch Acc: 0.3750\n",
      "[Batch 190/501] Loss: 1.8134, Batch Acc: 0.1250\n",
      "[Batch 200/501] Loss: 1.7338, Batch Acc: 0.1250\n",
      "[Batch 210/501] Loss: 1.4595, Batch Acc: 0.2500\n",
      "[Batch 220/501] Loss: 1.4127, Batch Acc: 0.3750\n",
      "[Batch 230/501] Loss: 1.7525, Batch Acc: 0.0000\n",
      "[Batch 240/501] Loss: 1.6049, Batch Acc: 0.2500\n",
      "[Batch 250/501] Loss: 1.5199, Batch Acc: 0.3750\n",
      "[Batch 260/501] Loss: 1.4547, Batch Acc: 0.2500\n",
      "[Batch 270/501] Loss: 1.3494, Batch Acc: 0.5000\n",
      "[Batch 280/501] Loss: 1.5604, Batch Acc: 0.2500\n",
      "[Batch 290/501] Loss: 1.6921, Batch Acc: 0.1250\n",
      "[Batch 300/501] Loss: 1.3459, Batch Acc: 0.3750\n",
      "[Batch 310/501] Loss: 1.5907, Batch Acc: 0.2500\n",
      "[Batch 320/501] Loss: 1.6413, Batch Acc: 0.1250\n",
      "[Batch 330/501] Loss: 1.5720, Batch Acc: 0.1250\n",
      "[Batch 340/501] Loss: 1.6182, Batch Acc: 0.1250\n",
      "[Batch 350/501] Loss: 1.6811, Batch Acc: 0.0000\n",
      "[Batch 360/501] Loss: 1.8341, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 1.8992, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.8790, Batch Acc: 0.1250\n",
      "[Batch 390/501] Loss: 2.0216, Batch Acc: 0.1250\n",
      "[Batch 400/501] Loss: 1.7950, Batch Acc: 0.1250\n",
      "[Batch 410/501] Loss: 1.6760, Batch Acc: 0.0000\n",
      "[Batch 420/501] Loss: 1.3371, Batch Acc: 0.5000\n",
      "[Batch 430/501] Loss: 1.3497, Batch Acc: 0.3750\n",
      "[Batch 440/501] Loss: 1.4811, Batch Acc: 0.3750\n",
      "[Batch 450/501] Loss: 1.6243, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.6213, Batch Acc: 0.3750\n",
      "[Batch 470/501] Loss: 1.4288, Batch Acc: 0.3750\n",
      "[Batch 480/501] Loss: 1.6170, Batch Acc: 0.1250\n",
      "[Batch 490/501] Loss: 1.2908, Batch Acc: 0.3750\n",
      "[Batch 500/501] Loss: 1.4727, Batch Acc: 0.5000\n",
      "Epoch 5 Summary - Loss: 803.9263, Train Accuracy: 0.2607\n",
      "[Batch 10/501] Loss: 1.5993, Batch Acc: 0.2500\n",
      "[Batch 20/501] Loss: 1.6209, Batch Acc: 0.3750\n",
      "[Batch 30/501] Loss: 1.8031, Batch Acc: 0.1250\n",
      "[Batch 40/501] Loss: 1.5400, Batch Acc: 0.3750\n",
      "[Batch 50/501] Loss: 1.7092, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.8165, Batch Acc: 0.2500\n",
      "[Batch 70/501] Loss: 1.5185, Batch Acc: 0.3750\n",
      "[Batch 80/501] Loss: 1.8088, Batch Acc: 0.1250\n",
      "[Batch 90/501] Loss: 1.5275, Batch Acc: 0.3750\n",
      "[Batch 100/501] Loss: 1.7094, Batch Acc: 0.1250\n",
      "[Batch 110/501] Loss: 1.6003, Batch Acc: 0.2500\n",
      "[Batch 120/501] Loss: 1.8547, Batch Acc: 0.1250\n",
      "[Batch 130/501] Loss: 1.7468, Batch Acc: 0.1250\n",
      "[Batch 140/501] Loss: 1.7351, Batch Acc: 0.2500\n",
      "[Batch 150/501] Loss: 1.5003, Batch Acc: 0.2500\n",
      "[Batch 160/501] Loss: 1.3699, Batch Acc: 0.3750\n",
      "[Batch 170/501] Loss: 1.5369, Batch Acc: 0.1250\n",
      "[Batch 180/501] Loss: 1.3993, Batch Acc: 0.2500\n",
      "[Batch 190/501] Loss: 1.7025, Batch Acc: 0.0000\n",
      "[Batch 200/501] Loss: 1.5965, Batch Acc: 0.2500\n",
      "[Batch 210/501] Loss: 1.5459, Batch Acc: 0.1250\n",
      "[Batch 220/501] Loss: 1.7560, Batch Acc: 0.1250\n",
      "[Batch 230/501] Loss: 1.4635, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.8384, Batch Acc: 0.0000\n",
      "[Batch 250/501] Loss: 1.5809, Batch Acc: 0.1250\n",
      "[Batch 260/501] Loss: 1.7310, Batch Acc: 0.1250\n",
      "[Batch 270/501] Loss: 1.7598, Batch Acc: 0.2500\n",
      "[Batch 280/501] Loss: 1.5949, Batch Acc: 0.2500\n",
      "[Batch 290/501] Loss: 1.6410, Batch Acc: 0.1250\n",
      "[Batch 300/501] Loss: 1.4825, Batch Acc: 0.5000\n",
      "[Batch 310/501] Loss: 1.5923, Batch Acc: 0.2500\n",
      "[Batch 320/501] Loss: 1.4364, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 1.6896, Batch Acc: 0.1250\n",
      "[Batch 340/501] Loss: 1.6680, Batch Acc: 0.2500\n",
      "[Batch 350/501] Loss: 1.7025, Batch Acc: 0.1250\n",
      "[Batch 360/501] Loss: 1.6715, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 1.2498, Batch Acc: 0.6250\n",
      "[Batch 380/501] Loss: 1.5300, Batch Acc: 0.3750\n",
      "[Batch 390/501] Loss: 1.7997, Batch Acc: 0.0000\n",
      "[Batch 400/501] Loss: 1.5180, Batch Acc: 0.2500\n",
      "[Batch 410/501] Loss: 1.6022, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.6536, Batch Acc: 0.1250\n",
      "[Batch 430/501] Loss: 1.5701, Batch Acc: 0.2500\n",
      "[Batch 440/501] Loss: 1.6410, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.4772, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.7630, Batch Acc: 0.1250\n",
      "[Batch 470/501] Loss: 1.3740, Batch Acc: 0.5000\n",
      "[Batch 480/501] Loss: 1.4730, Batch Acc: 0.3750\n",
      "[Batch 490/501] Loss: 1.5910, Batch Acc: 0.2500\n",
      "[Batch 500/501] Loss: 1.5586, Batch Acc: 0.2500\n",
      "Epoch 6 Summary - Loss: 804.4776, Train Accuracy: 0.2600\n",
      "[Batch 10/501] Loss: 1.5949, Batch Acc: 0.2500\n",
      "[Batch 20/501] Loss: 1.5649, Batch Acc: 0.5000\n",
      "[Batch 30/501] Loss: 1.7901, Batch Acc: 0.1250\n",
      "[Batch 40/501] Loss: 1.8054, Batch Acc: 0.0000\n",
      "[Batch 50/501] Loss: 1.6762, Batch Acc: 0.1250\n",
      "[Batch 60/501] Loss: 1.4164, Batch Acc: 0.5000\n",
      "[Batch 70/501] Loss: 1.5461, Batch Acc: 0.1250\n",
      "[Batch 80/501] Loss: 1.2876, Batch Acc: 0.3750\n",
      "[Batch 90/501] Loss: 1.5803, Batch Acc: 0.1250\n",
      "[Batch 100/501] Loss: 1.3994, Batch Acc: 0.3750\n",
      "[Batch 110/501] Loss: 1.5363, Batch Acc: 0.2500\n",
      "[Batch 120/501] Loss: 1.3804, Batch Acc: 0.5000\n",
      "[Batch 130/501] Loss: 1.4416, Batch Acc: 0.2500\n",
      "[Batch 140/501] Loss: 1.4529, Batch Acc: 0.2500\n",
      "[Batch 150/501] Loss: 1.3936, Batch Acc: 0.2500\n",
      "[Batch 160/501] Loss: 1.5280, Batch Acc: 0.2500\n",
      "[Batch 170/501] Loss: 1.6526, Batch Acc: 0.2500\n",
      "[Batch 180/501] Loss: 1.6143, Batch Acc: 0.2500\n",
      "[Batch 190/501] Loss: 1.9279, Batch Acc: 0.1250\n",
      "[Batch 200/501] Loss: 1.4124, Batch Acc: 0.3750\n",
      "[Batch 210/501] Loss: 1.4551, Batch Acc: 0.3750\n",
      "[Batch 220/501] Loss: 1.6205, Batch Acc: 0.2500\n",
      "[Batch 230/501] Loss: 1.6817, Batch Acc: 0.1250\n",
      "[Batch 240/501] Loss: 1.6805, Batch Acc: 0.1250\n",
      "[Batch 250/501] Loss: 1.5632, Batch Acc: 0.2500\n",
      "[Batch 260/501] Loss: 1.6027, Batch Acc: 0.2500\n",
      "[Batch 270/501] Loss: 1.7051, Batch Acc: 0.2500\n",
      "[Batch 280/501] Loss: 1.6852, Batch Acc: 0.2500\n",
      "[Batch 290/501] Loss: 1.2656, Batch Acc: 0.7500\n",
      "[Batch 300/501] Loss: 1.4935, Batch Acc: 0.5000\n",
      "[Batch 310/501] Loss: 1.7897, Batch Acc: 0.1250\n",
      "[Batch 320/501] Loss: 1.6410, Batch Acc: 0.3750\n",
      "[Batch 330/501] Loss: 1.6175, Batch Acc: 0.3750\n",
      "[Batch 340/501] Loss: 1.6625, Batch Acc: 0.1250\n",
      "[Batch 350/501] Loss: 1.4077, Batch Acc: 0.3750\n",
      "[Batch 360/501] Loss: 1.6118, Batch Acc: 0.3750\n",
      "[Batch 370/501] Loss: 1.8663, Batch Acc: 0.0000\n",
      "[Batch 380/501] Loss: 1.4883, Batch Acc: 0.3750\n",
      "[Batch 390/501] Loss: 1.6655, Batch Acc: 0.1250\n",
      "[Batch 400/501] Loss: 1.5793, Batch Acc: 0.3750\n",
      "[Batch 410/501] Loss: 1.5657, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.6885, Batch Acc: 0.3750\n",
      "[Batch 430/501] Loss: 1.3523, Batch Acc: 0.3750\n",
      "[Batch 440/501] Loss: 1.5311, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.5285, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.4253, Batch Acc: 0.5000\n",
      "[Batch 470/501] Loss: 1.5370, Batch Acc: 0.2500\n",
      "[Batch 480/501] Loss: 1.3872, Batch Acc: 0.5000\n",
      "[Batch 490/501] Loss: 1.7731, Batch Acc: 0.2500\n",
      "[Batch 500/501] Loss: 1.8708, Batch Acc: 0.1250\n",
      "Epoch 7 Summary - Loss: 799.7736, Train Accuracy: 0.2697\n",
      "[Batch 10/501] Loss: 1.6718, Batch Acc: 0.2500\n",
      "[Batch 20/501] Loss: 1.4932, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.5341, Batch Acc: 0.2500\n",
      "[Batch 40/501] Loss: 1.8293, Batch Acc: 0.2500\n",
      "[Batch 50/501] Loss: 1.7044, Batch Acc: 0.0000\n",
      "[Batch 60/501] Loss: 1.5222, Batch Acc: 0.2500\n",
      "[Batch 70/501] Loss: 1.6632, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.7291, Batch Acc: 0.0000\n",
      "[Batch 90/501] Loss: 1.4500, Batch Acc: 0.1250\n",
      "[Batch 100/501] Loss: 1.8213, Batch Acc: 0.3750\n",
      "[Batch 110/501] Loss: 1.6920, Batch Acc: 0.1250\n",
      "[Batch 120/501] Loss: 1.5801, Batch Acc: 0.1250\n",
      "[Batch 130/501] Loss: 1.8217, Batch Acc: 0.1250\n",
      "[Batch 140/501] Loss: 1.6087, Batch Acc: 0.1250\n",
      "[Batch 150/501] Loss: 1.5920, Batch Acc: 0.2500\n",
      "[Batch 160/501] Loss: 1.4965, Batch Acc: 0.3750\n",
      "[Batch 170/501] Loss: 1.7075, Batch Acc: 0.3750\n",
      "[Batch 180/501] Loss: 1.4752, Batch Acc: 0.2500\n",
      "[Batch 190/501] Loss: 1.4756, Batch Acc: 0.1250\n",
      "[Batch 200/501] Loss: 1.6001, Batch Acc: 0.2500\n",
      "[Batch 210/501] Loss: 1.9005, Batch Acc: 0.1250\n",
      "[Batch 220/501] Loss: 1.7283, Batch Acc: 0.0000\n",
      "[Batch 230/501] Loss: 1.6183, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.7739, Batch Acc: 0.1250\n",
      "[Batch 250/501] Loss: 1.4191, Batch Acc: 0.5000\n",
      "[Batch 260/501] Loss: 1.4876, Batch Acc: 0.2500\n",
      "[Batch 270/501] Loss: 1.7254, Batch Acc: 0.1250\n",
      "[Batch 280/501] Loss: 1.7820, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 1.5760, Batch Acc: 0.2500\n",
      "[Batch 300/501] Loss: 1.7033, Batch Acc: 0.3750\n",
      "[Batch 310/501] Loss: 1.7511, Batch Acc: 0.2500\n",
      "[Batch 320/501] Loss: 1.6503, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 1.6033, Batch Acc: 0.2500\n",
      "[Batch 340/501] Loss: 1.6031, Batch Acc: 0.1250\n",
      "[Batch 350/501] Loss: 1.3805, Batch Acc: 0.3750\n",
      "[Batch 360/501] Loss: 1.3301, Batch Acc: 0.3750\n",
      "[Batch 370/501] Loss: 1.4962, Batch Acc: 0.3750\n",
      "[Batch 380/501] Loss: 1.4981, Batch Acc: 0.3750\n",
      "[Batch 390/501] Loss: 1.5224, Batch Acc: 0.3750\n",
      "[Batch 400/501] Loss: 1.5135, Batch Acc: 0.3750\n",
      "[Batch 410/501] Loss: 1.3789, Batch Acc: 0.5000\n",
      "[Batch 420/501] Loss: 1.2588, Batch Acc: 0.6250\n",
      "[Batch 430/501] Loss: 1.6937, Batch Acc: 0.0000\n",
      "[Batch 440/501] Loss: 1.7872, Batch Acc: 0.1250\n",
      "[Batch 450/501] Loss: 1.6100, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.9589, Batch Acc: 0.0000\n",
      "[Batch 470/501] Loss: 1.5397, Batch Acc: 0.0000\n",
      "[Batch 480/501] Loss: 1.4664, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.4061, Batch Acc: 0.3750\n",
      "[Batch 500/501] Loss: 1.6665, Batch Acc: 0.1250\n",
      "Epoch 8 Summary - Loss: 798.9235, Train Accuracy: 0.2607\n",
      "[Batch 10/501] Loss: 1.3595, Batch Acc: 0.5000\n",
      "[Batch 20/501] Loss: 1.7860, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.5792, Batch Acc: 0.5000\n",
      "[Batch 40/501] Loss: 1.3456, Batch Acc: 0.5000\n",
      "[Batch 50/501] Loss: 1.6226, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.7721, Batch Acc: 0.2500\n",
      "[Batch 70/501] Loss: 1.5945, Batch Acc: 0.1250\n",
      "[Batch 80/501] Loss: 1.6565, Batch Acc: 0.3750\n",
      "[Batch 90/501] Loss: 1.6905, Batch Acc: 0.1250\n",
      "[Batch 100/501] Loss: 1.5125, Batch Acc: 0.3750\n",
      "[Batch 110/501] Loss: 1.8264, Batch Acc: 0.1250\n",
      "[Batch 120/501] Loss: 1.6847, Batch Acc: 0.2500\n",
      "[Batch 130/501] Loss: 1.6630, Batch Acc: 0.3750\n",
      "[Batch 140/501] Loss: 1.5986, Batch Acc: 0.3750\n",
      "[Batch 150/501] Loss: 1.5628, Batch Acc: 0.3750\n",
      "[Batch 160/501] Loss: 1.3793, Batch Acc: 0.6250\n",
      "[Batch 170/501] Loss: 1.5580, Batch Acc: 0.3750\n",
      "[Batch 180/501] Loss: 1.5085, Batch Acc: 0.2500\n",
      "[Batch 190/501] Loss: 1.2470, Batch Acc: 0.6250\n",
      "[Batch 200/501] Loss: 1.2821, Batch Acc: 0.6250\n",
      "[Batch 210/501] Loss: 1.6277, Batch Acc: 0.2500\n",
      "[Batch 220/501] Loss: 1.5588, Batch Acc: 0.1250\n",
      "[Batch 230/501] Loss: 1.4629, Batch Acc: 0.1250\n",
      "[Batch 240/501] Loss: 1.4893, Batch Acc: 0.2500\n",
      "[Batch 250/501] Loss: 1.6400, Batch Acc: 0.2500\n",
      "[Batch 260/501] Loss: 1.6555, Batch Acc: 0.2500\n",
      "[Batch 270/501] Loss: 1.5101, Batch Acc: 0.3750\n",
      "[Batch 280/501] Loss: 1.8791, Batch Acc: 0.0000\n",
      "[Batch 290/501] Loss: 1.5097, Batch Acc: 0.2500\n",
      "[Batch 300/501] Loss: 1.4933, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 1.7007, Batch Acc: 0.3750\n",
      "[Batch 320/501] Loss: 1.6136, Batch Acc: 0.5000\n",
      "[Batch 330/501] Loss: 1.7463, Batch Acc: 0.1250\n",
      "[Batch 340/501] Loss: 1.6098, Batch Acc: 0.3750\n",
      "[Batch 350/501] Loss: 1.4723, Batch Acc: 0.2500\n",
      "[Batch 360/501] Loss: 1.6768, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 1.5906, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.7026, Batch Acc: 0.0000\n",
      "[Batch 390/501] Loss: 1.6513, Batch Acc: 0.3750\n",
      "[Batch 400/501] Loss: 1.2840, Batch Acc: 0.3750\n",
      "[Batch 410/501] Loss: 1.4254, Batch Acc: 0.3750\n",
      "[Batch 420/501] Loss: 1.8424, Batch Acc: 0.1250\n",
      "[Batch 430/501] Loss: 1.7899, Batch Acc: 0.1250\n",
      "[Batch 440/501] Loss: 1.4906, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.5373, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.5040, Batch Acc: 0.3750\n",
      "[Batch 470/501] Loss: 1.5017, Batch Acc: 0.3750\n",
      "[Batch 480/501] Loss: 1.6652, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.4540, Batch Acc: 0.2500\n",
      "[Batch 500/501] Loss: 1.7819, Batch Acc: 0.1250\n",
      "Epoch 9 Summary - Loss: 799.6512, Train Accuracy: 0.2667\n",
      "[Batch 10/501] Loss: 1.6159, Batch Acc: 0.1250\n",
      "[Batch 20/501] Loss: 1.6227, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.7726, Batch Acc: 0.2500\n",
      "[Batch 40/501] Loss: 1.5967, Batch Acc: 0.3750\n",
      "[Batch 50/501] Loss: 1.7952, Batch Acc: 0.1250\n",
      "[Batch 60/501] Loss: 1.4599, Batch Acc: 0.2500\n",
      "[Batch 70/501] Loss: 1.6587, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.8373, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.9326, Batch Acc: 0.0000\n",
      "[Batch 100/501] Loss: 1.8912, Batch Acc: 0.0000\n",
      "[Batch 110/501] Loss: 1.4887, Batch Acc: 0.6250\n",
      "[Batch 120/501] Loss: 1.7056, Batch Acc: 0.2500\n",
      "[Batch 130/501] Loss: 1.5711, Batch Acc: 0.2500\n",
      "[Batch 140/501] Loss: 1.6352, Batch Acc: 0.5000\n",
      "[Batch 150/501] Loss: 1.3113, Batch Acc: 0.5000\n",
      "[Batch 160/501] Loss: 1.6535, Batch Acc: 0.1250\n",
      "[Batch 170/501] Loss: 1.6677, Batch Acc: 0.2500\n",
      "[Batch 180/501] Loss: 1.5933, Batch Acc: 0.1250\n",
      "[Batch 190/501] Loss: 1.9209, Batch Acc: 0.0000\n",
      "[Batch 200/501] Loss: 1.4571, Batch Acc: 0.3750\n",
      "[Batch 210/501] Loss: 1.8416, Batch Acc: 0.2500\n",
      "[Batch 220/501] Loss: 2.0789, Batch Acc: 0.0000\n",
      "[Batch 230/501] Loss: 1.6446, Batch Acc: 0.3750\n",
      "[Batch 240/501] Loss: 1.5236, Batch Acc: 0.5000\n",
      "[Batch 250/501] Loss: 1.4212, Batch Acc: 0.3750\n",
      "[Batch 260/501] Loss: 1.8217, Batch Acc: 0.2500\n",
      "[Batch 270/501] Loss: 1.5989, Batch Acc: 0.2500\n",
      "[Batch 280/501] Loss: 1.6658, Batch Acc: 0.2500\n",
      "[Batch 290/501] Loss: 1.4817, Batch Acc: 0.2500\n",
      "[Batch 300/501] Loss: 1.5574, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 1.5966, Batch Acc: 0.2500\n",
      "[Batch 320/501] Loss: 1.9805, Batch Acc: 0.1250\n",
      "[Batch 330/501] Loss: 1.8728, Batch Acc: 0.0000\n",
      "[Batch 340/501] Loss: 1.3734, Batch Acc: 0.3750\n",
      "[Batch 350/501] Loss: 1.6820, Batch Acc: 0.3750\n",
      "[Batch 360/501] Loss: 1.3664, Batch Acc: 0.3750\n",
      "[Batch 370/501] Loss: 1.7095, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.5949, Batch Acc: 0.2500\n",
      "[Batch 390/501] Loss: 1.7572, Batch Acc: 0.0000\n",
      "[Batch 400/501] Loss: 1.7494, Batch Acc: 0.1250\n",
      "[Batch 410/501] Loss: 1.4108, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.6398, Batch Acc: 0.2500\n",
      "[Batch 430/501] Loss: 1.6551, Batch Acc: 0.1250\n",
      "[Batch 440/501] Loss: 1.2861, Batch Acc: 0.7500\n",
      "[Batch 450/501] Loss: 1.7756, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.7971, Batch Acc: 0.1250\n",
      "[Batch 470/501] Loss: 1.3911, Batch Acc: 0.5000\n",
      "[Batch 480/501] Loss: 1.4673, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.5251, Batch Acc: 0.5000\n",
      "[Batch 500/501] Loss: 1.7388, Batch Acc: 0.1250\n",
      "Epoch 10 Summary - Loss: 804.3426, Train Accuracy: 0.2675\n",
      "Test Accuracy: 0.2779\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_accuracy</td><td>▄▅█▄▁▁▄▁▃▄</td></tr><tr><td>train_loss</td><td>█▄▃▃▇█▂▁▂█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>test_accuracy</td><td>0.27794</td></tr><tr><td>test_loss</td><td>197.84244</td></tr><tr><td>train_accuracy</td><td>0.26747</td></tr><tr><td>train_loss</td><td>804.34264</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">visionary-sweep-9</strong> at: <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/l4r42d7q' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/l4r42d7q</a><br> View project at: <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250615_141833-l4r42d7q/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: nwezjnzp with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_nodes: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ajit.kumar4@happiestminds.com/Documents/Drackula/ComputerVisionFromScratch/wandb/run-20250615_142146-nwezjnzp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/nwezjnzp' target=\"_blank\">smooth-sweep-10</a></strong> to <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/nwezjnzp' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/nwezjnzp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 10/501] Loss: 1.6817, Batch Acc: 0.2500\n",
      "[Batch 20/501] Loss: 2.1558, Batch Acc: 0.0000\n",
      "[Batch 30/501] Loss: 1.7368, Batch Acc: 0.0000\n",
      "[Batch 40/501] Loss: 1.6601, Batch Acc: 0.1250\n",
      "[Batch 50/501] Loss: 1.7515, Batch Acc: 0.0000\n",
      "[Batch 60/501] Loss: 1.8095, Batch Acc: 0.1250\n",
      "[Batch 70/501] Loss: 1.6838, Batch Acc: 0.1250\n",
      "[Batch 80/501] Loss: 1.7178, Batch Acc: 0.1250\n",
      "[Batch 90/501] Loss: 1.6654, Batch Acc: 0.1250\n",
      "[Batch 100/501] Loss: 1.6685, Batch Acc: 0.3750\n",
      "[Batch 110/501] Loss: 1.5130, Batch Acc: 0.3750\n",
      "[Batch 120/501] Loss: 2.0162, Batch Acc: 0.0000\n",
      "[Batch 130/501] Loss: 1.8595, Batch Acc: 0.2500\n",
      "[Batch 140/501] Loss: 1.8983, Batch Acc: 0.1250\n",
      "[Batch 150/501] Loss: 1.9918, Batch Acc: 0.1250\n",
      "[Batch 160/501] Loss: 1.6936, Batch Acc: 0.3750\n",
      "[Batch 170/501] Loss: 1.8717, Batch Acc: 0.0000\n",
      "[Batch 180/501] Loss: 1.8650, Batch Acc: 0.2500\n",
      "[Batch 190/501] Loss: 1.7229, Batch Acc: 0.2500\n",
      "[Batch 200/501] Loss: 1.7194, Batch Acc: 0.1250\n",
      "[Batch 210/501] Loss: 1.8472, Batch Acc: 0.0000\n",
      "[Batch 220/501] Loss: 1.7306, Batch Acc: 0.0000\n",
      "[Batch 230/501] Loss: 1.7893, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.9915, Batch Acc: 0.0000\n",
      "[Batch 250/501] Loss: 1.7888, Batch Acc: 0.0000\n",
      "[Batch 260/501] Loss: 1.5928, Batch Acc: 0.3750\n",
      "[Batch 270/501] Loss: 1.6878, Batch Acc: 0.1250\n",
      "[Batch 280/501] Loss: 1.6828, Batch Acc: 0.2500\n",
      "[Batch 290/501] Loss: 1.8758, Batch Acc: 0.0000\n",
      "[Batch 300/501] Loss: 1.6586, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 1.5529, Batch Acc: 0.2500\n",
      "[Batch 320/501] Loss: 1.6536, Batch Acc: 0.3750\n",
      "[Batch 330/501] Loss: 2.0284, Batch Acc: 0.0000\n",
      "[Batch 340/501] Loss: 1.6894, Batch Acc: 0.3750\n",
      "[Batch 350/501] Loss: 1.6134, Batch Acc: 0.1250\n",
      "[Batch 360/501] Loss: 1.8135, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 1.6510, Batch Acc: 0.3750\n",
      "[Batch 380/501] Loss: 1.7991, Batch Acc: 0.1250\n",
      "[Batch 390/501] Loss: 1.7660, Batch Acc: 0.1250\n",
      "[Batch 400/501] Loss: 1.3970, Batch Acc: 0.3750\n",
      "[Batch 410/501] Loss: 1.6841, Batch Acc: 0.1250\n",
      "[Batch 420/501] Loss: 1.7700, Batch Acc: 0.1250\n",
      "[Batch 430/501] Loss: 1.6296, Batch Acc: 0.2500\n",
      "[Batch 440/501] Loss: 1.3347, Batch Acc: 0.3750\n",
      "[Batch 450/501] Loss: 1.7062, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.5950, Batch Acc: 0.1250\n",
      "[Batch 470/501] Loss: 1.8532, Batch Acc: 0.1250\n",
      "[Batch 480/501] Loss: 1.8276, Batch Acc: 0.0000\n",
      "[Batch 490/501] Loss: 1.8242, Batch Acc: 0.2500\n",
      "[Batch 500/501] Loss: 1.7871, Batch Acc: 0.1250\n",
      "Epoch 1 Summary - Loss: 856.8765, Train Accuracy: 0.1949\n",
      "[Batch 10/501] Loss: 1.5287, Batch Acc: 0.3750\n",
      "[Batch 20/501] Loss: 1.7718, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.9726, Batch Acc: 0.0000\n",
      "[Batch 40/501] Loss: 1.8049, Batch Acc: 0.1250\n",
      "[Batch 50/501] Loss: 1.4850, Batch Acc: 0.3750\n",
      "[Batch 60/501] Loss: 1.5634, Batch Acc: 0.1250\n",
      "[Batch 70/501] Loss: 1.7087, Batch Acc: 0.1250\n",
      "[Batch 80/501] Loss: 1.9037, Batch Acc: 0.1250\n",
      "[Batch 90/501] Loss: 1.6611, Batch Acc: 0.2500\n",
      "[Batch 100/501] Loss: 1.6081, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 1.7578, Batch Acc: 0.1250\n",
      "[Batch 120/501] Loss: 1.7287, Batch Acc: 0.1250\n",
      "[Batch 130/501] Loss: 1.9225, Batch Acc: 0.0000\n",
      "[Batch 140/501] Loss: 1.7834, Batch Acc: 0.2500\n",
      "[Batch 150/501] Loss: 2.0387, Batch Acc: 0.0000\n",
      "[Batch 160/501] Loss: 1.8446, Batch Acc: 0.0000\n",
      "[Batch 170/501] Loss: 1.9131, Batch Acc: 0.0000\n",
      "[Batch 180/501] Loss: 2.0875, Batch Acc: 0.1250\n",
      "[Batch 190/501] Loss: 1.7384, Batch Acc: 0.0000\n",
      "[Batch 200/501] Loss: 1.6601, Batch Acc: 0.1250\n",
      "[Batch 210/501] Loss: 1.7575, Batch Acc: 0.1250\n",
      "[Batch 220/501] Loss: 1.7708, Batch Acc: 0.2500\n",
      "[Batch 230/501] Loss: 1.7084, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.6488, Batch Acc: 0.1250\n",
      "[Batch 250/501] Loss: 1.3557, Batch Acc: 0.5000\n",
      "[Batch 260/501] Loss: 1.6744, Batch Acc: 0.2500\n",
      "[Batch 270/501] Loss: 1.5074, Batch Acc: 0.2500\n",
      "[Batch 280/501] Loss: 1.7982, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 1.3372, Batch Acc: 0.3750\n",
      "[Batch 300/501] Loss: 1.5880, Batch Acc: 0.3750\n",
      "[Batch 310/501] Loss: 1.5347, Batch Acc: 0.5000\n",
      "[Batch 320/501] Loss: 1.7466, Batch Acc: 0.1250\n",
      "[Batch 330/501] Loss: 1.7852, Batch Acc: 0.2500\n",
      "[Batch 340/501] Loss: 1.5916, Batch Acc: 0.2500\n",
      "[Batch 350/501] Loss: 1.7508, Batch Acc: 0.1250\n",
      "[Batch 360/501] Loss: 1.7008, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 1.7952, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.5305, Batch Acc: 0.3750\n",
      "[Batch 390/501] Loss: 1.7080, Batch Acc: 0.0000\n",
      "[Batch 400/501] Loss: 1.4532, Batch Acc: 0.5000\n",
      "[Batch 410/501] Loss: 1.6823, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.6820, Batch Acc: 0.1250\n",
      "[Batch 430/501] Loss: 1.4789, Batch Acc: 0.3750\n",
      "[Batch 440/501] Loss: 1.8762, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.9421, Batch Acc: 0.0000\n",
      "[Batch 460/501] Loss: 1.8741, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.5767, Batch Acc: 0.2500\n",
      "[Batch 480/501] Loss: 1.5416, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.8487, Batch Acc: 0.1250\n",
      "[Batch 500/501] Loss: 1.8664, Batch Acc: 0.0000\n",
      "Epoch 2 Summary - Loss: 856.9538, Train Accuracy: 0.1936\n",
      "[Batch 10/501] Loss: 1.8417, Batch Acc: 0.1250\n",
      "[Batch 20/501] Loss: 1.7990, Batch Acc: 0.1250\n",
      "[Batch 30/501] Loss: 1.7424, Batch Acc: 0.1250\n",
      "[Batch 40/501] Loss: 2.0493, Batch Acc: 0.1250\n",
      "[Batch 50/501] Loss: 1.9110, Batch Acc: 0.1250\n",
      "[Batch 60/501] Loss: 1.9351, Batch Acc: 0.1250\n",
      "[Batch 70/501] Loss: 1.5176, Batch Acc: 0.1250\n",
      "[Batch 80/501] Loss: 1.6819, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.8721, Batch Acc: 0.3750\n",
      "[Batch 100/501] Loss: 1.5590, Batch Acc: 0.1250\n",
      "[Batch 110/501] Loss: 1.5890, Batch Acc: 0.1250\n",
      "[Batch 120/501] Loss: 1.6789, Batch Acc: 0.1250\n",
      "[Batch 130/501] Loss: 1.5983, Batch Acc: 0.2500\n",
      "[Batch 140/501] Loss: 1.6368, Batch Acc: 0.1250\n",
      "[Batch 150/501] Loss: 1.6808, Batch Acc: 0.3750\n",
      "[Batch 160/501] Loss: 1.6730, Batch Acc: 0.1250\n",
      "[Batch 170/501] Loss: 1.9450, Batch Acc: 0.0000\n",
      "[Batch 180/501] Loss: 1.8557, Batch Acc: 0.0000\n",
      "[Batch 190/501] Loss: 1.7817, Batch Acc: 0.2500\n",
      "[Batch 200/501] Loss: 1.8308, Batch Acc: 0.2500\n",
      "[Batch 210/501] Loss: 1.8276, Batch Acc: 0.0000\n",
      "[Batch 220/501] Loss: 2.1432, Batch Acc: 0.0000\n",
      "[Batch 230/501] Loss: 1.7044, Batch Acc: 0.3750\n",
      "[Batch 240/501] Loss: 1.6375, Batch Acc: 0.2500\n",
      "[Batch 250/501] Loss: 1.7444, Batch Acc: 0.0000\n",
      "[Batch 260/501] Loss: 1.5934, Batch Acc: 0.1250\n",
      "[Batch 270/501] Loss: 1.6842, Batch Acc: 0.2500\n",
      "[Batch 280/501] Loss: 1.4658, Batch Acc: 0.5000\n",
      "[Batch 290/501] Loss: 1.7752, Batch Acc: 0.1250\n",
      "[Batch 300/501] Loss: 1.7891, Batch Acc: 0.1250\n",
      "[Batch 310/501] Loss: 1.5926, Batch Acc: 0.2500\n",
      "[Batch 320/501] Loss: 1.9968, Batch Acc: 0.1250\n",
      "[Batch 330/501] Loss: 1.5674, Batch Acc: 0.1250\n",
      "[Batch 340/501] Loss: 1.5759, Batch Acc: 0.2500\n",
      "[Batch 350/501] Loss: 1.9190, Batch Acc: 0.2500\n",
      "[Batch 360/501] Loss: 1.6138, Batch Acc: 0.3750\n",
      "[Batch 370/501] Loss: 1.6859, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.7141, Batch Acc: 0.0000\n",
      "[Batch 390/501] Loss: 1.4543, Batch Acc: 0.3750\n",
      "[Batch 400/501] Loss: 1.3106, Batch Acc: 0.5000\n",
      "[Batch 410/501] Loss: 1.4830, Batch Acc: 0.5000\n",
      "[Batch 420/501] Loss: 1.3609, Batch Acc: 0.3750\n",
      "[Batch 430/501] Loss: 1.9125, Batch Acc: 0.0000\n",
      "[Batch 440/501] Loss: 1.6963, Batch Acc: 0.1250\n",
      "[Batch 450/501] Loss: 1.7045, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 2.1493, Batch Acc: 0.0000\n",
      "[Batch 470/501] Loss: 2.0279, Batch Acc: 0.0000\n",
      "[Batch 480/501] Loss: 1.5146, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.8174, Batch Acc: 0.3750\n",
      "[Batch 500/501] Loss: 1.7883, Batch Acc: 0.1250\n",
      "Epoch 3 Summary - Loss: 860.2165, Train Accuracy: 0.1919\n",
      "[Batch 10/501] Loss: 1.6160, Batch Acc: 0.3750\n",
      "[Batch 20/501] Loss: 1.4957, Batch Acc: 0.3750\n",
      "[Batch 30/501] Loss: 1.6739, Batch Acc: 0.1250\n",
      "[Batch 40/501] Loss: 1.6229, Batch Acc: 0.2500\n",
      "[Batch 50/501] Loss: 1.6796, Batch Acc: 0.1250\n",
      "[Batch 60/501] Loss: 1.6005, Batch Acc: 0.1250\n",
      "[Batch 70/501] Loss: 1.7096, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.6941, Batch Acc: 0.1250\n",
      "[Batch 90/501] Loss: 1.5957, Batch Acc: 0.3750\n",
      "[Batch 100/501] Loss: 1.4949, Batch Acc: 0.3750\n",
      "[Batch 110/501] Loss: 1.4237, Batch Acc: 0.5000\n",
      "[Batch 120/501] Loss: 1.8774, Batch Acc: 0.0000\n",
      "[Batch 130/501] Loss: 2.0004, Batch Acc: 0.0000\n",
      "[Batch 140/501] Loss: 1.3020, Batch Acc: 0.5000\n",
      "[Batch 150/501] Loss: 1.6666, Batch Acc: 0.0000\n",
      "[Batch 160/501] Loss: 1.5741, Batch Acc: 0.2500\n",
      "[Batch 170/501] Loss: 1.6500, Batch Acc: 0.1250\n",
      "[Batch 180/501] Loss: 1.8139, Batch Acc: 0.1250\n",
      "[Batch 190/501] Loss: 1.6735, Batch Acc: 0.0000\n",
      "[Batch 200/501] Loss: 1.9558, Batch Acc: 0.0000\n",
      "[Batch 210/501] Loss: 1.8242, Batch Acc: 0.2500\n",
      "[Batch 220/501] Loss: 1.9733, Batch Acc: 0.1250\n",
      "[Batch 230/501] Loss: 1.8335, Batch Acc: 0.3750\n",
      "[Batch 240/501] Loss: 1.5536, Batch Acc: 0.2500\n",
      "[Batch 250/501] Loss: 1.6035, Batch Acc: 0.2500\n",
      "[Batch 260/501] Loss: 1.6843, Batch Acc: 0.2500\n",
      "[Batch 270/501] Loss: 1.8393, Batch Acc: 0.0000\n",
      "[Batch 280/501] Loss: 1.6018, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 1.8252, Batch Acc: 0.1250\n",
      "[Batch 300/501] Loss: 1.9318, Batch Acc: 0.1250\n",
      "[Batch 310/501] Loss: 1.6561, Batch Acc: 0.2500\n",
      "[Batch 320/501] Loss: 1.8351, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 1.6854, Batch Acc: 0.0000\n",
      "[Batch 340/501] Loss: 1.4260, Batch Acc: 0.6250\n",
      "[Batch 350/501] Loss: 1.8784, Batch Acc: 0.1250\n",
      "[Batch 360/501] Loss: 1.5945, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 1.7104, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.6733, Batch Acc: 0.3750\n",
      "[Batch 390/501] Loss: 1.6499, Batch Acc: 0.2500\n",
      "[Batch 400/501] Loss: 1.4586, Batch Acc: 0.3750\n",
      "[Batch 410/501] Loss: 1.9120, Batch Acc: 0.0000\n",
      "[Batch 420/501] Loss: 1.9167, Batch Acc: 0.1250\n",
      "[Batch 430/501] Loss: 1.6172, Batch Acc: 0.2500\n",
      "[Batch 440/501] Loss: 1.8323, Batch Acc: 0.1250\n",
      "[Batch 450/501] Loss: 1.7175, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.6581, Batch Acc: 0.1250\n",
      "[Batch 470/501] Loss: 1.5778, Batch Acc: 0.2500\n",
      "[Batch 480/501] Loss: 2.0467, Batch Acc: 0.1250\n",
      "[Batch 490/501] Loss: 1.6301, Batch Acc: 0.1250\n",
      "[Batch 500/501] Loss: 1.6939, Batch Acc: 0.2500\n",
      "Epoch 4 Summary - Loss: 856.1398, Train Accuracy: 0.1969\n",
      "[Batch 10/501] Loss: 1.6283, Batch Acc: 0.2500\n",
      "[Batch 20/501] Loss: 1.9506, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 2.0337, Batch Acc: 0.0000\n",
      "[Batch 40/501] Loss: 1.6016, Batch Acc: 0.2500\n",
      "[Batch 50/501] Loss: 1.8656, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.5052, Batch Acc: 0.2500\n",
      "[Batch 70/501] Loss: 1.6272, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.4770, Batch Acc: 0.6250\n",
      "[Batch 90/501] Loss: 1.7413, Batch Acc: 0.1250\n",
      "[Batch 100/501] Loss: 1.7434, Batch Acc: 0.3750\n",
      "[Batch 110/501] Loss: 1.6589, Batch Acc: 0.1250\n",
      "[Batch 120/501] Loss: 1.4954, Batch Acc: 0.5000\n",
      "[Batch 130/501] Loss: 1.6292, Batch Acc: 0.1250\n",
      "[Batch 140/501] Loss: 2.0085, Batch Acc: 0.0000\n",
      "[Batch 150/501] Loss: 1.4039, Batch Acc: 0.3750\n",
      "[Batch 160/501] Loss: 1.8841, Batch Acc: 0.0000\n",
      "[Batch 170/501] Loss: 1.9610, Batch Acc: 0.1250\n",
      "[Batch 180/501] Loss: 1.5870, Batch Acc: 0.1250\n",
      "[Batch 190/501] Loss: 1.8555, Batch Acc: 0.0000\n",
      "[Batch 200/501] Loss: 1.6970, Batch Acc: 0.3750\n",
      "[Batch 210/501] Loss: 1.7370, Batch Acc: 0.3750\n",
      "[Batch 220/501] Loss: 1.7751, Batch Acc: 0.1250\n",
      "[Batch 230/501] Loss: 1.7614, Batch Acc: 0.1250\n",
      "[Batch 240/501] Loss: 1.7274, Batch Acc: 0.2500\n",
      "[Batch 250/501] Loss: 1.7082, Batch Acc: 0.1250\n",
      "[Batch 260/501] Loss: 1.8499, Batch Acc: 0.0000\n",
      "[Batch 270/501] Loss: 1.7319, Batch Acc: 0.1250\n",
      "[Batch 280/501] Loss: 1.5422, Batch Acc: 0.2500\n",
      "[Batch 290/501] Loss: 1.4547, Batch Acc: 0.5000\n",
      "[Batch 300/501] Loss: 1.8385, Batch Acc: 0.3750\n",
      "[Batch 310/501] Loss: 1.8383, Batch Acc: 0.0000\n",
      "[Batch 320/501] Loss: 1.8165, Batch Acc: 0.0000\n",
      "[Batch 330/501] Loss: 1.6877, Batch Acc: 0.2500\n",
      "[Batch 340/501] Loss: 1.6781, Batch Acc: 0.1250\n",
      "[Batch 350/501] Loss: 1.9824, Batch Acc: 0.0000\n",
      "[Batch 360/501] Loss: 1.5883, Batch Acc: 0.3750\n",
      "[Batch 370/501] Loss: 1.5950, Batch Acc: 0.3750\n",
      "[Batch 380/501] Loss: 1.4827, Batch Acc: 0.3750\n",
      "[Batch 390/501] Loss: 1.6189, Batch Acc: 0.2500\n",
      "[Batch 400/501] Loss: 1.8997, Batch Acc: 0.1250\n",
      "[Batch 410/501] Loss: 1.6959, Batch Acc: 0.1250\n",
      "[Batch 420/501] Loss: 1.9500, Batch Acc: 0.1250\n",
      "[Batch 430/501] Loss: 1.6666, Batch Acc: 0.1250\n",
      "[Batch 440/501] Loss: 1.8620, Batch Acc: 0.1250\n",
      "[Batch 450/501] Loss: 1.7226, Batch Acc: 0.0000\n",
      "[Batch 460/501] Loss: 1.6417, Batch Acc: 0.1250\n",
      "[Batch 470/501] Loss: 1.6706, Batch Acc: 0.1250\n",
      "[Batch 480/501] Loss: 1.4025, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.9238, Batch Acc: 0.0000\n",
      "[Batch 500/501] Loss: 1.5183, Batch Acc: 0.2500\n",
      "Epoch 5 Summary - Loss: 855.5922, Train Accuracy: 0.2048\n",
      "[Batch 10/501] Loss: 1.7566, Batch Acc: 0.2500\n",
      "[Batch 20/501] Loss: 1.7257, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 2.1033, Batch Acc: 0.0000\n",
      "[Batch 40/501] Loss: 1.4478, Batch Acc: 0.3750\n",
      "[Batch 50/501] Loss: 1.6663, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.7948, Batch Acc: 0.1250\n",
      "[Batch 70/501] Loss: 1.7203, Batch Acc: 0.1250\n",
      "[Batch 80/501] Loss: 1.9909, Batch Acc: 0.1250\n",
      "[Batch 90/501] Loss: 1.6994, Batch Acc: 0.1250\n",
      "[Batch 100/501] Loss: 1.5917, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 1.6505, Batch Acc: 0.3750\n",
      "[Batch 120/501] Loss: 1.5538, Batch Acc: 0.3750\n",
      "[Batch 130/501] Loss: 1.5691, Batch Acc: 0.1250\n",
      "[Batch 140/501] Loss: 1.6338, Batch Acc: 0.2500\n",
      "[Batch 150/501] Loss: 1.7427, Batch Acc: 0.1250\n",
      "[Batch 160/501] Loss: 1.7686, Batch Acc: 0.1250\n",
      "[Batch 170/501] Loss: 1.5133, Batch Acc: 0.3750\n",
      "[Batch 180/501] Loss: 1.7731, Batch Acc: 0.1250\n",
      "[Batch 190/501] Loss: 1.6929, Batch Acc: 0.1250\n",
      "[Batch 200/501] Loss: 1.6792, Batch Acc: 0.1250\n",
      "[Batch 210/501] Loss: 1.6145, Batch Acc: 0.2500\n",
      "[Batch 220/501] Loss: 1.9481, Batch Acc: 0.0000\n",
      "[Batch 230/501] Loss: 1.6698, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.8420, Batch Acc: 0.0000\n",
      "[Batch 250/501] Loss: 1.6261, Batch Acc: 0.3750\n",
      "[Batch 260/501] Loss: 1.9689, Batch Acc: 0.0000\n",
      "[Batch 270/501] Loss: 1.8252, Batch Acc: 0.2500\n",
      "[Batch 280/501] Loss: 1.7391, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 1.8451, Batch Acc: 0.2500\n",
      "[Batch 300/501] Loss: 1.6286, Batch Acc: 0.1250\n",
      "[Batch 310/501] Loss: 1.8795, Batch Acc: 0.1250\n",
      "[Batch 320/501] Loss: 1.5269, Batch Acc: 0.1250\n",
      "[Batch 330/501] Loss: 1.4855, Batch Acc: 0.5000\n",
      "[Batch 340/501] Loss: 1.7423, Batch Acc: 0.1250\n",
      "[Batch 350/501] Loss: 1.4909, Batch Acc: 0.2500\n",
      "[Batch 360/501] Loss: 1.8044, Batch Acc: 0.3750\n",
      "[Batch 370/501] Loss: 1.5977, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 2.0034, Batch Acc: 0.0000\n",
      "[Batch 390/501] Loss: 1.5076, Batch Acc: 0.2500\n",
      "[Batch 400/501] Loss: 1.6539, Batch Acc: 0.2500\n",
      "[Batch 410/501] Loss: 1.6538, Batch Acc: 0.3750\n",
      "[Batch 420/501] Loss: 1.4243, Batch Acc: 0.2500\n",
      "[Batch 430/501] Loss: 1.9709, Batch Acc: 0.0000\n",
      "[Batch 440/501] Loss: 1.8049, Batch Acc: 0.0000\n",
      "[Batch 450/501] Loss: 1.4323, Batch Acc: 0.3750\n",
      "[Batch 460/501] Loss: 1.6521, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.6245, Batch Acc: 0.3750\n",
      "[Batch 480/501] Loss: 1.7654, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.8901, Batch Acc: 0.1250\n",
      "[Batch 500/501] Loss: 1.4784, Batch Acc: 0.2500\n",
      "Epoch 6 Summary - Loss: 858.5204, Train Accuracy: 0.1941\n",
      "[Batch 10/501] Loss: 1.5635, Batch Acc: 0.1250\n",
      "[Batch 20/501] Loss: 1.8338, Batch Acc: 0.1250\n",
      "[Batch 30/501] Loss: 1.8065, Batch Acc: 0.1250\n",
      "[Batch 40/501] Loss: 1.6085, Batch Acc: 0.1250\n",
      "[Batch 50/501] Loss: 1.8051, Batch Acc: 0.1250\n",
      "[Batch 60/501] Loss: 2.0110, Batch Acc: 0.1250\n",
      "[Batch 70/501] Loss: 1.7733, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 2.0045, Batch Acc: 0.1250\n",
      "[Batch 90/501] Loss: 1.6054, Batch Acc: 0.3750\n",
      "[Batch 100/501] Loss: 1.7637, Batch Acc: 0.3750\n",
      "[Batch 110/501] Loss: 1.8168, Batch Acc: 0.2500\n",
      "[Batch 120/501] Loss: 1.6178, Batch Acc: 0.1250\n",
      "[Batch 130/501] Loss: 1.8039, Batch Acc: 0.1250\n",
      "[Batch 140/501] Loss: 1.7822, Batch Acc: 0.1250\n",
      "[Batch 150/501] Loss: 1.8306, Batch Acc: 0.1250\n",
      "[Batch 160/501] Loss: 1.7242, Batch Acc: 0.2500\n",
      "[Batch 170/501] Loss: 1.7911, Batch Acc: 0.0000\n",
      "[Batch 180/501] Loss: 1.8078, Batch Acc: 0.1250\n",
      "[Batch 190/501] Loss: 1.8382, Batch Acc: 0.2500\n",
      "[Batch 200/501] Loss: 2.0167, Batch Acc: 0.1250\n",
      "[Batch 210/501] Loss: 2.0046, Batch Acc: 0.1250\n",
      "[Batch 220/501] Loss: 1.7667, Batch Acc: 0.0000\n",
      "[Batch 230/501] Loss: 1.6968, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.8763, Batch Acc: 0.2500\n",
      "[Batch 250/501] Loss: 1.8110, Batch Acc: 0.1250\n",
      "[Batch 260/501] Loss: 1.7011, Batch Acc: 0.3750\n",
      "[Batch 270/501] Loss: 1.5426, Batch Acc: 0.2500\n",
      "[Batch 280/501] Loss: 1.7317, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 1.6669, Batch Acc: 0.2500\n",
      "[Batch 300/501] Loss: 1.6709, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 1.8481, Batch Acc: 0.5000\n",
      "[Batch 320/501] Loss: 1.5408, Batch Acc: 0.5000\n",
      "[Batch 330/501] Loss: 1.6325, Batch Acc: 0.1250\n",
      "[Batch 340/501] Loss: 1.9481, Batch Acc: 0.1250\n",
      "[Batch 350/501] Loss: 1.6638, Batch Acc: 0.1250\n",
      "[Batch 360/501] Loss: 1.5752, Batch Acc: 0.1250\n",
      "[Batch 370/501] Loss: 1.5387, Batch Acc: 0.2500\n",
      "[Batch 380/501] Loss: 1.5803, Batch Acc: 0.2500\n",
      "[Batch 390/501] Loss: 1.8632, Batch Acc: 0.3750\n",
      "[Batch 400/501] Loss: 1.8068, Batch Acc: 0.1250\n",
      "[Batch 410/501] Loss: 1.8791, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.5706, Batch Acc: 0.1250\n",
      "[Batch 430/501] Loss: 1.6221, Batch Acc: 0.1250\n",
      "[Batch 440/501] Loss: 1.5768, Batch Acc: 0.5000\n",
      "[Batch 450/501] Loss: 1.7671, Batch Acc: 0.1250\n",
      "[Batch 460/501] Loss: 1.6928, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.6684, Batch Acc: 0.2500\n",
      "[Batch 480/501] Loss: 1.5087, Batch Acc: 0.3750\n",
      "[Batch 490/501] Loss: 1.5587, Batch Acc: 0.2500\n",
      "[Batch 500/501] Loss: 1.8477, Batch Acc: 0.1250\n",
      "Epoch 7 Summary - Loss: 853.9939, Train Accuracy: 0.1966\n",
      "[Batch 10/501] Loss: 1.9466, Batch Acc: 0.1250\n",
      "[Batch 20/501] Loss: 1.6266, Batch Acc: 0.1250\n",
      "[Batch 30/501] Loss: 1.8618, Batch Acc: 0.0000\n",
      "[Batch 40/501] Loss: 1.7660, Batch Acc: 0.0000\n",
      "[Batch 50/501] Loss: 1.9072, Batch Acc: 0.1250\n",
      "[Batch 60/501] Loss: 1.6990, Batch Acc: 0.1250\n",
      "[Batch 70/501] Loss: 1.6977, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.5612, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.9266, Batch Acc: 0.1250\n",
      "[Batch 100/501] Loss: 1.3062, Batch Acc: 0.5000\n",
      "[Batch 110/501] Loss: 1.6292, Batch Acc: 0.2500\n",
      "[Batch 120/501] Loss: 1.5345, Batch Acc: 0.5000\n",
      "[Batch 130/501] Loss: 1.4835, Batch Acc: 0.2500\n",
      "[Batch 140/501] Loss: 1.8848, Batch Acc: 0.0000\n",
      "[Batch 150/501] Loss: 1.7560, Batch Acc: 0.2500\n",
      "[Batch 160/501] Loss: 1.6538, Batch Acc: 0.1250\n",
      "[Batch 170/501] Loss: 1.5501, Batch Acc: 0.5000\n",
      "[Batch 180/501] Loss: 1.8171, Batch Acc: 0.0000\n",
      "[Batch 190/501] Loss: 1.7664, Batch Acc: 0.2500\n",
      "[Batch 200/501] Loss: 1.5047, Batch Acc: 0.3750\n",
      "[Batch 210/501] Loss: 1.7723, Batch Acc: 0.0000\n",
      "[Batch 220/501] Loss: 1.8942, Batch Acc: 0.0000\n",
      "[Batch 230/501] Loss: 1.9940, Batch Acc: 0.1250\n",
      "[Batch 240/501] Loss: 1.9082, Batch Acc: 0.1250\n",
      "[Batch 250/501] Loss: 1.5506, Batch Acc: 0.2500\n",
      "[Batch 260/501] Loss: 1.7792, Batch Acc: 0.2500\n",
      "[Batch 270/501] Loss: 1.8606, Batch Acc: 0.1250\n",
      "[Batch 280/501] Loss: 1.8398, Batch Acc: 0.0000\n",
      "[Batch 290/501] Loss: 1.6154, Batch Acc: 0.3750\n",
      "[Batch 300/501] Loss: 1.7837, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 1.6522, Batch Acc: 0.2500\n",
      "[Batch 320/501] Loss: 1.7490, Batch Acc: 0.1250\n",
      "[Batch 330/501] Loss: 1.7708, Batch Acc: 0.1250\n",
      "[Batch 340/501] Loss: 1.7405, Batch Acc: 0.0000\n",
      "[Batch 350/501] Loss: 1.6066, Batch Acc: 0.1250\n",
      "[Batch 360/501] Loss: 1.7439, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 1.9990, Batch Acc: 0.0000\n",
      "[Batch 380/501] Loss: 1.7721, Batch Acc: 0.0000\n",
      "[Batch 390/501] Loss: 1.8309, Batch Acc: 0.1250\n",
      "[Batch 400/501] Loss: 1.6687, Batch Acc: 0.2500\n",
      "[Batch 410/501] Loss: 1.6604, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.6519, Batch Acc: 0.1250\n",
      "[Batch 430/501] Loss: 1.6204, Batch Acc: 0.3750\n",
      "[Batch 440/501] Loss: 1.9273, Batch Acc: 0.0000\n",
      "[Batch 450/501] Loss: 1.8617, Batch Acc: 0.0000\n",
      "[Batch 460/501] Loss: 1.8846, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.5673, Batch Acc: 0.2500\n",
      "[Batch 480/501] Loss: 1.7865, Batch Acc: 0.1250\n",
      "[Batch 490/501] Loss: 1.5084, Batch Acc: 0.2500\n",
      "[Batch 500/501] Loss: 1.7725, Batch Acc: 0.0000\n",
      "Epoch 8 Summary - Loss: 859.2818, Train Accuracy: 0.1954\n",
      "[Batch 10/501] Loss: 1.6161, Batch Acc: 0.1250\n",
      "[Batch 20/501] Loss: 1.7218, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.8937, Batch Acc: 0.1250\n",
      "[Batch 40/501] Loss: 1.3430, Batch Acc: 0.5000\n",
      "[Batch 50/501] Loss: 1.7102, Batch Acc: 0.0000\n",
      "[Batch 60/501] Loss: 1.7355, Batch Acc: 0.2500\n",
      "[Batch 70/501] Loss: 1.6965, Batch Acc: 0.1250\n",
      "[Batch 80/501] Loss: 1.5901, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.5118, Batch Acc: 0.2500\n",
      "[Batch 100/501] Loss: 1.8215, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 1.6526, Batch Acc: 0.1250\n",
      "[Batch 120/501] Loss: 1.9680, Batch Acc: 0.0000\n",
      "[Batch 130/501] Loss: 1.5288, Batch Acc: 0.3750\n",
      "[Batch 140/501] Loss: 1.6317, Batch Acc: 0.1250\n",
      "[Batch 150/501] Loss: 1.5411, Batch Acc: 0.5000\n",
      "[Batch 160/501] Loss: 1.5164, Batch Acc: 0.3750\n",
      "[Batch 170/501] Loss: 1.5017, Batch Acc: 0.2500\n",
      "[Batch 180/501] Loss: 1.4772, Batch Acc: 0.3750\n",
      "[Batch 190/501] Loss: 1.8004, Batch Acc: 0.0000\n",
      "[Batch 200/501] Loss: 1.7895, Batch Acc: 0.1250\n",
      "[Batch 210/501] Loss: 1.6471, Batch Acc: 0.1250\n",
      "[Batch 220/501] Loss: 1.6803, Batch Acc: 0.1250\n",
      "[Batch 230/501] Loss: 1.7927, Batch Acc: 0.0000\n",
      "[Batch 240/501] Loss: 1.6048, Batch Acc: 0.5000\n",
      "[Batch 250/501] Loss: 1.6506, Batch Acc: 0.2500\n",
      "[Batch 260/501] Loss: 1.7754, Batch Acc: 0.1250\n",
      "[Batch 270/501] Loss: 1.8659, Batch Acc: 0.0000\n",
      "[Batch 280/501] Loss: 1.6549, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 1.4873, Batch Acc: 0.3750\n",
      "[Batch 300/501] Loss: 1.4968, Batch Acc: 0.1250\n",
      "[Batch 310/501] Loss: 1.3000, Batch Acc: 0.3750\n",
      "[Batch 320/501] Loss: 1.6145, Batch Acc: 0.1250\n",
      "[Batch 330/501] Loss: 1.7658, Batch Acc: 0.1250\n",
      "[Batch 340/501] Loss: 1.6179, Batch Acc: 0.2500\n",
      "[Batch 350/501] Loss: 1.8182, Batch Acc: 0.0000\n",
      "[Batch 360/501] Loss: 1.5240, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 1.6506, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.7054, Batch Acc: 0.0000\n",
      "[Batch 390/501] Loss: 1.7519, Batch Acc: 0.1250\n",
      "[Batch 400/501] Loss: 1.7620, Batch Acc: 0.2500\n",
      "[Batch 410/501] Loss: 1.8257, Batch Acc: 0.1250\n",
      "[Batch 420/501] Loss: 1.7449, Batch Acc: 0.0000\n",
      "[Batch 430/501] Loss: 1.6028, Batch Acc: 0.2500\n",
      "[Batch 440/501] Loss: 1.8926, Batch Acc: 0.0000\n",
      "[Batch 450/501] Loss: 1.8928, Batch Acc: 0.1250\n",
      "[Batch 460/501] Loss: 2.0449, Batch Acc: 0.0000\n",
      "[Batch 470/501] Loss: 1.6945, Batch Acc: 0.1250\n",
      "[Batch 480/501] Loss: 1.5543, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.7249, Batch Acc: 0.2500\n",
      "[Batch 500/501] Loss: 1.8698, Batch Acc: 0.0000\n",
      "Epoch 9 Summary - Loss: 858.3796, Train Accuracy: 0.1891\n",
      "[Batch 10/501] Loss: 2.0527, Batch Acc: 0.2500\n",
      "[Batch 20/501] Loss: 1.9774, Batch Acc: 0.0000\n",
      "[Batch 30/501] Loss: 1.7515, Batch Acc: 0.2500\n",
      "[Batch 40/501] Loss: 1.8719, Batch Acc: 0.2500\n",
      "[Batch 50/501] Loss: 1.8782, Batch Acc: 0.0000\n",
      "[Batch 60/501] Loss: 1.5787, Batch Acc: 0.3750\n",
      "[Batch 70/501] Loss: 1.8784, Batch Acc: 0.1250\n",
      "[Batch 80/501] Loss: 1.7428, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.5041, Batch Acc: 0.3750\n",
      "[Batch 100/501] Loss: 1.9062, Batch Acc: 0.0000\n",
      "[Batch 110/501] Loss: 1.4106, Batch Acc: 0.5000\n",
      "[Batch 120/501] Loss: 1.6510, Batch Acc: 0.1250\n",
      "[Batch 130/501] Loss: 1.7334, Batch Acc: 0.2500\n",
      "[Batch 140/501] Loss: 1.9818, Batch Acc: 0.0000\n",
      "[Batch 150/501] Loss: 1.4717, Batch Acc: 0.2500\n",
      "[Batch 160/501] Loss: 1.8510, Batch Acc: 0.0000\n",
      "[Batch 170/501] Loss: 1.9331, Batch Acc: 0.1250\n",
      "[Batch 180/501] Loss: 1.7194, Batch Acc: 0.1250\n",
      "[Batch 190/501] Loss: 1.7838, Batch Acc: 0.1250\n",
      "[Batch 200/501] Loss: 1.9208, Batch Acc: 0.1250\n",
      "[Batch 210/501] Loss: 1.7685, Batch Acc: 0.2500\n",
      "[Batch 220/501] Loss: 1.6959, Batch Acc: 0.2500\n",
      "[Batch 230/501] Loss: 1.3707, Batch Acc: 0.6250\n",
      "[Batch 240/501] Loss: 1.6496, Batch Acc: 0.2500\n",
      "[Batch 250/501] Loss: 1.5408, Batch Acc: 0.3750\n",
      "[Batch 260/501] Loss: 1.7696, Batch Acc: 0.0000\n",
      "[Batch 270/501] Loss: 1.3612, Batch Acc: 0.2500\n",
      "[Batch 280/501] Loss: 1.6469, Batch Acc: 0.3750\n",
      "[Batch 290/501] Loss: 1.8190, Batch Acc: 0.2500\n",
      "[Batch 300/501] Loss: 1.7711, Batch Acc: 0.1250\n",
      "[Batch 310/501] Loss: 1.6066, Batch Acc: 0.2500\n",
      "[Batch 320/501] Loss: 1.8320, Batch Acc: 0.1250\n",
      "[Batch 330/501] Loss: 1.4046, Batch Acc: 0.3750\n",
      "[Batch 340/501] Loss: 1.7007, Batch Acc: 0.1250\n",
      "[Batch 350/501] Loss: 1.4802, Batch Acc: 0.0000\n",
      "[Batch 360/501] Loss: 1.8233, Batch Acc: 0.1250\n",
      "[Batch 370/501] Loss: 1.6925, Batch Acc: 0.2500\n",
      "[Batch 380/501] Loss: 1.5210, Batch Acc: 0.2500\n",
      "[Batch 390/501] Loss: 1.5992, Batch Acc: 0.3750\n",
      "[Batch 400/501] Loss: 1.5815, Batch Acc: 0.1250\n",
      "[Batch 410/501] Loss: 1.7081, Batch Acc: 0.3750\n",
      "[Batch 420/501] Loss: 1.4747, Batch Acc: 0.2500\n",
      "[Batch 430/501] Loss: 1.6031, Batch Acc: 0.1250\n",
      "[Batch 440/501] Loss: 1.7807, Batch Acc: 0.1250\n",
      "[Batch 450/501] Loss: 1.5722, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.6677, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.5437, Batch Acc: 0.2500\n",
      "[Batch 480/501] Loss: 1.7035, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.6377, Batch Acc: 0.1250\n",
      "[Batch 500/501] Loss: 1.9659, Batch Acc: 0.0000\n",
      "Epoch 10 Summary - Loss: 854.0461, Train Accuracy: 0.1941\n",
      "Test Accuracy: 0.2117\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_accuracy</td><td>▄▃▂▄█▃▄▄▁▃</td></tr><tr><td>train_loss</td><td>▄▄█▃▃▆▁▇▆▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>test_accuracy</td><td>0.21167</td></tr><tr><td>test_loss</td><td>214.90164</td></tr><tr><td>train_accuracy</td><td>0.19411</td></tr><tr><td>train_loss</td><td>854.04612</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">smooth-sweep-10</strong> at: <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/nwezjnzp' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/nwezjnzp</a><br> View project at: <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250615_142146-nwezjnzp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: kpq2y5io with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_nodes: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ajit.kumar4@happiestminds.com/Documents/Drackula/ComputerVisionFromScratch/wandb/run-20250615_142453-kpq2y5io</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/kpq2y5io' target=\"_blank\">usual-sweep-11</a></strong> to <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/kpq2y5io' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/kpq2y5io</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 10/501] Loss: 2.0650, Batch Acc: 0.0000\n",
      "[Batch 20/501] Loss: 1.8550, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.9329, Batch Acc: 0.0000\n",
      "[Batch 40/501] Loss: 1.8500, Batch Acc: 0.5000\n",
      "[Batch 50/501] Loss: 1.4750, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.5369, Batch Acc: 0.5000\n",
      "[Batch 70/501] Loss: 1.2234, Batch Acc: 0.5000\n",
      "[Batch 80/501] Loss: 1.5946, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.8326, Batch Acc: 0.2500\n",
      "[Batch 100/501] Loss: 1.9718, Batch Acc: 0.0000\n",
      "[Batch 110/501] Loss: 1.6258, Batch Acc: 0.6250\n",
      "[Batch 120/501] Loss: 1.5924, Batch Acc: 0.2500\n",
      "[Batch 130/501] Loss: 1.9638, Batch Acc: 0.0000\n",
      "[Batch 140/501] Loss: 2.0833, Batch Acc: 0.0000\n",
      "[Batch 150/501] Loss: 1.4975, Batch Acc: 0.1250\n",
      "[Batch 160/501] Loss: 1.8376, Batch Acc: 0.1250\n",
      "[Batch 170/501] Loss: 1.5249, Batch Acc: 0.2500\n",
      "[Batch 180/501] Loss: 1.7782, Batch Acc: 0.2500\n",
      "[Batch 190/501] Loss: 1.7709, Batch Acc: 0.0000\n",
      "[Batch 200/501] Loss: 1.7184, Batch Acc: 0.2500\n",
      "[Batch 210/501] Loss: 1.8584, Batch Acc: 0.3750\n",
      "[Batch 220/501] Loss: 1.7238, Batch Acc: 0.2500\n",
      "[Batch 230/501] Loss: 1.8904, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.5455, Batch Acc: 0.2500\n",
      "[Batch 250/501] Loss: 1.4717, Batch Acc: 0.2500\n",
      "[Batch 260/501] Loss: 1.6169, Batch Acc: 0.2500\n",
      "[Batch 270/501] Loss: 1.9842, Batch Acc: 0.2500\n",
      "[Batch 280/501] Loss: 1.5287, Batch Acc: 0.3750\n",
      "[Batch 290/501] Loss: 1.7729, Batch Acc: 0.1250\n",
      "[Batch 300/501] Loss: 1.7807, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 1.8929, Batch Acc: 0.2500\n",
      "[Batch 320/501] Loss: 1.5251, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 1.2409, Batch Acc: 0.7500\n",
      "[Batch 340/501] Loss: 1.6033, Batch Acc: 0.2500\n",
      "[Batch 350/501] Loss: 1.9152, Batch Acc: 0.3750\n",
      "[Batch 360/501] Loss: 1.5701, Batch Acc: 0.5000\n",
      "[Batch 370/501] Loss: 1.9892, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.7739, Batch Acc: 0.2500\n",
      "[Batch 390/501] Loss: 2.0147, Batch Acc: 0.3750\n",
      "[Batch 400/501] Loss: 1.6137, Batch Acc: 0.1250\n",
      "[Batch 410/501] Loss: 1.5900, Batch Acc: 0.1250\n",
      "[Batch 420/501] Loss: 1.5708, Batch Acc: 0.6250\n",
      "[Batch 430/501] Loss: 1.5914, Batch Acc: 0.5000\n",
      "[Batch 440/501] Loss: 1.9537, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.5817, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.3990, Batch Acc: 0.5000\n",
      "[Batch 470/501] Loss: 1.7461, Batch Acc: 0.1250\n",
      "[Batch 480/501] Loss: 1.3168, Batch Acc: 0.6250\n",
      "[Batch 490/501] Loss: 1.6441, Batch Acc: 0.2500\n",
      "[Batch 500/501] Loss: 1.6048, Batch Acc: 0.2500\n",
      "Epoch 1 Summary - Loss: 860.6143, Train Accuracy: 0.2512\n",
      "[Batch 10/501] Loss: 1.6565, Batch Acc: 0.1250\n",
      "[Batch 20/501] Loss: 1.5701, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.9130, Batch Acc: 0.0000\n",
      "[Batch 40/501] Loss: 1.8616, Batch Acc: 0.0000\n",
      "[Batch 50/501] Loss: 1.4998, Batch Acc: 0.6250\n",
      "[Batch 60/501] Loss: 1.5434, Batch Acc: 0.3750\n",
      "[Batch 70/501] Loss: 2.0432, Batch Acc: 0.1250\n",
      "[Batch 80/501] Loss: 1.5186, Batch Acc: 0.3750\n",
      "[Batch 90/501] Loss: 1.7121, Batch Acc: 0.1250\n",
      "[Batch 100/501] Loss: 1.7672, Batch Acc: 0.3750\n",
      "[Batch 110/501] Loss: 1.5298, Batch Acc: 0.3750\n",
      "[Batch 120/501] Loss: 2.2317, Batch Acc: 0.0000\n",
      "[Batch 130/501] Loss: 1.7652, Batch Acc: 0.2500\n",
      "[Batch 140/501] Loss: 1.6677, Batch Acc: 0.2500\n",
      "[Batch 150/501] Loss: 1.7607, Batch Acc: 0.1250\n",
      "[Batch 160/501] Loss: 1.5200, Batch Acc: 0.5000\n",
      "[Batch 170/501] Loss: 1.6400, Batch Acc: 0.5000\n",
      "[Batch 180/501] Loss: 1.4488, Batch Acc: 0.3750\n",
      "[Batch 190/501] Loss: 1.9023, Batch Acc: 0.1250\n",
      "[Batch 200/501] Loss: 1.8091, Batch Acc: 0.2500\n",
      "[Batch 210/501] Loss: 1.9519, Batch Acc: 0.0000\n",
      "[Batch 220/501] Loss: 2.2021, Batch Acc: 0.1250\n",
      "[Batch 230/501] Loss: 1.4239, Batch Acc: 0.3750\n",
      "[Batch 240/501] Loss: 1.4427, Batch Acc: 0.5000\n",
      "[Batch 250/501] Loss: 1.9034, Batch Acc: 0.2500\n",
      "[Batch 260/501] Loss: 1.4666, Batch Acc: 0.3750\n",
      "[Batch 270/501] Loss: 1.7249, Batch Acc: 0.3750\n",
      "[Batch 280/501] Loss: 1.4243, Batch Acc: 0.5000\n",
      "[Batch 290/501] Loss: 1.5054, Batch Acc: 0.2500\n",
      "[Batch 300/501] Loss: 1.6346, Batch Acc: 0.3750\n",
      "[Batch 310/501] Loss: 1.4072, Batch Acc: 0.3750\n",
      "[Batch 320/501] Loss: 1.3985, Batch Acc: 0.3750\n",
      "[Batch 330/501] Loss: 1.9092, Batch Acc: 0.1250\n",
      "[Batch 340/501] Loss: 1.4541, Batch Acc: 0.2500\n",
      "[Batch 350/501] Loss: 1.7407, Batch Acc: 0.2500\n",
      "[Batch 360/501] Loss: 1.3794, Batch Acc: 0.3750\n",
      "[Batch 370/501] Loss: 1.8249, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.7016, Batch Acc: 0.1250\n",
      "[Batch 390/501] Loss: 1.4781, Batch Acc: 0.3750\n",
      "[Batch 400/501] Loss: 1.7466, Batch Acc: 0.2500\n",
      "[Batch 410/501] Loss: 1.9211, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 2.2537, Batch Acc: 0.0000\n",
      "[Batch 430/501] Loss: 1.7724, Batch Acc: 0.0000\n",
      "[Batch 440/501] Loss: 1.7306, Batch Acc: 0.3750\n",
      "[Batch 450/501] Loss: 1.7883, Batch Acc: 0.0000\n",
      "[Batch 460/501] Loss: 1.6587, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.7224, Batch Acc: 0.1250\n",
      "[Batch 480/501] Loss: 1.8000, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.7013, Batch Acc: 0.1250\n",
      "[Batch 500/501] Loss: 2.0933, Batch Acc: 0.1250\n",
      "Epoch 2 Summary - Loss: 858.4823, Train Accuracy: 0.2505\n",
      "[Batch 10/501] Loss: 1.7129, Batch Acc: 0.2500\n",
      "[Batch 20/501] Loss: 2.0499, Batch Acc: 0.3750\n",
      "[Batch 30/501] Loss: 1.8321, Batch Acc: 0.2500\n",
      "[Batch 40/501] Loss: 1.7783, Batch Acc: 0.2500\n",
      "[Batch 50/501] Loss: 1.7024, Batch Acc: 0.1250\n",
      "[Batch 60/501] Loss: 1.2715, Batch Acc: 0.6250\n",
      "[Batch 70/501] Loss: 1.6483, Batch Acc: 0.3750\n",
      "[Batch 80/501] Loss: 1.4414, Batch Acc: 0.6250\n",
      "[Batch 90/501] Loss: 1.6945, Batch Acc: 0.1250\n",
      "[Batch 100/501] Loss: 1.8714, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 1.9634, Batch Acc: 0.2500\n",
      "[Batch 120/501] Loss: 2.0489, Batch Acc: 0.0000\n",
      "[Batch 130/501] Loss: 1.6250, Batch Acc: 0.2500\n",
      "[Batch 140/501] Loss: 1.7496, Batch Acc: 0.2500\n",
      "[Batch 150/501] Loss: 1.7225, Batch Acc: 0.3750\n",
      "[Batch 160/501] Loss: 1.9760, Batch Acc: 0.2500\n",
      "[Batch 170/501] Loss: 1.9239, Batch Acc: 0.0000\n",
      "[Batch 180/501] Loss: 1.6342, Batch Acc: 0.3750\n",
      "[Batch 190/501] Loss: 1.6185, Batch Acc: 0.3750\n",
      "[Batch 200/501] Loss: 1.2774, Batch Acc: 0.3750\n",
      "[Batch 210/501] Loss: 1.7900, Batch Acc: 0.5000\n",
      "[Batch 220/501] Loss: 1.5821, Batch Acc: 0.3750\n",
      "[Batch 230/501] Loss: 1.7326, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.2877, Batch Acc: 0.5000\n",
      "[Batch 250/501] Loss: 1.8974, Batch Acc: 0.2500\n",
      "[Batch 260/501] Loss: 1.8267, Batch Acc: 0.1250\n",
      "[Batch 270/501] Loss: 1.6427, Batch Acc: 0.5000\n",
      "[Batch 280/501] Loss: 1.5506, Batch Acc: 0.5000\n",
      "[Batch 290/501] Loss: 1.4778, Batch Acc: 0.2500\n",
      "[Batch 300/501] Loss: 1.6567, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 1.7233, Batch Acc: 0.1250\n",
      "[Batch 320/501] Loss: 1.7299, Batch Acc: 0.3750\n",
      "[Batch 330/501] Loss: 1.9627, Batch Acc: 0.0000\n",
      "[Batch 340/501] Loss: 1.9266, Batch Acc: 0.1250\n",
      "[Batch 350/501] Loss: 2.0065, Batch Acc: 0.2500\n",
      "[Batch 360/501] Loss: 1.7924, Batch Acc: 0.3750\n",
      "[Batch 370/501] Loss: 1.8140, Batch Acc: 0.0000\n",
      "[Batch 380/501] Loss: 1.5476, Batch Acc: 0.3750\n",
      "[Batch 390/501] Loss: 1.9287, Batch Acc: 0.1250\n",
      "[Batch 400/501] Loss: 1.9233, Batch Acc: 0.1250\n",
      "[Batch 410/501] Loss: 1.5772, Batch Acc: 0.3750\n",
      "[Batch 420/501] Loss: 1.8894, Batch Acc: 0.1250\n",
      "[Batch 430/501] Loss: 2.0734, Batch Acc: 0.0000\n",
      "[Batch 440/501] Loss: 2.4073, Batch Acc: 0.1250\n",
      "[Batch 450/501] Loss: 1.8355, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.7808, Batch Acc: 0.1250\n",
      "[Batch 470/501] Loss: 1.6042, Batch Acc: 0.3750\n",
      "[Batch 480/501] Loss: 1.7558, Batch Acc: 0.3750\n",
      "[Batch 490/501] Loss: 1.3829, Batch Acc: 0.3750\n",
      "[Batch 500/501] Loss: 1.8268, Batch Acc: 0.3750\n",
      "Epoch 3 Summary - Loss: 863.9787, Train Accuracy: 0.2535\n",
      "[Batch 10/501] Loss: 1.3655, Batch Acc: 0.3750\n",
      "[Batch 20/501] Loss: 1.9289, Batch Acc: 0.1250\n",
      "[Batch 30/501] Loss: 1.2640, Batch Acc: 0.5000\n",
      "[Batch 40/501] Loss: 1.7379, Batch Acc: 0.2500\n",
      "[Batch 50/501] Loss: 1.3112, Batch Acc: 0.3750\n",
      "[Batch 60/501] Loss: 1.7608, Batch Acc: 0.1250\n",
      "[Batch 70/501] Loss: 1.5355, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.7199, Batch Acc: 0.1250\n",
      "[Batch 90/501] Loss: 1.8165, Batch Acc: 0.2500\n",
      "[Batch 100/501] Loss: 1.8912, Batch Acc: 0.0000\n",
      "[Batch 110/501] Loss: 1.7433, Batch Acc: 0.1250\n",
      "[Batch 120/501] Loss: 1.2856, Batch Acc: 0.3750\n",
      "[Batch 130/501] Loss: 1.8425, Batch Acc: 0.1250\n",
      "[Batch 140/501] Loss: 1.6016, Batch Acc: 0.5000\n",
      "[Batch 150/501] Loss: 1.6975, Batch Acc: 0.2500\n",
      "[Batch 160/501] Loss: 2.0809, Batch Acc: 0.1250\n",
      "[Batch 170/501] Loss: 1.6632, Batch Acc: 0.2500\n",
      "[Batch 180/501] Loss: 1.6018, Batch Acc: 0.5000\n",
      "[Batch 190/501] Loss: 1.6371, Batch Acc: 0.2500\n",
      "[Batch 200/501] Loss: 1.8870, Batch Acc: 0.1250\n",
      "[Batch 210/501] Loss: 1.6503, Batch Acc: 0.1250\n",
      "[Batch 220/501] Loss: 1.6021, Batch Acc: 0.3750\n",
      "[Batch 230/501] Loss: 1.8086, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.9977, Batch Acc: 0.2500\n",
      "[Batch 250/501] Loss: 1.9320, Batch Acc: 0.2500\n",
      "[Batch 260/501] Loss: 1.5371, Batch Acc: 0.3750\n",
      "[Batch 270/501] Loss: 2.0476, Batch Acc: 0.0000\n",
      "[Batch 280/501] Loss: 2.2274, Batch Acc: 0.3750\n",
      "[Batch 290/501] Loss: 1.8376, Batch Acc: 0.3750\n",
      "[Batch 300/501] Loss: 1.9781, Batch Acc: 0.0000\n",
      "[Batch 310/501] Loss: 1.6636, Batch Acc: 0.1250\n",
      "[Batch 320/501] Loss: 1.7438, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 1.8250, Batch Acc: 0.1250\n",
      "[Batch 340/501] Loss: 1.7984, Batch Acc: 0.2500\n",
      "[Batch 350/501] Loss: 1.6211, Batch Acc: 0.1250\n",
      "[Batch 360/501] Loss: 1.5662, Batch Acc: 0.3750\n",
      "[Batch 370/501] Loss: 1.4544, Batch Acc: 0.6250\n",
      "[Batch 380/501] Loss: 1.5591, Batch Acc: 0.1250\n",
      "[Batch 390/501] Loss: 1.6594, Batch Acc: 0.3750\n",
      "[Batch 400/501] Loss: 1.3040, Batch Acc: 0.6250\n",
      "[Batch 410/501] Loss: 1.8650, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 2.2666, Batch Acc: 0.1250\n",
      "[Batch 430/501] Loss: 1.5560, Batch Acc: 0.2500\n",
      "[Batch 440/501] Loss: 1.7085, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.5825, Batch Acc: 0.3750\n",
      "[Batch 460/501] Loss: 1.3931, Batch Acc: 0.6250\n",
      "[Batch 470/501] Loss: 1.6274, Batch Acc: 0.2500\n",
      "[Batch 480/501] Loss: 1.5964, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.6773, Batch Acc: 0.2500\n",
      "[Batch 500/501] Loss: 1.8135, Batch Acc: 0.2500\n",
      "Epoch 4 Summary - Loss: 864.5690, Train Accuracy: 0.2483\n",
      "[Batch 10/501] Loss: 1.6773, Batch Acc: 0.1250\n",
      "[Batch 20/501] Loss: 1.5912, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.8448, Batch Acc: 0.1250\n",
      "[Batch 40/501] Loss: 1.8097, Batch Acc: 0.3750\n",
      "[Batch 50/501] Loss: 1.8278, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.8591, Batch Acc: 0.0000\n",
      "[Batch 70/501] Loss: 1.6205, Batch Acc: 0.1250\n",
      "[Batch 80/501] Loss: 1.8805, Batch Acc: 0.0000\n",
      "[Batch 90/501] Loss: 1.7828, Batch Acc: 0.2500\n",
      "[Batch 100/501] Loss: 1.3821, Batch Acc: 0.6250\n",
      "[Batch 110/501] Loss: 1.6414, Batch Acc: 0.2500\n",
      "[Batch 120/501] Loss: 1.8674, Batch Acc: 0.1250\n",
      "[Batch 130/501] Loss: 1.8688, Batch Acc: 0.2500\n",
      "[Batch 140/501] Loss: 1.8176, Batch Acc: 0.2500\n",
      "[Batch 150/501] Loss: 1.5284, Batch Acc: 0.2500\n",
      "[Batch 160/501] Loss: 1.7241, Batch Acc: 0.2500\n",
      "[Batch 170/501] Loss: 1.2126, Batch Acc: 0.7500\n",
      "[Batch 180/501] Loss: 1.5477, Batch Acc: 0.3750\n",
      "[Batch 190/501] Loss: 1.4798, Batch Acc: 0.1250\n",
      "[Batch 200/501] Loss: 1.6385, Batch Acc: 0.2500\n",
      "[Batch 210/501] Loss: 1.5693, Batch Acc: 0.3750\n",
      "[Batch 220/501] Loss: 1.3889, Batch Acc: 0.2500\n",
      "[Batch 230/501] Loss: 1.5097, Batch Acc: 0.5000\n",
      "[Batch 240/501] Loss: 1.2471, Batch Acc: 0.7500\n",
      "[Batch 250/501] Loss: 1.9358, Batch Acc: 0.0000\n",
      "[Batch 260/501] Loss: 1.6627, Batch Acc: 0.2500\n",
      "[Batch 270/501] Loss: 2.2840, Batch Acc: 0.0000\n",
      "[Batch 280/501] Loss: 1.6553, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 1.3684, Batch Acc: 0.6250\n",
      "[Batch 300/501] Loss: 1.9426, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 1.9914, Batch Acc: 0.1250\n",
      "[Batch 320/501] Loss: 1.5482, Batch Acc: 0.5000\n",
      "[Batch 330/501] Loss: 1.5411, Batch Acc: 0.6250\n",
      "[Batch 340/501] Loss: 2.0623, Batch Acc: 0.0000\n",
      "[Batch 350/501] Loss: 1.6560, Batch Acc: 0.1250\n",
      "[Batch 360/501] Loss: 1.8055, Batch Acc: 0.1250\n",
      "[Batch 370/501] Loss: 1.5232, Batch Acc: 0.3750\n",
      "[Batch 380/501] Loss: 1.6125, Batch Acc: 0.3750\n",
      "[Batch 390/501] Loss: 1.7658, Batch Acc: 0.1250\n",
      "[Batch 400/501] Loss: 1.6702, Batch Acc: 0.2500\n",
      "[Batch 410/501] Loss: 1.3272, Batch Acc: 0.3750\n",
      "[Batch 420/501] Loss: 1.5326, Batch Acc: 0.2500\n",
      "[Batch 430/501] Loss: 1.5055, Batch Acc: 0.1250\n",
      "[Batch 440/501] Loss: 1.8090, Batch Acc: 0.1250\n",
      "[Batch 450/501] Loss: 1.8566, Batch Acc: 0.0000\n",
      "[Batch 460/501] Loss: 1.7074, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 2.0978, Batch Acc: 0.1250\n",
      "[Batch 480/501] Loss: 1.8031, Batch Acc: 0.0000\n",
      "[Batch 490/501] Loss: 2.0051, Batch Acc: 0.1250\n",
      "[Batch 500/501] Loss: 1.4695, Batch Acc: 0.3750\n",
      "Epoch 5 Summary - Loss: 858.5982, Train Accuracy: 0.2512\n",
      "[Batch 10/501] Loss: 1.8072, Batch Acc: 0.0000\n",
      "[Batch 20/501] Loss: 1.3836, Batch Acc: 0.3750\n",
      "[Batch 30/501] Loss: 1.5363, Batch Acc: 0.5000\n",
      "[Batch 40/501] Loss: 1.3429, Batch Acc: 0.2500\n",
      "[Batch 50/501] Loss: 1.8191, Batch Acc: 0.1250\n",
      "[Batch 60/501] Loss: 2.0542, Batch Acc: 0.1250\n",
      "[Batch 70/501] Loss: 1.5743, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.6884, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.8729, Batch Acc: 0.1250\n",
      "[Batch 100/501] Loss: 1.4554, Batch Acc: 0.3750\n",
      "[Batch 110/501] Loss: 2.2669, Batch Acc: 0.1250\n",
      "[Batch 120/501] Loss: 2.1680, Batch Acc: 0.1250\n",
      "[Batch 130/501] Loss: 1.6594, Batch Acc: 0.3750\n",
      "[Batch 140/501] Loss: 1.4826, Batch Acc: 0.1250\n",
      "[Batch 150/501] Loss: 1.8419, Batch Acc: 0.1250\n",
      "[Batch 160/501] Loss: 1.5196, Batch Acc: 0.3750\n",
      "[Batch 170/501] Loss: 1.6864, Batch Acc: 0.3750\n",
      "[Batch 180/501] Loss: 1.9783, Batch Acc: 0.1250\n",
      "[Batch 190/501] Loss: 1.8548, Batch Acc: 0.1250\n",
      "[Batch 200/501] Loss: 1.5113, Batch Acc: 0.3750\n",
      "[Batch 210/501] Loss: 1.6404, Batch Acc: 0.3750\n",
      "[Batch 220/501] Loss: 1.6948, Batch Acc: 0.3750\n",
      "[Batch 230/501] Loss: 1.6601, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.6304, Batch Acc: 0.0000\n",
      "[Batch 250/501] Loss: 1.8135, Batch Acc: 0.2500\n",
      "[Batch 260/501] Loss: 1.6194, Batch Acc: 0.3750\n",
      "[Batch 270/501] Loss: 1.6163, Batch Acc: 0.2500\n",
      "[Batch 280/501] Loss: 1.6440, Batch Acc: 0.2500\n",
      "[Batch 290/501] Loss: 2.0370, Batch Acc: 0.1250\n",
      "[Batch 300/501] Loss: 1.6275, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 1.7097, Batch Acc: 0.1250\n",
      "[Batch 320/501] Loss: 1.7944, Batch Acc: 0.1250\n",
      "[Batch 330/501] Loss: 1.4235, Batch Acc: 0.5000\n",
      "[Batch 340/501] Loss: 1.9124, Batch Acc: 0.1250\n",
      "[Batch 350/501] Loss: 2.3969, Batch Acc: 0.1250\n",
      "[Batch 360/501] Loss: 1.6635, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 1.7527, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.8909, Batch Acc: 0.2500\n",
      "[Batch 390/501] Loss: 1.5779, Batch Acc: 0.2500\n",
      "[Batch 400/501] Loss: 1.6732, Batch Acc: 0.3750\n",
      "[Batch 410/501] Loss: 1.7102, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.7013, Batch Acc: 0.3750\n",
      "[Batch 430/501] Loss: 2.2175, Batch Acc: 0.1250\n",
      "[Batch 440/501] Loss: 2.2188, Batch Acc: 0.1250\n",
      "[Batch 450/501] Loss: 1.9490, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.7448, Batch Acc: 0.3750\n",
      "[Batch 470/501] Loss: 2.0430, Batch Acc: 0.1250\n",
      "[Batch 480/501] Loss: 1.7892, Batch Acc: 0.0000\n",
      "[Batch 490/501] Loss: 1.8273, Batch Acc: 0.2500\n",
      "[Batch 500/501] Loss: 1.7365, Batch Acc: 0.2500\n",
      "Epoch 6 Summary - Loss: 859.0066, Train Accuracy: 0.2502\n",
      "[Batch 10/501] Loss: 1.8384, Batch Acc: 0.1250\n",
      "[Batch 20/501] Loss: 1.6375, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.5793, Batch Acc: 0.2500\n",
      "[Batch 40/501] Loss: 1.5227, Batch Acc: 0.2500\n",
      "[Batch 50/501] Loss: 1.8178, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.9676, Batch Acc: 0.1250\n",
      "[Batch 70/501] Loss: 1.3993, Batch Acc: 0.5000\n",
      "[Batch 80/501] Loss: 1.8544, Batch Acc: 0.1250\n",
      "[Batch 90/501] Loss: 1.7654, Batch Acc: 0.1250\n",
      "[Batch 100/501] Loss: 1.7097, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 1.8214, Batch Acc: 0.0000\n",
      "[Batch 120/501] Loss: 1.5924, Batch Acc: 0.2500\n",
      "[Batch 130/501] Loss: 1.3094, Batch Acc: 0.2500\n",
      "[Batch 140/501] Loss: 2.0790, Batch Acc: 0.1250\n",
      "[Batch 150/501] Loss: 1.4389, Batch Acc: 0.5000\n",
      "[Batch 160/501] Loss: 1.7435, Batch Acc: 0.1250\n",
      "[Batch 170/501] Loss: 1.6047, Batch Acc: 0.3750\n",
      "[Batch 180/501] Loss: 1.8764, Batch Acc: 0.1250\n",
      "[Batch 190/501] Loss: 1.5714, Batch Acc: 0.5000\n",
      "[Batch 200/501] Loss: 1.5448, Batch Acc: 0.3750\n",
      "[Batch 210/501] Loss: 1.5867, Batch Acc: 0.3750\n",
      "[Batch 220/501] Loss: 1.8867, Batch Acc: 0.0000\n",
      "[Batch 230/501] Loss: 2.0382, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.7256, Batch Acc: 0.2500\n",
      "[Batch 250/501] Loss: 1.6033, Batch Acc: 0.1250\n",
      "[Batch 260/501] Loss: 1.7475, Batch Acc: 0.2500\n",
      "[Batch 270/501] Loss: 1.7874, Batch Acc: 0.0000\n",
      "[Batch 280/501] Loss: 1.6389, Batch Acc: 0.3750\n",
      "[Batch 290/501] Loss: 1.8803, Batch Acc: 0.1250\n",
      "[Batch 300/501] Loss: 1.7650, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 1.5762, Batch Acc: 0.3750\n",
      "[Batch 320/501] Loss: 1.7411, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 2.0560, Batch Acc: 0.3750\n",
      "[Batch 340/501] Loss: 1.7696, Batch Acc: 0.2500\n",
      "[Batch 350/501] Loss: 1.9370, Batch Acc: 0.2500\n",
      "[Batch 360/501] Loss: 1.6826, Batch Acc: 0.1250\n",
      "[Batch 370/501] Loss: 2.1347, Batch Acc: 0.2500\n",
      "[Batch 380/501] Loss: 1.8361, Batch Acc: 0.2500\n",
      "[Batch 390/501] Loss: 1.7726, Batch Acc: 0.1250\n",
      "[Batch 400/501] Loss: 1.5395, Batch Acc: 0.3750\n",
      "[Batch 410/501] Loss: 1.8450, Batch Acc: 0.1250\n",
      "[Batch 420/501] Loss: 1.4795, Batch Acc: 0.2500\n",
      "[Batch 430/501] Loss: 1.7303, Batch Acc: 0.3750\n",
      "[Batch 440/501] Loss: 1.7049, Batch Acc: 0.1250\n",
      "[Batch 450/501] Loss: 1.8110, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.6932, Batch Acc: 0.0000\n",
      "[Batch 470/501] Loss: 1.9516, Batch Acc: 0.1250\n",
      "[Batch 480/501] Loss: 1.6671, Batch Acc: 0.1250\n",
      "[Batch 490/501] Loss: 1.4830, Batch Acc: 0.3750\n",
      "[Batch 500/501] Loss: 2.2127, Batch Acc: 0.1250\n",
      "Epoch 7 Summary - Loss: 856.2165, Train Accuracy: 0.2547\n",
      "[Batch 10/501] Loss: 1.3562, Batch Acc: 0.5000\n",
      "[Batch 20/501] Loss: 1.4879, Batch Acc: 0.5000\n",
      "[Batch 30/501] Loss: 1.7811, Batch Acc: 0.2500\n",
      "[Batch 40/501] Loss: 1.7091, Batch Acc: 0.3750\n",
      "[Batch 50/501] Loss: 1.8733, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.9024, Batch Acc: 0.1250\n",
      "[Batch 70/501] Loss: 2.0306, Batch Acc: 0.3750\n",
      "[Batch 80/501] Loss: 1.6971, Batch Acc: 0.0000\n",
      "[Batch 90/501] Loss: 1.4315, Batch Acc: 0.3750\n",
      "[Batch 100/501] Loss: 1.8381, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 1.3693, Batch Acc: 0.3750\n",
      "[Batch 120/501] Loss: 1.7495, Batch Acc: 0.1250\n",
      "[Batch 130/501] Loss: 1.6884, Batch Acc: 0.2500\n",
      "[Batch 140/501] Loss: 1.8935, Batch Acc: 0.0000\n",
      "[Batch 150/501] Loss: 2.0623, Batch Acc: 0.3750\n",
      "[Batch 160/501] Loss: 1.5952, Batch Acc: 0.1250\n",
      "[Batch 170/501] Loss: 1.6029, Batch Acc: 0.1250\n",
      "[Batch 180/501] Loss: 1.6702, Batch Acc: 0.3750\n",
      "[Batch 190/501] Loss: 1.6638, Batch Acc: 0.2500\n",
      "[Batch 200/501] Loss: 1.6523, Batch Acc: 0.3750\n",
      "[Batch 210/501] Loss: 1.5616, Batch Acc: 0.3750\n",
      "[Batch 220/501] Loss: 1.8291, Batch Acc: 0.1250\n",
      "[Batch 230/501] Loss: 1.7954, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.6872, Batch Acc: 0.0000\n",
      "[Batch 250/501] Loss: 1.7190, Batch Acc: 0.2500\n",
      "[Batch 260/501] Loss: 1.1304, Batch Acc: 0.6250\n",
      "[Batch 270/501] Loss: 1.5453, Batch Acc: 0.3750\n",
      "[Batch 280/501] Loss: 1.8448, Batch Acc: 0.0000\n",
      "[Batch 290/501] Loss: 2.0213, Batch Acc: 0.0000\n",
      "[Batch 300/501] Loss: 1.6522, Batch Acc: 0.1250\n",
      "[Batch 310/501] Loss: 2.0538, Batch Acc: 0.0000\n",
      "[Batch 320/501] Loss: 1.6732, Batch Acc: 0.3750\n",
      "[Batch 330/501] Loss: 1.5308, Batch Acc: 0.2500\n",
      "[Batch 340/501] Loss: 2.0074, Batch Acc: 0.0000\n",
      "[Batch 350/501] Loss: 1.8051, Batch Acc: 0.1250\n",
      "[Batch 360/501] Loss: 2.1125, Batch Acc: 0.0000\n",
      "[Batch 370/501] Loss: 2.1965, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.2302, Batch Acc: 0.6250\n",
      "[Batch 390/501] Loss: 1.3543, Batch Acc: 0.6250\n",
      "[Batch 400/501] Loss: 1.9183, Batch Acc: 0.5000\n",
      "[Batch 410/501] Loss: 1.4039, Batch Acc: 0.5000\n",
      "[Batch 420/501] Loss: 1.5101, Batch Acc: 0.2500\n",
      "[Batch 430/501] Loss: 1.7956, Batch Acc: 0.1250\n",
      "[Batch 440/501] Loss: 1.7808, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 2.1648, Batch Acc: 0.1250\n",
      "[Batch 460/501] Loss: 1.8689, Batch Acc: 0.0000\n",
      "[Batch 470/501] Loss: 1.3547, Batch Acc: 0.5000\n",
      "[Batch 480/501] Loss: 1.8961, Batch Acc: 0.3750\n",
      "[Batch 490/501] Loss: 1.7421, Batch Acc: 0.1250\n",
      "[Batch 500/501] Loss: 1.6727, Batch Acc: 0.2500\n",
      "Epoch 8 Summary - Loss: 859.7640, Train Accuracy: 0.2527\n",
      "[Batch 10/501] Loss: 2.2060, Batch Acc: 0.2500\n",
      "[Batch 20/501] Loss: 1.7381, Batch Acc: 0.3750\n",
      "[Batch 30/501] Loss: 1.1734, Batch Acc: 0.6250\n",
      "[Batch 40/501] Loss: 1.5877, Batch Acc: 0.0000\n",
      "[Batch 50/501] Loss: 1.8655, Batch Acc: 0.1250\n",
      "[Batch 60/501] Loss: 1.9087, Batch Acc: 0.1250\n",
      "[Batch 70/501] Loss: 1.8664, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.6564, Batch Acc: 0.3750\n",
      "[Batch 90/501] Loss: 1.2888, Batch Acc: 0.3750\n",
      "[Batch 100/501] Loss: 2.1498, Batch Acc: 0.0000\n",
      "[Batch 110/501] Loss: 1.9766, Batch Acc: 0.1250\n",
      "[Batch 120/501] Loss: 1.5993, Batch Acc: 0.3750\n",
      "[Batch 130/501] Loss: 1.4460, Batch Acc: 0.3750\n",
      "[Batch 140/501] Loss: 1.4702, Batch Acc: 0.2500\n",
      "[Batch 150/501] Loss: 1.8071, Batch Acc: 0.3750\n",
      "[Batch 160/501] Loss: 2.0610, Batch Acc: 0.0000\n",
      "[Batch 170/501] Loss: 1.8015, Batch Acc: 0.1250\n",
      "[Batch 180/501] Loss: 1.5978, Batch Acc: 0.3750\n",
      "[Batch 190/501] Loss: 2.4925, Batch Acc: 0.0000\n",
      "[Batch 200/501] Loss: 1.6464, Batch Acc: 0.3750\n",
      "[Batch 210/501] Loss: 1.4621, Batch Acc: 0.3750\n",
      "[Batch 220/501] Loss: 1.9136, Batch Acc: 0.2500\n",
      "[Batch 230/501] Loss: 1.6810, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.5160, Batch Acc: 0.3750\n",
      "[Batch 250/501] Loss: 1.6759, Batch Acc: 0.2500\n",
      "[Batch 260/501] Loss: 2.0195, Batch Acc: 0.0000\n",
      "[Batch 270/501] Loss: 1.7247, Batch Acc: 0.3750\n",
      "[Batch 280/501] Loss: 1.6699, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 2.0124, Batch Acc: 0.2500\n",
      "[Batch 300/501] Loss: 1.9260, Batch Acc: 0.1250\n",
      "[Batch 310/501] Loss: 2.3192, Batch Acc: 0.1250\n",
      "[Batch 320/501] Loss: 1.9464, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 1.6745, Batch Acc: 0.3750\n",
      "[Batch 340/501] Loss: 1.7242, Batch Acc: 0.3750\n",
      "[Batch 350/501] Loss: 1.4834, Batch Acc: 0.1250\n",
      "[Batch 360/501] Loss: 1.6705, Batch Acc: 0.5000\n",
      "[Batch 370/501] Loss: 1.7751, Batch Acc: 0.0000\n",
      "[Batch 380/501] Loss: 1.5303, Batch Acc: 0.2500\n",
      "[Batch 390/501] Loss: 1.6467, Batch Acc: 0.6250\n",
      "[Batch 400/501] Loss: 1.7944, Batch Acc: 0.1250\n",
      "[Batch 410/501] Loss: 1.8933, Batch Acc: 0.1250\n",
      "[Batch 420/501] Loss: 1.4124, Batch Acc: 0.6250\n",
      "[Batch 430/501] Loss: 1.5001, Batch Acc: 0.3750\n",
      "[Batch 440/501] Loss: 1.7443, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.6384, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.4285, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.6591, Batch Acc: 0.3750\n",
      "[Batch 480/501] Loss: 1.8744, Batch Acc: 0.3750\n",
      "[Batch 490/501] Loss: 1.5083, Batch Acc: 0.2500\n",
      "[Batch 500/501] Loss: 1.4847, Batch Acc: 0.2500\n",
      "Epoch 9 Summary - Loss: 859.6636, Train Accuracy: 0.2517\n",
      "[Batch 10/501] Loss: 1.6754, Batch Acc: 0.2500\n",
      "[Batch 20/501] Loss: 1.6553, Batch Acc: 0.3750\n",
      "[Batch 30/501] Loss: 2.0663, Batch Acc: 0.1250\n",
      "[Batch 40/501] Loss: 1.9581, Batch Acc: 0.2500\n",
      "[Batch 50/501] Loss: 2.2142, Batch Acc: 0.1250\n",
      "[Batch 60/501] Loss: 1.5313, Batch Acc: 0.2500\n",
      "[Batch 70/501] Loss: 1.7984, Batch Acc: 0.1250\n",
      "[Batch 80/501] Loss: 1.3382, Batch Acc: 0.3750\n",
      "[Batch 90/501] Loss: 1.7341, Batch Acc: 0.2500\n",
      "[Batch 100/501] Loss: 1.9108, Batch Acc: 0.0000\n",
      "[Batch 110/501] Loss: 1.6804, Batch Acc: 0.2500\n",
      "[Batch 120/501] Loss: 1.5192, Batch Acc: 0.6250\n",
      "[Batch 130/501] Loss: 1.4164, Batch Acc: 0.7500\n",
      "[Batch 140/501] Loss: 1.9158, Batch Acc: 0.0000\n",
      "[Batch 150/501] Loss: 1.8441, Batch Acc: 0.2500\n",
      "[Batch 160/501] Loss: 1.7372, Batch Acc: 0.1250\n",
      "[Batch 170/501] Loss: 1.6402, Batch Acc: 0.1250\n",
      "[Batch 180/501] Loss: 1.7345, Batch Acc: 0.3750\n",
      "[Batch 190/501] Loss: 1.7927, Batch Acc: 0.1250\n",
      "[Batch 200/501] Loss: 1.8944, Batch Acc: 0.3750\n",
      "[Batch 210/501] Loss: 1.6038, Batch Acc: 0.3750\n",
      "[Batch 220/501] Loss: 1.5210, Batch Acc: 0.2500\n",
      "[Batch 230/501] Loss: 1.4061, Batch Acc: 0.5000\n",
      "[Batch 240/501] Loss: 1.6454, Batch Acc: 0.1250\n",
      "[Batch 250/501] Loss: 2.0971, Batch Acc: 0.1250\n",
      "[Batch 260/501] Loss: 2.0505, Batch Acc: 0.1250\n",
      "[Batch 270/501] Loss: 1.6677, Batch Acc: 0.2500\n",
      "[Batch 280/501] Loss: 2.0649, Batch Acc: 0.0000\n",
      "[Batch 290/501] Loss: 1.8040, Batch Acc: 0.3750\n",
      "[Batch 300/501] Loss: 1.9307, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 1.8313, Batch Acc: 0.0000\n",
      "[Batch 320/501] Loss: 1.5130, Batch Acc: 0.1250\n",
      "[Batch 330/501] Loss: 1.3392, Batch Acc: 0.3750\n",
      "[Batch 340/501] Loss: 1.7756, Batch Acc: 0.2500\n",
      "[Batch 350/501] Loss: 1.4452, Batch Acc: 0.3750\n",
      "[Batch 360/501] Loss: 1.5282, Batch Acc: 0.3750\n",
      "[Batch 370/501] Loss: 1.4126, Batch Acc: 0.3750\n",
      "[Batch 380/501] Loss: 1.5278, Batch Acc: 0.5000\n",
      "[Batch 390/501] Loss: 1.4263, Batch Acc: 0.3750\n",
      "[Batch 400/501] Loss: 1.7273, Batch Acc: 0.2500\n",
      "[Batch 410/501] Loss: 1.6080, Batch Acc: 0.6250\n",
      "[Batch 420/501] Loss: 1.5114, Batch Acc: 0.6250\n",
      "[Batch 430/501] Loss: 1.3792, Batch Acc: 0.6250\n",
      "[Batch 440/501] Loss: 1.5593, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.4799, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.6984, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.7438, Batch Acc: 0.1250\n",
      "[Batch 480/501] Loss: 1.5222, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.7816, Batch Acc: 0.2500\n",
      "[Batch 500/501] Loss: 1.6557, Batch Acc: 0.1250\n",
      "Epoch 10 Summary - Loss: 858.5775, Train Accuracy: 0.2485\n",
      "Test Accuracy: 0.2404\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_accuracy</td><td>▄▃▇▁▄▃█▆▅▁</td></tr><tr><td>train_loss</td><td>▅▃██▃▃▁▄▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>test_accuracy</td><td>0.24036</td></tr><tr><td>test_loss</td><td>217.75261</td></tr><tr><td>train_accuracy</td><td>0.2485</td></tr><tr><td>train_loss</td><td>858.57746</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">usual-sweep-11</strong> at: <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/kpq2y5io' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/kpq2y5io</a><br> View project at: <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250615_142453-kpq2y5io/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 087nxvhe with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_nodes: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ajit.kumar4@happiestminds.com/Documents/Drackula/ComputerVisionFromScratch/wandb/run-20250615_142828-087nxvhe</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/087nxvhe' target=\"_blank\">sweet-sweep-12</a></strong> to <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/087nxvhe' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/087nxvhe</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 10/501] Loss: 1.8572, Batch Acc: 0.2500\n",
      "[Batch 20/501] Loss: 1.7466, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.4482, Batch Acc: 0.2500\n",
      "[Batch 40/501] Loss: 1.6065, Batch Acc: 0.3750\n",
      "[Batch 50/501] Loss: 1.5878, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.2744, Batch Acc: 0.5000\n",
      "[Batch 70/501] Loss: 1.6756, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.5053, Batch Acc: 0.3750\n",
      "[Batch 90/501] Loss: 1.7127, Batch Acc: 0.0000\n",
      "[Batch 100/501] Loss: 1.7780, Batch Acc: 0.1250\n",
      "[Batch 110/501] Loss: 1.4531, Batch Acc: 0.3750\n",
      "[Batch 120/501] Loss: 1.6846, Batch Acc: 0.1250\n",
      "[Batch 130/501] Loss: 1.6342, Batch Acc: 0.2500\n",
      "[Batch 140/501] Loss: 1.7016, Batch Acc: 0.1250\n",
      "[Batch 150/501] Loss: 1.4003, Batch Acc: 0.5000\n",
      "[Batch 160/501] Loss: 1.7583, Batch Acc: 0.0000\n",
      "[Batch 170/501] Loss: 1.7736, Batch Acc: 0.0000\n",
      "[Batch 180/501] Loss: 1.4775, Batch Acc: 0.5000\n",
      "[Batch 190/501] Loss: 1.6958, Batch Acc: 0.5000\n",
      "[Batch 200/501] Loss: 2.0444, Batch Acc: 0.1250\n",
      "[Batch 210/501] Loss: 1.8600, Batch Acc: 0.0000\n",
      "[Batch 220/501] Loss: 1.4234, Batch Acc: 0.2500\n",
      "[Batch 230/501] Loss: 2.1455, Batch Acc: 0.1250\n",
      "[Batch 240/501] Loss: 1.5800, Batch Acc: 0.2500\n",
      "[Batch 250/501] Loss: 1.2535, Batch Acc: 0.3750\n",
      "[Batch 260/501] Loss: 1.6774, Batch Acc: 0.2500\n",
      "[Batch 270/501] Loss: 1.6110, Batch Acc: 0.3750\n",
      "[Batch 280/501] Loss: 1.5588, Batch Acc: 0.5000\n",
      "[Batch 290/501] Loss: 1.7563, Batch Acc: 0.1250\n",
      "[Batch 300/501] Loss: 1.4041, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 1.3474, Batch Acc: 0.3750\n",
      "[Batch 320/501] Loss: 1.8002, Batch Acc: 0.0000\n",
      "[Batch 330/501] Loss: 1.7259, Batch Acc: 0.1250\n",
      "[Batch 340/501] Loss: 1.4149, Batch Acc: 0.3750\n",
      "[Batch 350/501] Loss: 1.6731, Batch Acc: 0.0000\n",
      "[Batch 360/501] Loss: 1.7026, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 1.8220, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.8153, Batch Acc: 0.1250\n",
      "[Batch 390/501] Loss: 1.6659, Batch Acc: 0.3750\n",
      "[Batch 400/501] Loss: 1.8112, Batch Acc: 0.0000\n",
      "[Batch 410/501] Loss: 1.4796, Batch Acc: 0.3750\n",
      "[Batch 420/501] Loss: 1.4754, Batch Acc: 0.3750\n",
      "[Batch 430/501] Loss: 1.7483, Batch Acc: 0.0000\n",
      "[Batch 440/501] Loss: 1.4983, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.5412, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.5795, Batch Acc: 0.3750\n",
      "[Batch 470/501] Loss: 1.7062, Batch Acc: 0.2500\n",
      "[Batch 480/501] Loss: 1.7347, Batch Acc: 0.0000\n",
      "[Batch 490/501] Loss: 1.4024, Batch Acc: 0.3750\n",
      "[Batch 500/501] Loss: 1.7368, Batch Acc: 0.2500\n",
      "Epoch 1 Summary - Loss: 813.2894, Train Accuracy: 0.2522\n",
      "[Batch 10/501] Loss: 1.8120, Batch Acc: 0.0000\n",
      "[Batch 20/501] Loss: 1.6542, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.8565, Batch Acc: 0.1250\n",
      "[Batch 40/501] Loss: 1.7644, Batch Acc: 0.1250\n",
      "[Batch 50/501] Loss: 1.5474, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.8663, Batch Acc: 0.2500\n",
      "[Batch 70/501] Loss: 1.7000, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.7784, Batch Acc: 0.1250\n",
      "[Batch 90/501] Loss: 1.6321, Batch Acc: 0.2500\n",
      "[Batch 100/501] Loss: 1.6467, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 1.8003, Batch Acc: 0.1250\n",
      "[Batch 120/501] Loss: 1.8339, Batch Acc: 0.0000\n",
      "[Batch 130/501] Loss: 1.4429, Batch Acc: 0.3750\n",
      "[Batch 140/501] Loss: 1.6107, Batch Acc: 0.5000\n",
      "[Batch 150/501] Loss: 2.0987, Batch Acc: 0.0000\n",
      "[Batch 160/501] Loss: 1.5251, Batch Acc: 0.2500\n",
      "[Batch 170/501] Loss: 1.5535, Batch Acc: 0.2500\n",
      "[Batch 180/501] Loss: 1.5419, Batch Acc: 0.5000\n",
      "[Batch 190/501] Loss: 1.9348, Batch Acc: 0.1250\n",
      "[Batch 200/501] Loss: 1.5570, Batch Acc: 0.1250\n",
      "[Batch 210/501] Loss: 1.5996, Batch Acc: 0.2500\n",
      "[Batch 220/501] Loss: 1.4583, Batch Acc: 0.6250\n",
      "[Batch 230/501] Loss: 1.5156, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.6214, Batch Acc: 0.3750\n",
      "[Batch 250/501] Loss: 1.8567, Batch Acc: 0.0000\n",
      "[Batch 260/501] Loss: 1.6494, Batch Acc: 0.2500\n",
      "[Batch 270/501] Loss: 1.6486, Batch Acc: 0.3750\n",
      "[Batch 280/501] Loss: 1.5579, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 1.5738, Batch Acc: 0.1250\n",
      "[Batch 300/501] Loss: 1.5706, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 1.6991, Batch Acc: 0.1250\n",
      "[Batch 320/501] Loss: 1.4155, Batch Acc: 0.5000\n",
      "[Batch 330/501] Loss: 1.5641, Batch Acc: 0.2500\n",
      "[Batch 340/501] Loss: 1.6046, Batch Acc: 0.2500\n",
      "[Batch 350/501] Loss: 1.5881, Batch Acc: 0.3750\n",
      "[Batch 360/501] Loss: 1.4414, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 1.7281, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.6108, Batch Acc: 0.1250\n",
      "[Batch 390/501] Loss: 1.4663, Batch Acc: 0.5000\n",
      "[Batch 400/501] Loss: 2.0425, Batch Acc: 0.0000\n",
      "[Batch 410/501] Loss: 1.4355, Batch Acc: 0.3750\n",
      "[Batch 420/501] Loss: 1.0762, Batch Acc: 0.8750\n",
      "[Batch 430/501] Loss: 1.4270, Batch Acc: 0.1250\n",
      "[Batch 440/501] Loss: 1.7938, Batch Acc: 0.0000\n",
      "[Batch 450/501] Loss: 1.6268, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.6364, Batch Acc: 0.1250\n",
      "[Batch 470/501] Loss: 1.6324, Batch Acc: 0.0000\n",
      "[Batch 480/501] Loss: 2.0698, Batch Acc: 0.1250\n",
      "[Batch 490/501] Loss: 1.4641, Batch Acc: 0.3750\n",
      "[Batch 500/501] Loss: 1.6878, Batch Acc: 0.2500\n",
      "Epoch 2 Summary - Loss: 810.7991, Train Accuracy: 0.2515\n",
      "[Batch 10/501] Loss: 1.6761, Batch Acc: 0.2500\n",
      "[Batch 20/501] Loss: 1.4295, Batch Acc: 0.5000\n",
      "[Batch 30/501] Loss: 1.4192, Batch Acc: 0.2500\n",
      "[Batch 40/501] Loss: 1.6347, Batch Acc: 0.1250\n",
      "[Batch 50/501] Loss: 1.5560, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.5689, Batch Acc: 0.3750\n",
      "[Batch 70/501] Loss: 1.6124, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.5852, Batch Acc: 0.3750\n",
      "[Batch 90/501] Loss: 1.4382, Batch Acc: 0.2500\n",
      "[Batch 100/501] Loss: 1.7184, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 1.7353, Batch Acc: 0.0000\n",
      "[Batch 120/501] Loss: 1.6312, Batch Acc: 0.2500\n",
      "[Batch 130/501] Loss: 1.6335, Batch Acc: 0.1250\n",
      "[Batch 140/501] Loss: 1.6294, Batch Acc: 0.2500\n",
      "[Batch 150/501] Loss: 1.6171, Batch Acc: 0.1250\n",
      "[Batch 160/501] Loss: 1.8163, Batch Acc: 0.1250\n",
      "[Batch 170/501] Loss: 1.9781, Batch Acc: 0.1250\n",
      "[Batch 180/501] Loss: 1.6831, Batch Acc: 0.2500\n",
      "[Batch 190/501] Loss: 1.5583, Batch Acc: 0.2500\n",
      "[Batch 200/501] Loss: 1.7503, Batch Acc: 0.1250\n",
      "[Batch 210/501] Loss: 1.6523, Batch Acc: 0.1250\n",
      "[Batch 220/501] Loss: 1.4405, Batch Acc: 0.3750\n",
      "[Batch 230/501] Loss: 1.7497, Batch Acc: 0.1250\n",
      "[Batch 240/501] Loss: 1.6272, Batch Acc: 0.2500\n",
      "[Batch 250/501] Loss: 1.7707, Batch Acc: 0.2500\n",
      "[Batch 260/501] Loss: 1.6630, Batch Acc: 0.2500\n",
      "[Batch 270/501] Loss: 1.6952, Batch Acc: 0.1250\n",
      "[Batch 280/501] Loss: 1.2287, Batch Acc: 0.5000\n",
      "[Batch 290/501] Loss: 1.8959, Batch Acc: 0.1250\n",
      "[Batch 300/501] Loss: 1.7867, Batch Acc: 0.0000\n",
      "[Batch 310/501] Loss: 1.4172, Batch Acc: 0.3750\n",
      "[Batch 320/501] Loss: 1.6060, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 1.6984, Batch Acc: 0.2500\n",
      "[Batch 340/501] Loss: 1.7012, Batch Acc: 0.1250\n",
      "[Batch 350/501] Loss: 1.6491, Batch Acc: 0.1250\n",
      "[Batch 360/501] Loss: 1.3756, Batch Acc: 0.7500\n",
      "[Batch 370/501] Loss: 1.5257, Batch Acc: 0.2500\n",
      "[Batch 380/501] Loss: 1.6386, Batch Acc: 0.1250\n",
      "[Batch 390/501] Loss: 1.6265, Batch Acc: 0.1250\n",
      "[Batch 400/501] Loss: 1.4850, Batch Acc: 0.2500\n",
      "[Batch 410/501] Loss: 1.9064, Batch Acc: 0.0000\n",
      "[Batch 420/501] Loss: 1.8879, Batch Acc: 0.0000\n",
      "[Batch 430/501] Loss: 1.5976, Batch Acc: 0.0000\n",
      "[Batch 440/501] Loss: 1.7848, Batch Acc: 0.0000\n",
      "[Batch 450/501] Loss: 1.4389, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.7184, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.3063, Batch Acc: 0.6250\n",
      "[Batch 480/501] Loss: 1.9406, Batch Acc: 0.0000\n",
      "[Batch 490/501] Loss: 1.6846, Batch Acc: 0.2500\n",
      "[Batch 500/501] Loss: 1.7320, Batch Acc: 0.1250\n",
      "Epoch 3 Summary - Loss: 813.4458, Train Accuracy: 0.2458\n",
      "[Batch 10/501] Loss: 1.5547, Batch Acc: 0.2500\n",
      "[Batch 20/501] Loss: 1.7509, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.6531, Batch Acc: 0.2500\n",
      "[Batch 40/501] Loss: 1.6095, Batch Acc: 0.3750\n",
      "[Batch 50/501] Loss: 1.2805, Batch Acc: 0.5000\n",
      "[Batch 60/501] Loss: 1.7331, Batch Acc: 0.1250\n",
      "[Batch 70/501] Loss: 1.9333, Batch Acc: 0.0000\n",
      "[Batch 80/501] Loss: 1.7479, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.5305, Batch Acc: 0.3750\n",
      "[Batch 100/501] Loss: 1.7138, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 1.6632, Batch Acc: 0.1250\n",
      "[Batch 120/501] Loss: 1.6206, Batch Acc: 0.1250\n",
      "[Batch 130/501] Loss: 1.6649, Batch Acc: 0.3750\n",
      "[Batch 140/501] Loss: 1.6956, Batch Acc: 0.2500\n",
      "[Batch 150/501] Loss: 1.7410, Batch Acc: 0.2500\n",
      "[Batch 160/501] Loss: 1.5630, Batch Acc: 0.2500\n",
      "[Batch 170/501] Loss: 1.7808, Batch Acc: 0.1250\n",
      "[Batch 180/501] Loss: 1.8072, Batch Acc: 0.0000\n",
      "[Batch 190/501] Loss: 1.7380, Batch Acc: 0.1250\n",
      "[Batch 200/501] Loss: 1.4275, Batch Acc: 0.2500\n",
      "[Batch 210/501] Loss: 1.5417, Batch Acc: 0.3750\n",
      "[Batch 220/501] Loss: 1.3707, Batch Acc: 0.7500\n",
      "[Batch 230/501] Loss: 1.3120, Batch Acc: 0.3750\n",
      "[Batch 240/501] Loss: 1.8935, Batch Acc: 0.0000\n",
      "[Batch 250/501] Loss: 1.7517, Batch Acc: 0.2500\n",
      "[Batch 260/501] Loss: 1.6745, Batch Acc: 0.0000\n",
      "[Batch 270/501] Loss: 1.8917, Batch Acc: 0.1250\n",
      "[Batch 280/501] Loss: 1.7540, Batch Acc: 0.0000\n",
      "[Batch 290/501] Loss: 1.5451, Batch Acc: 0.3750\n",
      "[Batch 300/501] Loss: 1.8137, Batch Acc: 0.1250\n",
      "[Batch 310/501] Loss: 1.7070, Batch Acc: 0.3750\n",
      "[Batch 320/501] Loss: 1.6635, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 1.5283, Batch Acc: 0.3750\n",
      "[Batch 340/501] Loss: 1.5826, Batch Acc: 0.2500\n",
      "[Batch 350/501] Loss: 1.6073, Batch Acc: 0.2500\n",
      "[Batch 360/501] Loss: 1.8308, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 1.4205, Batch Acc: 0.2500\n",
      "[Batch 380/501] Loss: 1.6123, Batch Acc: 0.3750\n",
      "[Batch 390/501] Loss: 1.6859, Batch Acc: 0.1250\n",
      "[Batch 400/501] Loss: 1.7268, Batch Acc: 0.1250\n",
      "[Batch 410/501] Loss: 1.5095, Batch Acc: 0.3750\n",
      "[Batch 420/501] Loss: 1.6287, Batch Acc: 0.3750\n",
      "[Batch 430/501] Loss: 1.5203, Batch Acc: 0.3750\n",
      "[Batch 440/501] Loss: 1.5881, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.3537, Batch Acc: 0.3750\n",
      "[Batch 460/501] Loss: 1.8355, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.2839, Batch Acc: 0.7500\n",
      "[Batch 480/501] Loss: 1.6470, Batch Acc: 0.1250\n",
      "[Batch 490/501] Loss: 1.7166, Batch Acc: 0.2500\n",
      "[Batch 500/501] Loss: 1.8088, Batch Acc: 0.1250\n",
      "Epoch 4 Summary - Loss: 810.8577, Train Accuracy: 0.2525\n",
      "[Batch 10/501] Loss: 1.4742, Batch Acc: 0.2500\n",
      "[Batch 20/501] Loss: 1.6825, Batch Acc: 0.1250\n",
      "[Batch 30/501] Loss: 1.7245, Batch Acc: 0.2500\n",
      "[Batch 40/501] Loss: 1.6952, Batch Acc: 0.2500\n",
      "[Batch 50/501] Loss: 1.3452, Batch Acc: 0.3750\n",
      "[Batch 60/501] Loss: 1.3730, Batch Acc: 0.5000\n",
      "[Batch 70/501] Loss: 1.6671, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.5461, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.7151, Batch Acc: 0.1250\n",
      "[Batch 100/501] Loss: 1.3638, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 1.6261, Batch Acc: 0.2500\n",
      "[Batch 120/501] Loss: 1.6541, Batch Acc: 0.2500\n",
      "[Batch 130/501] Loss: 1.8172, Batch Acc: 0.2500\n",
      "[Batch 140/501] Loss: 1.6372, Batch Acc: 0.1250\n",
      "[Batch 150/501] Loss: 1.5125, Batch Acc: 0.2500\n",
      "[Batch 160/501] Loss: 1.4483, Batch Acc: 0.3750\n",
      "[Batch 170/501] Loss: 1.7282, Batch Acc: 0.2500\n",
      "[Batch 180/501] Loss: 1.5293, Batch Acc: 0.2500\n",
      "[Batch 190/501] Loss: 1.6942, Batch Acc: 0.2500\n",
      "[Batch 200/501] Loss: 1.4770, Batch Acc: 0.3750\n",
      "[Batch 210/501] Loss: 1.7805, Batch Acc: 0.1250\n",
      "[Batch 220/501] Loss: 1.3003, Batch Acc: 0.7500\n",
      "[Batch 230/501] Loss: 1.4833, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.7621, Batch Acc: 0.2500\n",
      "[Batch 250/501] Loss: 1.6182, Batch Acc: 0.2500\n",
      "[Batch 260/501] Loss: 1.5211, Batch Acc: 0.3750\n",
      "[Batch 270/501] Loss: 1.3551, Batch Acc: 0.5000\n",
      "[Batch 280/501] Loss: 1.8214, Batch Acc: 0.2500\n",
      "[Batch 290/501] Loss: 1.6458, Batch Acc: 0.2500\n",
      "[Batch 300/501] Loss: 1.3064, Batch Acc: 0.7500\n",
      "[Batch 310/501] Loss: 1.4344, Batch Acc: 0.2500\n",
      "[Batch 320/501] Loss: 1.5781, Batch Acc: 0.1250\n",
      "[Batch 330/501] Loss: 1.4918, Batch Acc: 0.3750\n",
      "[Batch 340/501] Loss: 1.7451, Batch Acc: 0.2500\n",
      "[Batch 350/501] Loss: 1.4522, Batch Acc: 0.3750\n",
      "[Batch 360/501] Loss: 1.8104, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 1.3922, Batch Acc: 0.3750\n",
      "[Batch 380/501] Loss: 1.8326, Batch Acc: 0.1250\n",
      "[Batch 390/501] Loss: 1.3799, Batch Acc: 0.2500\n",
      "[Batch 400/501] Loss: 1.6250, Batch Acc: 0.2500\n",
      "[Batch 410/501] Loss: 1.7248, Batch Acc: 0.1250\n",
      "[Batch 420/501] Loss: 1.4433, Batch Acc: 0.3750\n",
      "[Batch 430/501] Loss: 1.4178, Batch Acc: 0.5000\n",
      "[Batch 440/501] Loss: 1.6734, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.4219, Batch Acc: 0.5000\n",
      "[Batch 460/501] Loss: 1.5657, Batch Acc: 0.3750\n",
      "[Batch 470/501] Loss: 1.5002, Batch Acc: 0.3750\n",
      "[Batch 480/501] Loss: 1.6640, Batch Acc: 0.3750\n",
      "[Batch 490/501] Loss: 1.7381, Batch Acc: 0.2500\n",
      "[Batch 500/501] Loss: 1.7305, Batch Acc: 0.2500\n",
      "Epoch 5 Summary - Loss: 811.8107, Train Accuracy: 0.2607\n",
      "[Batch 10/501] Loss: 1.4255, Batch Acc: 0.2500\n",
      "[Batch 20/501] Loss: 1.8681, Batch Acc: 0.1250\n",
      "[Batch 30/501] Loss: 1.5665, Batch Acc: 0.2500\n",
      "[Batch 40/501] Loss: 1.6564, Batch Acc: 0.1250\n",
      "[Batch 50/501] Loss: 1.6041, Batch Acc: 0.3750\n",
      "[Batch 60/501] Loss: 1.7281, Batch Acc: 0.0000\n",
      "[Batch 70/501] Loss: 1.8628, Batch Acc: 0.1250\n",
      "[Batch 80/501] Loss: 1.6900, Batch Acc: 0.3750\n",
      "[Batch 90/501] Loss: 1.4715, Batch Acc: 0.3750\n",
      "[Batch 100/501] Loss: 1.5420, Batch Acc: 0.6250\n",
      "[Batch 110/501] Loss: 1.3787, Batch Acc: 0.3750\n",
      "[Batch 120/501] Loss: 1.8471, Batch Acc: 0.1250\n",
      "[Batch 130/501] Loss: 1.5501, Batch Acc: 0.2500\n",
      "[Batch 140/501] Loss: 1.2029, Batch Acc: 0.5000\n",
      "[Batch 150/501] Loss: 1.4354, Batch Acc: 0.3750\n",
      "[Batch 160/501] Loss: 1.4195, Batch Acc: 0.3750\n",
      "[Batch 170/501] Loss: 1.7563, Batch Acc: 0.1250\n",
      "[Batch 180/501] Loss: 1.5431, Batch Acc: 0.3750\n",
      "[Batch 190/501] Loss: 1.8192, Batch Acc: 0.0000\n",
      "[Batch 200/501] Loss: 1.5091, Batch Acc: 0.3750\n",
      "[Batch 210/501] Loss: 1.6331, Batch Acc: 0.2500\n",
      "[Batch 220/501] Loss: 1.5353, Batch Acc: 0.3750\n",
      "[Batch 230/501] Loss: 1.8729, Batch Acc: 0.1250\n",
      "[Batch 240/501] Loss: 1.6818, Batch Acc: 0.1250\n",
      "[Batch 250/501] Loss: 1.4987, Batch Acc: 0.5000\n",
      "[Batch 260/501] Loss: 1.8501, Batch Acc: 0.1250\n",
      "[Batch 270/501] Loss: 1.6248, Batch Acc: 0.0000\n",
      "[Batch 280/501] Loss: 1.9052, Batch Acc: 0.0000\n",
      "[Batch 290/501] Loss: 1.5501, Batch Acc: 0.2500\n",
      "[Batch 300/501] Loss: 1.3533, Batch Acc: 0.5000\n",
      "[Batch 310/501] Loss: 1.8104, Batch Acc: 0.2500\n",
      "[Batch 320/501] Loss: 1.6190, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 1.5695, Batch Acc: 0.2500\n",
      "[Batch 340/501] Loss: 1.8587, Batch Acc: 0.2500\n",
      "[Batch 350/501] Loss: 1.6373, Batch Acc: 0.5000\n",
      "[Batch 360/501] Loss: 1.6946, Batch Acc: 0.1250\n",
      "[Batch 370/501] Loss: 1.8323, Batch Acc: 0.0000\n",
      "[Batch 380/501] Loss: 1.5664, Batch Acc: 0.5000\n",
      "[Batch 390/501] Loss: 1.8102, Batch Acc: 0.1250\n",
      "[Batch 400/501] Loss: 1.8013, Batch Acc: 0.1250\n",
      "[Batch 410/501] Loss: 1.3426, Batch Acc: 0.6250\n",
      "[Batch 420/501] Loss: 1.4908, Batch Acc: 0.3750\n",
      "[Batch 430/501] Loss: 1.6943, Batch Acc: 0.0000\n",
      "[Batch 440/501] Loss: 1.6878, Batch Acc: 0.1250\n",
      "[Batch 450/501] Loss: 1.4878, Batch Acc: 0.3750\n",
      "[Batch 460/501] Loss: 1.5475, Batch Acc: 0.1250\n",
      "[Batch 470/501] Loss: 1.8407, Batch Acc: 0.0000\n",
      "[Batch 480/501] Loss: 1.4239, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.4172, Batch Acc: 0.3750\n",
      "[Batch 500/501] Loss: 1.4896, Batch Acc: 0.5000\n",
      "Epoch 6 Summary - Loss: 812.0849, Train Accuracy: 0.2490\n",
      "[Batch 10/501] Loss: 1.6864, Batch Acc: 0.2500\n",
      "[Batch 20/501] Loss: 1.6588, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.7469, Batch Acc: 0.1250\n",
      "[Batch 40/501] Loss: 1.4339, Batch Acc: 0.3750\n",
      "[Batch 50/501] Loss: 1.6033, Batch Acc: 0.1250\n",
      "[Batch 60/501] Loss: 1.6383, Batch Acc: 0.3750\n",
      "[Batch 70/501] Loss: 1.7336, Batch Acc: 0.0000\n",
      "[Batch 80/501] Loss: 1.5843, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.6277, Batch Acc: 0.3750\n",
      "[Batch 100/501] Loss: 1.4338, Batch Acc: 0.3750\n",
      "[Batch 110/501] Loss: 1.6075, Batch Acc: 0.2500\n",
      "[Batch 120/501] Loss: 1.3257, Batch Acc: 0.6250\n",
      "[Batch 130/501] Loss: 1.2848, Batch Acc: 0.5000\n",
      "[Batch 140/501] Loss: 1.3504, Batch Acc: 0.5000\n",
      "[Batch 150/501] Loss: 1.7503, Batch Acc: 0.0000\n",
      "[Batch 160/501] Loss: 1.7492, Batch Acc: 0.1250\n",
      "[Batch 170/501] Loss: 1.6344, Batch Acc: 0.2500\n",
      "[Batch 180/501] Loss: 1.8407, Batch Acc: 0.0000\n",
      "[Batch 190/501] Loss: 1.6993, Batch Acc: 0.1250\n",
      "[Batch 200/501] Loss: 1.5099, Batch Acc: 0.3750\n",
      "[Batch 210/501] Loss: 1.8068, Batch Acc: 0.1250\n",
      "[Batch 220/501] Loss: 1.4179, Batch Acc: 0.3750\n",
      "[Batch 230/501] Loss: 1.6145, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.4114, Batch Acc: 0.5000\n",
      "[Batch 250/501] Loss: 1.5354, Batch Acc: 0.5000\n",
      "[Batch 260/501] Loss: 1.4059, Batch Acc: 0.5000\n",
      "[Batch 270/501] Loss: 1.5663, Batch Acc: 0.1250\n",
      "[Batch 280/501] Loss: 1.2683, Batch Acc: 0.6250\n",
      "[Batch 290/501] Loss: 1.6583, Batch Acc: 0.3750\n",
      "[Batch 300/501] Loss: 1.7835, Batch Acc: 0.1250\n",
      "[Batch 310/501] Loss: 1.5770, Batch Acc: 0.3750\n",
      "[Batch 320/501] Loss: 1.9750, Batch Acc: 0.0000\n",
      "[Batch 330/501] Loss: 1.6473, Batch Acc: 0.2500\n",
      "[Batch 340/501] Loss: 1.4695, Batch Acc: 0.5000\n",
      "[Batch 350/501] Loss: 1.5090, Batch Acc: 0.1250\n",
      "[Batch 360/501] Loss: 1.6192, Batch Acc: 0.3750\n",
      "[Batch 370/501] Loss: 1.6191, Batch Acc: 0.2500\n",
      "[Batch 380/501] Loss: 1.4591, Batch Acc: 0.5000\n",
      "[Batch 390/501] Loss: 1.5328, Batch Acc: 0.5000\n",
      "[Batch 400/501] Loss: 1.6339, Batch Acc: 0.5000\n",
      "[Batch 410/501] Loss: 1.4841, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.6070, Batch Acc: 0.1250\n",
      "[Batch 430/501] Loss: 1.8690, Batch Acc: 0.0000\n",
      "[Batch 440/501] Loss: 1.4273, Batch Acc: 0.3750\n",
      "[Batch 450/501] Loss: 1.8956, Batch Acc: 0.1250\n",
      "[Batch 460/501] Loss: 1.7426, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.7771, Batch Acc: 0.0000\n",
      "[Batch 480/501] Loss: 1.4903, Batch Acc: 0.3750\n",
      "[Batch 490/501] Loss: 1.8703, Batch Acc: 0.0000\n",
      "[Batch 500/501] Loss: 1.3643, Batch Acc: 0.2500\n",
      "Epoch 7 Summary - Loss: 811.7841, Train Accuracy: 0.2540\n",
      "[Batch 10/501] Loss: 1.6735, Batch Acc: 0.1250\n",
      "[Batch 20/501] Loss: 1.7257, Batch Acc: 0.1250\n",
      "[Batch 30/501] Loss: 1.5056, Batch Acc: 0.1250\n",
      "[Batch 40/501] Loss: 1.3283, Batch Acc: 0.5000\n",
      "[Batch 50/501] Loss: 1.4418, Batch Acc: 0.5000\n",
      "[Batch 60/501] Loss: 1.5092, Batch Acc: 0.3750\n",
      "[Batch 70/501] Loss: 1.8510, Batch Acc: 0.0000\n",
      "[Batch 80/501] Loss: 1.5891, Batch Acc: 0.3750\n",
      "[Batch 90/501] Loss: 1.6478, Batch Acc: 0.2500\n",
      "[Batch 100/501] Loss: 1.6163, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 1.5979, Batch Acc: 0.2500\n",
      "[Batch 120/501] Loss: 1.4913, Batch Acc: 0.6250\n",
      "[Batch 130/501] Loss: 1.7777, Batch Acc: 0.0000\n",
      "[Batch 140/501] Loss: 1.3282, Batch Acc: 0.5000\n",
      "[Batch 150/501] Loss: 1.6149, Batch Acc: 0.1250\n",
      "[Batch 160/501] Loss: 1.3872, Batch Acc: 0.2500\n",
      "[Batch 170/501] Loss: 1.5119, Batch Acc: 0.2500\n",
      "[Batch 180/501] Loss: 1.4947, Batch Acc: 0.3750\n",
      "[Batch 190/501] Loss: 1.4879, Batch Acc: 0.5000\n",
      "[Batch 200/501] Loss: 1.9063, Batch Acc: 0.1250\n",
      "[Batch 210/501] Loss: 1.5987, Batch Acc: 0.2500\n",
      "[Batch 220/501] Loss: 1.4961, Batch Acc: 0.3750\n",
      "[Batch 230/501] Loss: 1.6830, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.4416, Batch Acc: 0.5000\n",
      "[Batch 250/501] Loss: 1.9294, Batch Acc: 0.1250\n",
      "[Batch 260/501] Loss: 1.8334, Batch Acc: 0.2500\n",
      "[Batch 270/501] Loss: 1.5550, Batch Acc: 0.1250\n",
      "[Batch 280/501] Loss: 1.5176, Batch Acc: 0.3750\n",
      "[Batch 290/501] Loss: 1.6836, Batch Acc: 0.1250\n",
      "[Batch 300/501] Loss: 1.2719, Batch Acc: 0.5000\n",
      "[Batch 310/501] Loss: 1.7368, Batch Acc: 0.2500\n",
      "[Batch 320/501] Loss: 1.6199, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 1.7347, Batch Acc: 0.1250\n",
      "[Batch 340/501] Loss: 1.9236, Batch Acc: 0.1250\n",
      "[Batch 350/501] Loss: 1.7571, Batch Acc: 0.0000\n",
      "[Batch 360/501] Loss: 1.5649, Batch Acc: 0.3750\n",
      "[Batch 370/501] Loss: 1.5880, Batch Acc: 0.2500\n",
      "[Batch 380/501] Loss: 1.7354, Batch Acc: 0.2500\n",
      "[Batch 390/501] Loss: 1.5681, Batch Acc: 0.2500\n",
      "[Batch 400/501] Loss: 1.4901, Batch Acc: 0.3750\n",
      "[Batch 410/501] Loss: 1.5263, Batch Acc: 0.3750\n",
      "[Batch 420/501] Loss: 1.5572, Batch Acc: 0.3750\n",
      "[Batch 430/501] Loss: 1.5223, Batch Acc: 0.2500\n",
      "[Batch 440/501] Loss: 1.5460, Batch Acc: 0.1250\n",
      "[Batch 450/501] Loss: 1.6863, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.9032, Batch Acc: 0.0000\n",
      "[Batch 470/501] Loss: 1.4367, Batch Acc: 0.3750\n",
      "[Batch 480/501] Loss: 1.4124, Batch Acc: 0.5000\n",
      "[Batch 490/501] Loss: 1.3719, Batch Acc: 0.2500\n",
      "[Batch 500/501] Loss: 1.5550, Batch Acc: 0.1250\n",
      "Epoch 8 Summary - Loss: 808.9483, Train Accuracy: 0.2597\n",
      "[Batch 10/501] Loss: 1.8903, Batch Acc: 0.0000\n",
      "[Batch 20/501] Loss: 1.3899, Batch Acc: 0.3750\n",
      "[Batch 30/501] Loss: 1.9049, Batch Acc: 0.0000\n",
      "[Batch 40/501] Loss: 1.5504, Batch Acc: 0.3750\n",
      "[Batch 50/501] Loss: 1.6740, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.9188, Batch Acc: 0.0000\n",
      "[Batch 70/501] Loss: 1.9829, Batch Acc: 0.1250\n",
      "[Batch 80/501] Loss: 1.6369, Batch Acc: 0.1250\n",
      "[Batch 90/501] Loss: 1.7516, Batch Acc: 0.1250\n",
      "[Batch 100/501] Loss: 1.6807, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 1.7691, Batch Acc: 0.0000\n",
      "[Batch 120/501] Loss: 1.6911, Batch Acc: 0.3750\n",
      "[Batch 130/501] Loss: 1.8557, Batch Acc: 0.1250\n",
      "[Batch 140/501] Loss: 1.6591, Batch Acc: 0.0000\n",
      "[Batch 150/501] Loss: 1.5554, Batch Acc: 0.3750\n",
      "[Batch 160/501] Loss: 1.6328, Batch Acc: 0.2500\n",
      "[Batch 170/501] Loss: 1.3973, Batch Acc: 0.6250\n",
      "[Batch 180/501] Loss: 1.3625, Batch Acc: 0.5000\n",
      "[Batch 190/501] Loss: 1.7589, Batch Acc: 0.1250\n",
      "[Batch 200/501] Loss: 1.6974, Batch Acc: 0.1250\n",
      "[Batch 210/501] Loss: 1.5713, Batch Acc: 0.2500\n",
      "[Batch 220/501] Loss: 1.6199, Batch Acc: 0.2500\n",
      "[Batch 230/501] Loss: 1.7085, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.6162, Batch Acc: 0.2500\n",
      "[Batch 250/501] Loss: 1.7642, Batch Acc: 0.1250\n",
      "[Batch 260/501] Loss: 1.7089, Batch Acc: 0.2500\n",
      "[Batch 270/501] Loss: 1.9164, Batch Acc: 0.2500\n",
      "[Batch 280/501] Loss: 1.8206, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 1.6376, Batch Acc: 0.1250\n",
      "[Batch 300/501] Loss: 1.8700, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 1.4495, Batch Acc: 0.3750\n",
      "[Batch 320/501] Loss: 1.6941, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 1.9725, Batch Acc: 0.1250\n",
      "[Batch 340/501] Loss: 1.5247, Batch Acc: 0.5000\n",
      "[Batch 350/501] Loss: 1.5038, Batch Acc: 0.2500\n",
      "[Batch 360/501] Loss: 1.5903, Batch Acc: 0.1250\n",
      "[Batch 370/501] Loss: 1.4437, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.7008, Batch Acc: 0.2500\n",
      "[Batch 390/501] Loss: 1.6267, Batch Acc: 0.1250\n",
      "[Batch 400/501] Loss: 1.2961, Batch Acc: 0.7500\n",
      "[Batch 410/501] Loss: 1.8229, Batch Acc: 0.1250\n",
      "[Batch 420/501] Loss: 1.7246, Batch Acc: 0.3750\n",
      "[Batch 430/501] Loss: 1.4607, Batch Acc: 0.5000\n",
      "[Batch 440/501] Loss: 1.4738, Batch Acc: 0.3750\n",
      "[Batch 450/501] Loss: 1.8229, Batch Acc: 0.0000\n",
      "[Batch 460/501] Loss: 1.6134, Batch Acc: 0.3750\n",
      "[Batch 470/501] Loss: 1.5501, Batch Acc: 0.1250\n",
      "[Batch 480/501] Loss: 1.8101, Batch Acc: 0.0000\n",
      "[Batch 490/501] Loss: 1.6853, Batch Acc: 0.0000\n",
      "[Batch 500/501] Loss: 1.5998, Batch Acc: 0.5000\n",
      "Epoch 9 Summary - Loss: 815.5311, Train Accuracy: 0.2577\n",
      "[Batch 10/501] Loss: 1.8296, Batch Acc: 0.1250\n",
      "[Batch 20/501] Loss: 1.9545, Batch Acc: 0.0000\n",
      "[Batch 30/501] Loss: 1.3946, Batch Acc: 0.3750\n",
      "[Batch 40/501] Loss: 1.5577, Batch Acc: 0.2500\n",
      "[Batch 50/501] Loss: 1.8277, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.6479, Batch Acc: 0.3750\n",
      "[Batch 70/501] Loss: 1.6627, Batch Acc: 0.0000\n",
      "[Batch 80/501] Loss: 1.5202, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.6237, Batch Acc: 0.2500\n",
      "[Batch 100/501] Loss: 1.6733, Batch Acc: 0.1250\n",
      "[Batch 110/501] Loss: 1.6325, Batch Acc: 0.1250\n",
      "[Batch 120/501] Loss: 1.8478, Batch Acc: 0.1250\n",
      "[Batch 130/501] Loss: 1.7331, Batch Acc: 0.1250\n",
      "[Batch 140/501] Loss: 1.3538, Batch Acc: 0.3750\n",
      "[Batch 150/501] Loss: 1.5145, Batch Acc: 0.2500\n",
      "[Batch 160/501] Loss: 1.9002, Batch Acc: 0.1250\n",
      "[Batch 170/501] Loss: 1.3924, Batch Acc: 0.3750\n",
      "[Batch 180/501] Loss: 1.4217, Batch Acc: 0.3750\n",
      "[Batch 190/501] Loss: 1.6908, Batch Acc: 0.2500\n",
      "[Batch 200/501] Loss: 1.4302, Batch Acc: 0.2500\n",
      "[Batch 210/501] Loss: 1.5940, Batch Acc: 0.3750\n",
      "[Batch 220/501] Loss: 1.6305, Batch Acc: 0.3750\n",
      "[Batch 230/501] Loss: 1.8360, Batch Acc: 0.0000\n",
      "[Batch 240/501] Loss: 1.6872, Batch Acc: 0.1250\n",
      "[Batch 250/501] Loss: 1.8545, Batch Acc: 0.1250\n",
      "[Batch 260/501] Loss: 1.5816, Batch Acc: 0.2500\n",
      "[Batch 270/501] Loss: 1.4223, Batch Acc: 0.5000\n",
      "[Batch 280/501] Loss: 1.6701, Batch Acc: 0.3750\n",
      "[Batch 290/501] Loss: 1.6931, Batch Acc: 0.0000\n",
      "[Batch 300/501] Loss: 1.4787, Batch Acc: 0.3750\n",
      "[Batch 310/501] Loss: 1.7433, Batch Acc: 0.1250\n",
      "[Batch 320/501] Loss: 1.5799, Batch Acc: 0.1250\n",
      "[Batch 330/501] Loss: 1.4622, Batch Acc: 0.5000\n",
      "[Batch 340/501] Loss: 1.7016, Batch Acc: 0.3750\n",
      "[Batch 350/501] Loss: 1.5486, Batch Acc: 0.2500\n",
      "[Batch 360/501] Loss: 1.5334, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 1.5495, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.4275, Batch Acc: 0.2500\n",
      "[Batch 390/501] Loss: 1.3501, Batch Acc: 0.5000\n",
      "[Batch 400/501] Loss: 1.5363, Batch Acc: 0.2500\n",
      "[Batch 410/501] Loss: 1.7030, Batch Acc: 0.1250\n",
      "[Batch 420/501] Loss: 1.5279, Batch Acc: 0.3750\n",
      "[Batch 430/501] Loss: 1.6991, Batch Acc: 0.2500\n",
      "[Batch 440/501] Loss: 1.5491, Batch Acc: 0.3750\n",
      "[Batch 450/501] Loss: 1.6084, Batch Acc: 0.1250\n",
      "[Batch 460/501] Loss: 1.5957, Batch Acc: 0.1250\n",
      "[Batch 470/501] Loss: 1.5388, Batch Acc: 0.1250\n",
      "[Batch 480/501] Loss: 1.6786, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.6326, Batch Acc: 0.2500\n",
      "[Batch 500/501] Loss: 1.5002, Batch Acc: 0.2500\n",
      "Epoch 10 Summary - Loss: 813.6724, Train Accuracy: 0.2413\n",
      "Test Accuracy: 0.2671\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_accuracy</td><td>▅▅▃▅█▄▆█▇▁</td></tr><tr><td>train_loss</td><td>▆▃▆▃▄▄▄▁█▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>test_accuracy</td><td>0.26706</td></tr><tr><td>test_loss</td><td>203.03015</td></tr><tr><td>train_accuracy</td><td>0.24127</td></tr><tr><td>train_loss</td><td>813.67239</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sweet-sweep-12</strong> at: <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/087nxvhe' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/087nxvhe</a><br> View project at: <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250615_142828-087nxvhe/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0d6fy896 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_nodes: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.6s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ajit.kumar4@happiestminds.com/Documents/Drackula/ComputerVisionFromScratch/wandb/run-20250615_143137-0d6fy896</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/0d6fy896' target=\"_blank\">fearless-sweep-13</a></strong> to <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/0d6fy896' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/0d6fy896</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 10/501] Loss: 1.4761, Batch Acc: 0.1250\n",
      "[Batch 20/501] Loss: 1.5668, Batch Acc: 0.5000\n",
      "[Batch 30/501] Loss: 1.6366, Batch Acc: 0.1250\n",
      "[Batch 40/501] Loss: 1.3892, Batch Acc: 0.3750\n",
      "[Batch 50/501] Loss: 1.6609, Batch Acc: 0.1250\n",
      "[Batch 60/501] Loss: 1.5015, Batch Acc: 0.5000\n",
      "[Batch 70/501] Loss: 1.6767, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.5135, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.5391, Batch Acc: 0.1250\n",
      "[Batch 100/501] Loss: 1.4678, Batch Acc: 0.3750\n",
      "[Batch 110/501] Loss: 1.7579, Batch Acc: 0.3750\n",
      "[Batch 120/501] Loss: 1.6019, Batch Acc: 0.5000\n",
      "[Batch 130/501] Loss: 1.3223, Batch Acc: 0.2500\n",
      "[Batch 140/501] Loss: 1.7060, Batch Acc: 0.1250\n",
      "[Batch 150/501] Loss: 1.3347, Batch Acc: 0.5000\n",
      "[Batch 160/501] Loss: 1.5209, Batch Acc: 0.3750\n",
      "[Batch 170/501] Loss: 1.4352, Batch Acc: 0.1250\n",
      "[Batch 180/501] Loss: 1.5613, Batch Acc: 0.3750\n",
      "[Batch 190/501] Loss: 1.3080, Batch Acc: 0.3750\n",
      "[Batch 200/501] Loss: 1.6482, Batch Acc: 0.5000\n",
      "[Batch 210/501] Loss: 1.9307, Batch Acc: 0.2500\n",
      "[Batch 220/501] Loss: 1.5796, Batch Acc: 0.1250\n",
      "[Batch 230/501] Loss: 1.4417, Batch Acc: 0.3750\n",
      "[Batch 240/501] Loss: 1.4061, Batch Acc: 0.3750\n",
      "[Batch 250/501] Loss: 1.7932, Batch Acc: 0.0000\n",
      "[Batch 260/501] Loss: 1.4954, Batch Acc: 0.2500\n",
      "[Batch 270/501] Loss: 1.6118, Batch Acc: 0.3750\n",
      "[Batch 280/501] Loss: 1.7460, Batch Acc: 0.0000\n",
      "[Batch 290/501] Loss: 1.9242, Batch Acc: 0.2500\n",
      "[Batch 300/501] Loss: 1.7778, Batch Acc: 0.3750\n",
      "[Batch 310/501] Loss: 1.5387, Batch Acc: 0.3750\n",
      "[Batch 320/501] Loss: 1.8334, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 1.7288, Batch Acc: 0.2500\n",
      "[Batch 340/501] Loss: 1.5010, Batch Acc: 0.3750\n",
      "[Batch 350/501] Loss: 1.5826, Batch Acc: 0.3750\n",
      "[Batch 360/501] Loss: 1.6610, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 1.5864, Batch Acc: 0.2500\n",
      "[Batch 380/501] Loss: 1.8306, Batch Acc: 0.1250\n",
      "[Batch 390/501] Loss: 1.3576, Batch Acc: 0.1250\n",
      "[Batch 400/501] Loss: 1.4839, Batch Acc: 0.1250\n",
      "[Batch 410/501] Loss: 1.3409, Batch Acc: 0.5000\n",
      "[Batch 420/501] Loss: 1.8593, Batch Acc: 0.5000\n",
      "[Batch 430/501] Loss: 1.6009, Batch Acc: 0.2500\n",
      "[Batch 440/501] Loss: 1.7454, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.7163, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.4274, Batch Acc: 0.3750\n",
      "[Batch 470/501] Loss: 1.7097, Batch Acc: 0.1250\n",
      "[Batch 480/501] Loss: 1.5762, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.5226, Batch Acc: 0.2500\n",
      "[Batch 500/501] Loss: 1.4596, Batch Acc: 0.1250\n",
      "Epoch 1 Summary - Loss: 821.5321, Train Accuracy: 0.2532\n",
      "[Batch 10/501] Loss: 1.7650, Batch Acc: 0.1250\n",
      "[Batch 20/501] Loss: 1.7206, Batch Acc: 0.0000\n",
      "[Batch 30/501] Loss: 1.6853, Batch Acc: 0.1250\n",
      "[Batch 40/501] Loss: 1.7919, Batch Acc: 0.2500\n",
      "[Batch 50/501] Loss: 1.6260, Batch Acc: 0.1250\n",
      "[Batch 60/501] Loss: 1.5096, Batch Acc: 0.2500\n",
      "[Batch 70/501] Loss: 1.9033, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.4182, Batch Acc: 0.5000\n",
      "[Batch 90/501] Loss: 1.4206, Batch Acc: 0.3750\n",
      "[Batch 100/501] Loss: 1.8675, Batch Acc: 0.1250\n",
      "[Batch 110/501] Loss: 1.8447, Batch Acc: 0.1250\n",
      "[Batch 120/501] Loss: 1.5968, Batch Acc: 0.1250\n",
      "[Batch 130/501] Loss: 1.7825, Batch Acc: 0.3750\n",
      "[Batch 140/501] Loss: 1.4459, Batch Acc: 0.5000\n",
      "[Batch 150/501] Loss: 2.0430, Batch Acc: 0.2500\n",
      "[Batch 160/501] Loss: 1.5184, Batch Acc: 0.2500\n",
      "[Batch 170/501] Loss: 1.9450, Batch Acc: 0.0000\n",
      "[Batch 180/501] Loss: 1.6401, Batch Acc: 0.2500\n",
      "[Batch 190/501] Loss: 1.2964, Batch Acc: 0.2500\n",
      "[Batch 200/501] Loss: 1.6749, Batch Acc: 0.2500\n",
      "[Batch 210/501] Loss: 2.2701, Batch Acc: 0.0000\n",
      "[Batch 220/501] Loss: 1.7033, Batch Acc: 0.2500\n",
      "[Batch 230/501] Loss: 1.6222, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.7906, Batch Acc: 0.1250\n",
      "[Batch 250/501] Loss: 1.4821, Batch Acc: 0.2500\n",
      "[Batch 260/501] Loss: 1.7414, Batch Acc: 0.0000\n",
      "[Batch 270/501] Loss: 1.3683, Batch Acc: 0.5000\n",
      "[Batch 280/501] Loss: 1.5382, Batch Acc: 0.3750\n",
      "[Batch 290/501] Loss: 1.5707, Batch Acc: 0.2500\n",
      "[Batch 300/501] Loss: 1.5538, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 1.9130, Batch Acc: 0.0000\n",
      "[Batch 320/501] Loss: 1.4129, Batch Acc: 0.5000\n",
      "[Batch 330/501] Loss: 1.4164, Batch Acc: 0.2500\n",
      "[Batch 340/501] Loss: 1.6310, Batch Acc: 0.2500\n",
      "[Batch 350/501] Loss: 2.1040, Batch Acc: 0.1250\n",
      "[Batch 360/501] Loss: 1.4755, Batch Acc: 0.5000\n",
      "[Batch 370/501] Loss: 1.8870, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.8857, Batch Acc: 0.3750\n",
      "[Batch 390/501] Loss: 1.6841, Batch Acc: 0.2500\n",
      "[Batch 400/501] Loss: 1.8631, Batch Acc: 0.2500\n",
      "[Batch 410/501] Loss: 1.5326, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.6630, Batch Acc: 0.3750\n",
      "[Batch 430/501] Loss: 1.8665, Batch Acc: 0.0000\n",
      "[Batch 440/501] Loss: 1.4790, Batch Acc: 0.3750\n",
      "[Batch 450/501] Loss: 1.6883, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.5428, Batch Acc: 0.1250\n",
      "[Batch 470/501] Loss: 1.5469, Batch Acc: 0.3750\n",
      "[Batch 480/501] Loss: 1.3690, Batch Acc: 0.3750\n",
      "[Batch 490/501] Loss: 1.2478, Batch Acc: 0.6250\n",
      "[Batch 500/501] Loss: 1.3111, Batch Acc: 0.3750\n",
      "Epoch 2 Summary - Loss: 821.2764, Train Accuracy: 0.2575\n",
      "[Batch 10/501] Loss: 1.2869, Batch Acc: 0.3750\n",
      "[Batch 20/501] Loss: 1.8103, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.4906, Batch Acc: 0.1250\n",
      "[Batch 40/501] Loss: 1.8672, Batch Acc: 0.1250\n",
      "[Batch 50/501] Loss: 1.7907, Batch Acc: 0.1250\n",
      "[Batch 60/501] Loss: 1.7294, Batch Acc: 0.1250\n",
      "[Batch 70/501] Loss: 1.7644, Batch Acc: 0.1250\n",
      "[Batch 80/501] Loss: 1.3915, Batch Acc: 0.5000\n",
      "[Batch 90/501] Loss: 1.5160, Batch Acc: 0.3750\n",
      "[Batch 100/501] Loss: 1.5770, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 1.5868, Batch Acc: 0.2500\n",
      "[Batch 120/501] Loss: 1.7538, Batch Acc: 0.2500\n",
      "[Batch 130/501] Loss: 1.5966, Batch Acc: 0.1250\n",
      "[Batch 140/501] Loss: 1.5162, Batch Acc: 0.1250\n",
      "[Batch 150/501] Loss: 1.7065, Batch Acc: 0.1250\n",
      "[Batch 160/501] Loss: 1.5592, Batch Acc: 0.2500\n",
      "[Batch 170/501] Loss: 1.5916, Batch Acc: 0.2500\n",
      "[Batch 180/501] Loss: 1.4504, Batch Acc: 0.2500\n",
      "[Batch 190/501] Loss: 1.7302, Batch Acc: 0.2500\n",
      "[Batch 200/501] Loss: 1.7675, Batch Acc: 0.2500\n",
      "[Batch 210/501] Loss: 1.7638, Batch Acc: 0.1250\n",
      "[Batch 220/501] Loss: 1.6837, Batch Acc: 0.2500\n",
      "[Batch 230/501] Loss: 1.5349, Batch Acc: 0.3750\n",
      "[Batch 240/501] Loss: 1.7503, Batch Acc: 0.5000\n",
      "[Batch 250/501] Loss: 1.5419, Batch Acc: 0.2500\n",
      "[Batch 260/501] Loss: 1.5301, Batch Acc: 0.1250\n",
      "[Batch 270/501] Loss: 1.5802, Batch Acc: 0.1250\n",
      "[Batch 280/501] Loss: 1.4985, Batch Acc: 0.3750\n",
      "[Batch 290/501] Loss: 1.7925, Batch Acc: 0.1250\n",
      "[Batch 300/501] Loss: 1.3332, Batch Acc: 0.6250\n",
      "[Batch 310/501] Loss: 1.7171, Batch Acc: 0.5000\n",
      "[Batch 320/501] Loss: 1.5359, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 1.8444, Batch Acc: 0.2500\n",
      "[Batch 340/501] Loss: 1.5787, Batch Acc: 0.2500\n",
      "[Batch 350/501] Loss: 1.6974, Batch Acc: 0.3750\n",
      "[Batch 360/501] Loss: 1.5042, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 1.6292, Batch Acc: 0.2500\n",
      "[Batch 380/501] Loss: 1.4867, Batch Acc: 0.3750\n",
      "[Batch 390/501] Loss: 1.6906, Batch Acc: 0.1250\n",
      "[Batch 400/501] Loss: 1.7749, Batch Acc: 0.2500\n",
      "[Batch 410/501] Loss: 1.6085, Batch Acc: 0.5000\n",
      "[Batch 420/501] Loss: 1.3931, Batch Acc: 0.2500\n",
      "[Batch 430/501] Loss: 1.9427, Batch Acc: 0.1250\n",
      "[Batch 440/501] Loss: 1.9481, Batch Acc: 0.0000\n",
      "[Batch 450/501] Loss: 1.3100, Batch Acc: 0.5000\n",
      "[Batch 460/501] Loss: 1.8711, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.4038, Batch Acc: 0.3750\n",
      "[Batch 480/501] Loss: 1.3871, Batch Acc: 0.6250\n",
      "[Batch 490/501] Loss: 1.9166, Batch Acc: 0.1250\n",
      "[Batch 500/501] Loss: 1.6149, Batch Acc: 0.1250\n",
      "Epoch 3 Summary - Loss: 818.9590, Train Accuracy: 0.2560\n",
      "[Batch 10/501] Loss: 1.3781, Batch Acc: 0.3750\n",
      "[Batch 20/501] Loss: 1.5874, Batch Acc: 0.3750\n",
      "[Batch 30/501] Loss: 1.6876, Batch Acc: 0.3750\n",
      "[Batch 40/501] Loss: 1.8506, Batch Acc: 0.1250\n",
      "[Batch 50/501] Loss: 1.6264, Batch Acc: 0.1250\n",
      "[Batch 60/501] Loss: 1.5098, Batch Acc: 0.3750\n",
      "[Batch 70/501] Loss: 1.5111, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.7438, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.4721, Batch Acc: 0.2500\n",
      "[Batch 100/501] Loss: 1.3340, Batch Acc: 0.3750\n",
      "[Batch 110/501] Loss: 1.6307, Batch Acc: 0.2500\n",
      "[Batch 120/501] Loss: 1.6020, Batch Acc: 0.5000\n",
      "[Batch 130/501] Loss: 1.6007, Batch Acc: 0.1250\n",
      "[Batch 140/501] Loss: 1.6132, Batch Acc: 0.1250\n",
      "[Batch 150/501] Loss: 1.7762, Batch Acc: 0.2500\n",
      "[Batch 160/501] Loss: 1.6313, Batch Acc: 0.2500\n",
      "[Batch 170/501] Loss: 1.8140, Batch Acc: 0.3750\n",
      "[Batch 180/501] Loss: 1.3503, Batch Acc: 0.3750\n",
      "[Batch 190/501] Loss: 1.7077, Batch Acc: 0.3750\n",
      "[Batch 200/501] Loss: 1.4274, Batch Acc: 0.6250\n",
      "[Batch 210/501] Loss: 1.6494, Batch Acc: 0.2500\n",
      "[Batch 220/501] Loss: 1.7193, Batch Acc: 0.1250\n",
      "[Batch 230/501] Loss: 1.8420, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.3381, Batch Acc: 0.5000\n",
      "[Batch 250/501] Loss: 1.7688, Batch Acc: 0.3750\n",
      "[Batch 260/501] Loss: 1.7566, Batch Acc: 0.1250\n",
      "[Batch 270/501] Loss: 1.8784, Batch Acc: 0.1250\n",
      "[Batch 280/501] Loss: 1.6805, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 1.6436, Batch Acc: 0.2500\n",
      "[Batch 300/501] Loss: 1.5704, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 1.6071, Batch Acc: 0.1250\n",
      "[Batch 320/501] Loss: 1.6633, Batch Acc: 0.1250\n",
      "[Batch 330/501] Loss: 1.4424, Batch Acc: 0.0000\n",
      "[Batch 340/501] Loss: 1.5166, Batch Acc: 0.2500\n",
      "[Batch 350/501] Loss: 1.5658, Batch Acc: 0.2500\n",
      "[Batch 360/501] Loss: 1.7771, Batch Acc: 0.0000\n",
      "[Batch 370/501] Loss: 2.0912, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.5681, Batch Acc: 0.5000\n",
      "[Batch 390/501] Loss: 1.8527, Batch Acc: 0.3750\n",
      "[Batch 400/501] Loss: 1.6369, Batch Acc: 0.1250\n",
      "[Batch 410/501] Loss: 2.1654, Batch Acc: 0.1250\n",
      "[Batch 420/501] Loss: 1.5002, Batch Acc: 0.3750\n",
      "[Batch 430/501] Loss: 1.7530, Batch Acc: 0.1250\n",
      "[Batch 440/501] Loss: 1.3071, Batch Acc: 0.6250\n",
      "[Batch 450/501] Loss: 1.3528, Batch Acc: 0.3750\n",
      "[Batch 460/501] Loss: 1.7947, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.3237, Batch Acc: 0.3750\n",
      "[Batch 480/501] Loss: 1.8578, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.7756, Batch Acc: 0.1250\n",
      "[Batch 500/501] Loss: 1.3468, Batch Acc: 0.3750\n",
      "Epoch 4 Summary - Loss: 818.5292, Train Accuracy: 0.2535\n",
      "[Batch 10/501] Loss: 1.6108, Batch Acc: 0.3750\n",
      "[Batch 20/501] Loss: 1.5691, Batch Acc: 0.3750\n",
      "[Batch 30/501] Loss: 1.6448, Batch Acc: 0.0000\n",
      "[Batch 40/501] Loss: 1.7068, Batch Acc: 0.0000\n",
      "[Batch 50/501] Loss: 1.5863, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.6051, Batch Acc: 0.2500\n",
      "[Batch 70/501] Loss: 1.7112, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.4244, Batch Acc: 0.1250\n",
      "[Batch 90/501] Loss: 1.6767, Batch Acc: 0.3750\n",
      "[Batch 100/501] Loss: 1.2963, Batch Acc: 0.3750\n",
      "[Batch 110/501] Loss: 1.5212, Batch Acc: 0.3750\n",
      "[Batch 120/501] Loss: 1.7723, Batch Acc: 0.2500\n",
      "[Batch 130/501] Loss: 1.4362, Batch Acc: 0.2500\n",
      "[Batch 140/501] Loss: 1.5776, Batch Acc: 0.2500\n",
      "[Batch 150/501] Loss: 1.8083, Batch Acc: 0.1250\n",
      "[Batch 160/501] Loss: 1.4055, Batch Acc: 0.5000\n",
      "[Batch 170/501] Loss: 1.6436, Batch Acc: 0.2500\n",
      "[Batch 180/501] Loss: 2.1852, Batch Acc: 0.0000\n",
      "[Batch 190/501] Loss: 1.6035, Batch Acc: 0.2500\n",
      "[Batch 200/501] Loss: 1.4259, Batch Acc: 0.5000\n",
      "[Batch 210/501] Loss: 1.6738, Batch Acc: 0.3750\n",
      "[Batch 220/501] Loss: 1.5755, Batch Acc: 0.1250\n",
      "[Batch 230/501] Loss: 1.3336, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.5819, Batch Acc: 0.1250\n",
      "[Batch 250/501] Loss: 1.4430, Batch Acc: 0.3750\n",
      "[Batch 260/501] Loss: 1.6558, Batch Acc: 0.2500\n",
      "[Batch 270/501] Loss: 1.6089, Batch Acc: 0.5000\n",
      "[Batch 280/501] Loss: 1.6201, Batch Acc: 0.3750\n",
      "[Batch 290/501] Loss: 1.4172, Batch Acc: 0.3750\n",
      "[Batch 300/501] Loss: 1.4846, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 1.8905, Batch Acc: 0.1250\n",
      "[Batch 320/501] Loss: 2.1495, Batch Acc: 0.1250\n",
      "[Batch 330/501] Loss: 1.4415, Batch Acc: 0.3750\n",
      "[Batch 340/501] Loss: 1.6897, Batch Acc: 0.1250\n",
      "[Batch 350/501] Loss: 1.6512, Batch Acc: 0.1250\n",
      "[Batch 360/501] Loss: 2.0597, Batch Acc: 0.0000\n",
      "[Batch 370/501] Loss: 1.3298, Batch Acc: 0.5000\n",
      "[Batch 380/501] Loss: 1.6549, Batch Acc: 0.0000\n",
      "[Batch 390/501] Loss: 1.3990, Batch Acc: 0.3750\n",
      "[Batch 400/501] Loss: 1.5596, Batch Acc: 0.1250\n",
      "[Batch 410/501] Loss: 1.6856, Batch Acc: 0.5000\n",
      "[Batch 420/501] Loss: 1.5839, Batch Acc: 0.3750\n",
      "[Batch 430/501] Loss: 1.6914, Batch Acc: 0.1250\n",
      "[Batch 440/501] Loss: 1.9111, Batch Acc: 0.1250\n",
      "[Batch 450/501] Loss: 1.5139, Batch Acc: 0.5000\n",
      "[Batch 460/501] Loss: 1.3621, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.6717, Batch Acc: 0.2500\n",
      "[Batch 480/501] Loss: 1.7127, Batch Acc: 0.1250\n",
      "[Batch 490/501] Loss: 1.9721, Batch Acc: 0.0000\n",
      "[Batch 500/501] Loss: 1.6446, Batch Acc: 0.1250\n",
      "Epoch 5 Summary - Loss: 820.0812, Train Accuracy: 0.2582\n",
      "[Batch 10/501] Loss: 1.4068, Batch Acc: 0.3750\n",
      "[Batch 20/501] Loss: 1.5242, Batch Acc: 0.3750\n",
      "[Batch 30/501] Loss: 1.7870, Batch Acc: 0.1250\n",
      "[Batch 40/501] Loss: 1.6222, Batch Acc: 0.0000\n",
      "[Batch 50/501] Loss: 1.4707, Batch Acc: 0.6250\n",
      "[Batch 60/501] Loss: 1.8258, Batch Acc: 0.3750\n",
      "[Batch 70/501] Loss: 1.7201, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.6764, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.3998, Batch Acc: 0.2500\n",
      "[Batch 100/501] Loss: 1.5741, Batch Acc: 0.5000\n",
      "[Batch 110/501] Loss: 1.6656, Batch Acc: 0.3750\n",
      "[Batch 120/501] Loss: 1.3932, Batch Acc: 0.3750\n",
      "[Batch 130/501] Loss: 1.3516, Batch Acc: 0.3750\n",
      "[Batch 140/501] Loss: 1.7486, Batch Acc: 0.2500\n",
      "[Batch 150/501] Loss: 1.7363, Batch Acc: 0.0000\n",
      "[Batch 160/501] Loss: 1.4381, Batch Acc: 0.2500\n",
      "[Batch 170/501] Loss: 1.6030, Batch Acc: 0.5000\n",
      "[Batch 180/501] Loss: 1.4724, Batch Acc: 0.1250\n",
      "[Batch 190/501] Loss: 1.4935, Batch Acc: 0.2500\n",
      "[Batch 200/501] Loss: 1.6548, Batch Acc: 0.3750\n",
      "[Batch 210/501] Loss: 1.6881, Batch Acc: 0.1250\n",
      "[Batch 220/501] Loss: 1.4023, Batch Acc: 0.3750\n",
      "[Batch 230/501] Loss: 1.3874, Batch Acc: 0.3750\n",
      "[Batch 240/501] Loss: 1.5482, Batch Acc: 0.2500\n",
      "[Batch 250/501] Loss: 1.9027, Batch Acc: 0.0000\n",
      "[Batch 260/501] Loss: 1.8772, Batch Acc: 0.1250\n",
      "[Batch 270/501] Loss: 1.4834, Batch Acc: 0.2500\n",
      "[Batch 280/501] Loss: 1.6255, Batch Acc: 0.2500\n",
      "[Batch 290/501] Loss: 1.7153, Batch Acc: 0.2500\n",
      "[Batch 300/501] Loss: 1.7431, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 1.5631, Batch Acc: 0.3750\n",
      "[Batch 320/501] Loss: 1.8744, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 1.5956, Batch Acc: 0.2500\n",
      "[Batch 340/501] Loss: 1.7049, Batch Acc: 0.3750\n",
      "[Batch 350/501] Loss: 1.4282, Batch Acc: 0.5000\n",
      "[Batch 360/501] Loss: 1.5973, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 1.4945, Batch Acc: 0.2500\n",
      "[Batch 380/501] Loss: 1.6437, Batch Acc: 0.1250\n",
      "[Batch 390/501] Loss: 1.7337, Batch Acc: 0.1250\n",
      "[Batch 400/501] Loss: 1.9727, Batch Acc: 0.0000\n",
      "[Batch 410/501] Loss: 1.4032, Batch Acc: 0.3750\n",
      "[Batch 420/501] Loss: 1.7445, Batch Acc: 0.1250\n",
      "[Batch 430/501] Loss: 1.9405, Batch Acc: 0.1250\n",
      "[Batch 440/501] Loss: 1.9034, Batch Acc: 0.0000\n",
      "[Batch 450/501] Loss: 1.8358, Batch Acc: 0.1250\n",
      "[Batch 460/501] Loss: 1.7810, Batch Acc: 0.3750\n",
      "[Batch 470/501] Loss: 1.5645, Batch Acc: 0.2500\n",
      "[Batch 480/501] Loss: 1.4896, Batch Acc: 0.6250\n",
      "[Batch 490/501] Loss: 1.3270, Batch Acc: 0.3750\n",
      "[Batch 500/501] Loss: 1.5817, Batch Acc: 0.2500\n",
      "Epoch 6 Summary - Loss: 817.3409, Train Accuracy: 0.2567\n",
      "[Batch 10/501] Loss: 1.7012, Batch Acc: 0.3750\n",
      "[Batch 20/501] Loss: 1.5977, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.6336, Batch Acc: 0.1250\n",
      "[Batch 40/501] Loss: 1.6108, Batch Acc: 0.3750\n",
      "[Batch 50/501] Loss: 1.6494, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.8089, Batch Acc: 0.2500\n",
      "[Batch 70/501] Loss: 1.5597, Batch Acc: 0.3750\n",
      "[Batch 80/501] Loss: 1.6390, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.5375, Batch Acc: 0.2500\n",
      "[Batch 100/501] Loss: 1.5635, Batch Acc: 0.3750\n",
      "[Batch 110/501] Loss: 1.8836, Batch Acc: 0.2500\n",
      "[Batch 120/501] Loss: 1.6962, Batch Acc: 0.0000\n",
      "[Batch 130/501] Loss: 1.9700, Batch Acc: 0.1250\n",
      "[Batch 140/501] Loss: 1.3111, Batch Acc: 0.2500\n",
      "[Batch 150/501] Loss: 1.7655, Batch Acc: 0.1250\n",
      "[Batch 160/501] Loss: 1.6035, Batch Acc: 0.0000\n",
      "[Batch 170/501] Loss: 1.8685, Batch Acc: 0.2500\n",
      "[Batch 180/501] Loss: 1.5603, Batch Acc: 0.1250\n",
      "[Batch 190/501] Loss: 1.9239, Batch Acc: 0.1250\n",
      "[Batch 200/501] Loss: 1.6942, Batch Acc: 0.1250\n",
      "[Batch 210/501] Loss: 1.7005, Batch Acc: 0.2500\n",
      "[Batch 220/501] Loss: 1.6326, Batch Acc: 0.3750\n",
      "[Batch 230/501] Loss: 1.6546, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.7263, Batch Acc: 0.1250\n",
      "[Batch 250/501] Loss: 1.9387, Batch Acc: 0.1250\n",
      "[Batch 260/501] Loss: 1.6386, Batch Acc: 0.3750\n",
      "[Batch 270/501] Loss: 1.6978, Batch Acc: 0.2500\n",
      "[Batch 280/501] Loss: 1.5290, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 2.0014, Batch Acc: 0.1250\n",
      "[Batch 300/501] Loss: 1.4891, Batch Acc: 0.1250\n",
      "[Batch 310/501] Loss: 1.7254, Batch Acc: 0.2500\n",
      "[Batch 320/501] Loss: 1.7303, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 1.8404, Batch Acc: 0.1250\n",
      "[Batch 340/501] Loss: 1.3960, Batch Acc: 0.5000\n",
      "[Batch 350/501] Loss: 1.6391, Batch Acc: 0.1250\n",
      "[Batch 360/501] Loss: 1.9173, Batch Acc: 0.1250\n",
      "[Batch 370/501] Loss: 1.3878, Batch Acc: 0.3750\n",
      "[Batch 380/501] Loss: 1.5097, Batch Acc: 0.2500\n",
      "[Batch 390/501] Loss: 1.6882, Batch Acc: 0.5000\n",
      "[Batch 400/501] Loss: 1.3411, Batch Acc: 0.3750\n",
      "[Batch 410/501] Loss: 1.6068, Batch Acc: 0.5000\n",
      "[Batch 420/501] Loss: 1.7189, Batch Acc: 0.1250\n",
      "[Batch 430/501] Loss: 1.6491, Batch Acc: 0.1250\n",
      "[Batch 440/501] Loss: 1.5064, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.5708, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.8173, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.6928, Batch Acc: 0.2500\n",
      "[Batch 480/501] Loss: 1.3066, Batch Acc: 0.5000\n",
      "[Batch 490/501] Loss: 1.8691, Batch Acc: 0.1250\n",
      "[Batch 500/501] Loss: 1.6748, Batch Acc: 0.2500\n",
      "Epoch 7 Summary - Loss: 820.1445, Train Accuracy: 0.2500\n",
      "[Batch 10/501] Loss: 1.8184, Batch Acc: 0.0000\n",
      "[Batch 20/501] Loss: 1.6119, Batch Acc: 0.3750\n",
      "[Batch 30/501] Loss: 1.6164, Batch Acc: 0.3750\n",
      "[Batch 40/501] Loss: 1.7465, Batch Acc: 0.2500\n",
      "[Batch 50/501] Loss: 1.1861, Batch Acc: 0.6250\n",
      "[Batch 60/501] Loss: 1.7701, Batch Acc: 0.2500\n",
      "[Batch 70/501] Loss: 1.5968, Batch Acc: 0.3750\n",
      "[Batch 80/501] Loss: 1.4973, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.7229, Batch Acc: 0.3750\n",
      "[Batch 100/501] Loss: 1.6810, Batch Acc: 0.1250\n",
      "[Batch 110/501] Loss: 1.4166, Batch Acc: 0.5000\n",
      "[Batch 120/501] Loss: 1.5612, Batch Acc: 0.1250\n",
      "[Batch 130/501] Loss: 1.5939, Batch Acc: 0.6250\n",
      "[Batch 140/501] Loss: 1.4088, Batch Acc: 0.3750\n",
      "[Batch 150/501] Loss: 1.5247, Batch Acc: 0.2500\n",
      "[Batch 160/501] Loss: 1.5192, Batch Acc: 0.5000\n",
      "[Batch 170/501] Loss: 1.6596, Batch Acc: 0.1250\n",
      "[Batch 180/501] Loss: 1.6145, Batch Acc: 0.5000\n",
      "[Batch 190/501] Loss: 1.4860, Batch Acc: 0.3750\n",
      "[Batch 200/501] Loss: 1.8385, Batch Acc: 0.1250\n",
      "[Batch 210/501] Loss: 1.7252, Batch Acc: 0.1250\n",
      "[Batch 220/501] Loss: 1.7064, Batch Acc: 0.0000\n",
      "[Batch 230/501] Loss: 1.5873, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.8316, Batch Acc: 0.2500\n",
      "[Batch 250/501] Loss: 1.3643, Batch Acc: 0.5000\n",
      "[Batch 260/501] Loss: 1.6339, Batch Acc: 0.3750\n",
      "[Batch 270/501] Loss: 1.5213, Batch Acc: 0.2500\n",
      "[Batch 280/501] Loss: 1.7973, Batch Acc: 0.0000\n",
      "[Batch 290/501] Loss: 1.6516, Batch Acc: 0.2500\n",
      "[Batch 300/501] Loss: 1.8159, Batch Acc: 0.1250\n",
      "[Batch 310/501] Loss: 1.5203, Batch Acc: 0.1250\n",
      "[Batch 320/501] Loss: 1.6408, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 1.5704, Batch Acc: 0.5000\n",
      "[Batch 340/501] Loss: 1.8916, Batch Acc: 0.2500\n",
      "[Batch 350/501] Loss: 1.5138, Batch Acc: 0.3750\n",
      "[Batch 360/501] Loss: 1.4955, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 1.5855, Batch Acc: 0.2500\n",
      "[Batch 380/501] Loss: 1.6412, Batch Acc: 0.2500\n",
      "[Batch 390/501] Loss: 1.4466, Batch Acc: 0.2500\n",
      "[Batch 400/501] Loss: 1.7419, Batch Acc: 0.3750\n",
      "[Batch 410/501] Loss: 1.5216, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.8366, Batch Acc: 0.1250\n",
      "[Batch 430/501] Loss: 2.0551, Batch Acc: 0.1250\n",
      "[Batch 440/501] Loss: 2.1122, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.8519, Batch Acc: 0.1250\n",
      "[Batch 460/501] Loss: 1.4708, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.4472, Batch Acc: 0.2500\n",
      "[Batch 480/501] Loss: 1.4679, Batch Acc: 0.5000\n",
      "[Batch 490/501] Loss: 1.3250, Batch Acc: 0.5000\n",
      "[Batch 500/501] Loss: 1.2623, Batch Acc: 0.3750\n",
      "Epoch 8 Summary - Loss: 814.0342, Train Accuracy: 0.2665\n",
      "[Batch 10/501] Loss: 1.5347, Batch Acc: 0.2500\n",
      "[Batch 20/501] Loss: 1.7058, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.5571, Batch Acc: 0.2500\n",
      "[Batch 40/501] Loss: 1.5963, Batch Acc: 0.1250\n",
      "[Batch 50/501] Loss: 1.6234, Batch Acc: 0.3750\n",
      "[Batch 60/501] Loss: 1.8070, Batch Acc: 0.1250\n",
      "[Batch 70/501] Loss: 1.5313, Batch Acc: 0.1250\n",
      "[Batch 80/501] Loss: 1.4521, Batch Acc: 0.6250\n",
      "[Batch 90/501] Loss: 1.5434, Batch Acc: 0.2500\n",
      "[Batch 100/501] Loss: 1.5237, Batch Acc: 0.1250\n",
      "[Batch 110/501] Loss: 1.3699, Batch Acc: 0.2500\n",
      "[Batch 120/501] Loss: 1.7035, Batch Acc: 0.0000\n",
      "[Batch 130/501] Loss: 1.8201, Batch Acc: 0.1250\n",
      "[Batch 140/501] Loss: 1.9382, Batch Acc: 0.2500\n",
      "[Batch 150/501] Loss: 1.4468, Batch Acc: 0.3750\n",
      "[Batch 160/501] Loss: 2.0754, Batch Acc: 0.0000\n",
      "[Batch 170/501] Loss: 1.5526, Batch Acc: 0.2500\n",
      "[Batch 180/501] Loss: 1.6267, Batch Acc: 0.3750\n",
      "[Batch 190/501] Loss: 1.5026, Batch Acc: 0.2500\n",
      "[Batch 200/501] Loss: 1.6971, Batch Acc: 0.2500\n",
      "[Batch 210/501] Loss: 1.6155, Batch Acc: 0.1250\n",
      "[Batch 220/501] Loss: 1.6435, Batch Acc: 0.2500\n",
      "[Batch 230/501] Loss: 1.5068, Batch Acc: 0.0000\n",
      "[Batch 240/501] Loss: 1.6311, Batch Acc: 0.1250\n",
      "[Batch 250/501] Loss: 1.5001, Batch Acc: 0.2500\n",
      "[Batch 260/501] Loss: 1.8435, Batch Acc: 0.2500\n",
      "[Batch 270/501] Loss: 1.7009, Batch Acc: 0.1250\n",
      "[Batch 280/501] Loss: 1.5018, Batch Acc: 0.2500\n",
      "[Batch 290/501] Loss: 1.4661, Batch Acc: 0.5000\n",
      "[Batch 300/501] Loss: 1.5516, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 1.9729, Batch Acc: 0.0000\n",
      "[Batch 320/501] Loss: 1.5348, Batch Acc: 0.1250\n",
      "[Batch 330/501] Loss: 1.2481, Batch Acc: 0.8750\n",
      "[Batch 340/501] Loss: 1.5230, Batch Acc: 0.0000\n",
      "[Batch 350/501] Loss: 1.5014, Batch Acc: 0.3750\n",
      "[Batch 360/501] Loss: 1.3867, Batch Acc: 0.6250\n",
      "[Batch 370/501] Loss: 1.5406, Batch Acc: 0.0000\n",
      "[Batch 380/501] Loss: 1.4615, Batch Acc: 0.2500\n",
      "[Batch 390/501] Loss: 1.5036, Batch Acc: 0.2500\n",
      "[Batch 400/501] Loss: 1.4761, Batch Acc: 0.3750\n",
      "[Batch 410/501] Loss: 1.6906, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.5826, Batch Acc: 0.3750\n",
      "[Batch 430/501] Loss: 1.8613, Batch Acc: 0.3750\n",
      "[Batch 440/501] Loss: 1.6502, Batch Acc: 0.3750\n",
      "[Batch 450/501] Loss: 1.5042, Batch Acc: 0.3750\n",
      "[Batch 460/501] Loss: 1.6371, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.5982, Batch Acc: 0.2500\n",
      "[Batch 480/501] Loss: 1.5945, Batch Acc: 0.5000\n",
      "[Batch 490/501] Loss: 1.6803, Batch Acc: 0.2500\n",
      "[Batch 500/501] Loss: 1.6522, Batch Acc: 0.2500\n",
      "Epoch 9 Summary - Loss: 816.0730, Train Accuracy: 0.2540\n",
      "[Batch 10/501] Loss: 1.4384, Batch Acc: 0.5000\n",
      "[Batch 20/501] Loss: 1.5174, Batch Acc: 0.3750\n",
      "[Batch 30/501] Loss: 1.6643, Batch Acc: 0.1250\n",
      "[Batch 40/501] Loss: 1.5136, Batch Acc: 0.3750\n",
      "[Batch 50/501] Loss: 1.8034, Batch Acc: 0.1250\n",
      "[Batch 60/501] Loss: 2.0042, Batch Acc: 0.0000\n",
      "[Batch 70/501] Loss: 1.7042, Batch Acc: 0.0000\n",
      "[Batch 80/501] Loss: 1.5623, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.6135, Batch Acc: 0.2500\n",
      "[Batch 100/501] Loss: 1.7943, Batch Acc: 0.1250\n",
      "[Batch 110/501] Loss: 1.6568, Batch Acc: 0.3750\n",
      "[Batch 120/501] Loss: 1.4673, Batch Acc: 0.3750\n",
      "[Batch 130/501] Loss: 1.5341, Batch Acc: 0.5000\n",
      "[Batch 140/501] Loss: 1.8179, Batch Acc: 0.5000\n",
      "[Batch 150/501] Loss: 1.7653, Batch Acc: 0.1250\n",
      "[Batch 160/501] Loss: 2.0144, Batch Acc: 0.0000\n",
      "[Batch 170/501] Loss: 1.4308, Batch Acc: 0.5000\n",
      "[Batch 180/501] Loss: 1.3176, Batch Acc: 0.3750\n",
      "[Batch 190/501] Loss: 1.6482, Batch Acc: 0.2500\n",
      "[Batch 200/501] Loss: 1.7087, Batch Acc: 0.5000\n",
      "[Batch 210/501] Loss: 1.4149, Batch Acc: 0.5000\n",
      "[Batch 220/501] Loss: 1.3757, Batch Acc: 0.3750\n",
      "[Batch 230/501] Loss: 1.8307, Batch Acc: 0.3750\n",
      "[Batch 240/501] Loss: 1.4654, Batch Acc: 0.6250\n",
      "[Batch 250/501] Loss: 1.5979, Batch Acc: 0.3750\n",
      "[Batch 260/501] Loss: 1.4670, Batch Acc: 0.2500\n",
      "[Batch 270/501] Loss: 1.7853, Batch Acc: 0.0000\n",
      "[Batch 280/501] Loss: 1.7515, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 1.4746, Batch Acc: 0.1250\n",
      "[Batch 300/501] Loss: 1.7646, Batch Acc: 0.1250\n",
      "[Batch 310/501] Loss: 1.6067, Batch Acc: 0.3750\n",
      "[Batch 320/501] Loss: 1.4709, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 1.7502, Batch Acc: 0.3750\n",
      "[Batch 340/501] Loss: 1.5811, Batch Acc: 0.3750\n",
      "[Batch 350/501] Loss: 1.4064, Batch Acc: 0.3750\n",
      "[Batch 360/501] Loss: 1.5717, Batch Acc: 0.5000\n",
      "[Batch 370/501] Loss: 1.6763, Batch Acc: 0.3750\n",
      "[Batch 380/501] Loss: 1.5454, Batch Acc: 0.1250\n",
      "[Batch 390/501] Loss: 1.7433, Batch Acc: 0.1250\n",
      "[Batch 400/501] Loss: 1.5063, Batch Acc: 0.2500\n",
      "[Batch 410/501] Loss: 1.7047, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.7896, Batch Acc: 0.0000\n",
      "[Batch 430/501] Loss: 1.6966, Batch Acc: 0.2500\n",
      "[Batch 440/501] Loss: 1.2975, Batch Acc: 0.5000\n",
      "[Batch 450/501] Loss: 1.6518, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.6324, Batch Acc: 0.0000\n",
      "[Batch 470/501] Loss: 1.5833, Batch Acc: 0.5000\n",
      "[Batch 480/501] Loss: 1.6925, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.4938, Batch Acc: 0.2500\n",
      "[Batch 500/501] Loss: 1.6985, Batch Acc: 0.1250\n",
      "Epoch 10 Summary - Loss: 818.3651, Train Accuracy: 0.2585\n",
      "Test Accuracy: 0.2423\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_accuracy</td><td>▂▄▄▂▅▄▁█▃▅</td></tr><tr><td>train_loss</td><td>██▆▅▇▄▇▁▃▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>test_accuracy</td><td>0.24233</td></tr><tr><td>test_loss</td><td>203.82297</td></tr><tr><td>train_accuracy</td><td>0.25848</td></tr><tr><td>train_loss</td><td>818.36509</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fearless-sweep-13</strong> at: <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/0d6fy896' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/0d6fy896</a><br> View project at: <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250615_143137-0d6fy896/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0zok0vlp with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_nodes: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.5s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ajit.kumar4@happiestminds.com/Documents/Drackula/ComputerVisionFromScratch/wandb/run-20250615_143424-0zok0vlp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/0zok0vlp' target=\"_blank\">dry-sweep-14</a></strong> to <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/0zok0vlp' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/0zok0vlp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 10/501] Loss: 1.8695, Batch Acc: 0.0000\n",
      "[Batch 20/501] Loss: 1.3898, Batch Acc: 0.3750\n",
      "[Batch 30/501] Loss: 1.7191, Batch Acc: 0.0000\n",
      "[Batch 40/501] Loss: 1.6643, Batch Acc: 0.2500\n",
      "[Batch 50/501] Loss: 1.6305, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.5705, Batch Acc: 0.2500\n",
      "[Batch 70/501] Loss: 1.3992, Batch Acc: 0.3750\n",
      "[Batch 80/501] Loss: 1.7229, Batch Acc: 0.0000\n",
      "[Batch 90/501] Loss: 1.5860, Batch Acc: 0.2500\n",
      "[Batch 100/501] Loss: 1.6639, Batch Acc: 0.0000\n",
      "[Batch 110/501] Loss: 1.5629, Batch Acc: 0.2500\n",
      "[Batch 120/501] Loss: 1.6914, Batch Acc: 0.2500\n",
      "[Batch 130/501] Loss: 1.6001, Batch Acc: 0.2500\n",
      "[Batch 140/501] Loss: 1.4085, Batch Acc: 0.3750\n",
      "[Batch 150/501] Loss: 1.5928, Batch Acc: 0.1250\n",
      "[Batch 160/501] Loss: 1.7299, Batch Acc: 0.1250\n",
      "[Batch 170/501] Loss: 1.5374, Batch Acc: 0.3750\n",
      "[Batch 180/501] Loss: 1.7297, Batch Acc: 0.1250\n",
      "[Batch 190/501] Loss: 1.8085, Batch Acc: 0.1250\n",
      "[Batch 200/501] Loss: 1.4237, Batch Acc: 0.2500\n",
      "[Batch 210/501] Loss: 1.8852, Batch Acc: 0.2500\n",
      "[Batch 220/501] Loss: 1.4487, Batch Acc: 0.5000\n",
      "[Batch 230/501] Loss: 1.5402, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.5511, Batch Acc: 0.2500\n",
      "[Batch 250/501] Loss: 1.3259, Batch Acc: 0.5000\n",
      "[Batch 260/501] Loss: 1.5984, Batch Acc: 0.2500\n",
      "[Batch 270/501] Loss: 1.7205, Batch Acc: 0.1250\n",
      "[Batch 280/501] Loss: 1.7637, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 1.6516, Batch Acc: 0.3750\n",
      "[Batch 300/501] Loss: 1.7931, Batch Acc: 0.1250\n",
      "[Batch 310/501] Loss: 1.7060, Batch Acc: 0.2500\n",
      "[Batch 320/501] Loss: 1.6637, Batch Acc: 0.1250\n",
      "[Batch 330/501] Loss: 1.4793, Batch Acc: 0.3750\n",
      "[Batch 340/501] Loss: 1.6222, Batch Acc: 0.1250\n",
      "[Batch 350/501] Loss: 1.7163, Batch Acc: 0.3750\n",
      "[Batch 360/501] Loss: 1.8765, Batch Acc: 0.0000\n",
      "[Batch 370/501] Loss: 1.9441, Batch Acc: 0.2500\n",
      "[Batch 380/501] Loss: 1.5079, Batch Acc: 0.2500\n",
      "[Batch 390/501] Loss: 1.8021, Batch Acc: 0.1250\n",
      "[Batch 400/501] Loss: 1.6931, Batch Acc: 0.1250\n",
      "[Batch 410/501] Loss: 1.7363, Batch Acc: 0.1250\n",
      "[Batch 420/501] Loss: 1.6279, Batch Acc: 0.2500\n",
      "[Batch 430/501] Loss: 1.3925, Batch Acc: 0.3750\n",
      "[Batch 440/501] Loss: 1.6474, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.7765, Batch Acc: 0.1250\n",
      "[Batch 460/501] Loss: 1.7217, Batch Acc: 0.1250\n",
      "[Batch 470/501] Loss: 1.7770, Batch Acc: 0.0000\n",
      "[Batch 480/501] Loss: 1.6535, Batch Acc: 0.1250\n",
      "[Batch 490/501] Loss: 1.7339, Batch Acc: 0.1250\n",
      "[Batch 500/501] Loss: 1.4954, Batch Acc: 0.5000\n",
      "Epoch 1 Summary - Loss: 829.9191, Train Accuracy: 0.2221\n",
      "[Batch 10/501] Loss: 2.0069, Batch Acc: 0.3750\n",
      "[Batch 20/501] Loss: 1.5966, Batch Acc: 0.1250\n",
      "[Batch 30/501] Loss: 1.5264, Batch Acc: 0.2500\n",
      "[Batch 40/501] Loss: 1.6538, Batch Acc: 0.1250\n",
      "[Batch 50/501] Loss: 1.4495, Batch Acc: 0.5000\n",
      "[Batch 60/501] Loss: 1.7949, Batch Acc: 0.0000\n",
      "[Batch 70/501] Loss: 1.6894, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.7619, Batch Acc: 0.0000\n",
      "[Batch 90/501] Loss: 1.7334, Batch Acc: 0.0000\n",
      "[Batch 100/501] Loss: 1.9634, Batch Acc: 0.0000\n",
      "[Batch 110/501] Loss: 1.6845, Batch Acc: 0.1250\n",
      "[Batch 120/501] Loss: 1.8702, Batch Acc: 0.1250\n",
      "[Batch 130/501] Loss: 1.6017, Batch Acc: 0.2500\n",
      "[Batch 140/501] Loss: 1.8169, Batch Acc: 0.0000\n",
      "[Batch 150/501] Loss: 1.5048, Batch Acc: 0.1250\n",
      "[Batch 160/501] Loss: 1.5358, Batch Acc: 0.2500\n",
      "[Batch 170/501] Loss: 1.7075, Batch Acc: 0.1250\n",
      "[Batch 180/501] Loss: 1.6852, Batch Acc: 0.0000\n",
      "[Batch 190/501] Loss: 1.3599, Batch Acc: 0.3750\n",
      "[Batch 200/501] Loss: 1.8267, Batch Acc: 0.1250\n",
      "[Batch 210/501] Loss: 1.6900, Batch Acc: 0.1250\n",
      "[Batch 220/501] Loss: 1.7560, Batch Acc: 0.1250\n",
      "[Batch 230/501] Loss: 1.5553, Batch Acc: 0.3750\n",
      "[Batch 240/501] Loss: 1.7182, Batch Acc: 0.2500\n",
      "[Batch 250/501] Loss: 1.5215, Batch Acc: 0.2500\n",
      "[Batch 260/501] Loss: 1.4669, Batch Acc: 0.3750\n",
      "[Batch 270/501] Loss: 1.8380, Batch Acc: 0.0000\n",
      "[Batch 280/501] Loss: 1.6507, Batch Acc: 0.2500\n",
      "[Batch 290/501] Loss: 1.7408, Batch Acc: 0.3750\n",
      "[Batch 300/501] Loss: 1.8456, Batch Acc: 0.1250\n",
      "[Batch 310/501] Loss: 1.6927, Batch Acc: 0.3750\n",
      "[Batch 320/501] Loss: 1.6846, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 1.6306, Batch Acc: 0.0000\n",
      "[Batch 340/501] Loss: 1.6771, Batch Acc: 0.1250\n",
      "[Batch 350/501] Loss: 1.5597, Batch Acc: 0.1250\n",
      "[Batch 360/501] Loss: 1.6778, Batch Acc: 0.1250\n",
      "[Batch 370/501] Loss: 1.8882, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.7178, Batch Acc: 0.2500\n",
      "[Batch 390/501] Loss: 1.7909, Batch Acc: 0.1250\n",
      "[Batch 400/501] Loss: 1.7012, Batch Acc: 0.3750\n",
      "[Batch 410/501] Loss: 1.6265, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.6337, Batch Acc: 0.2500\n",
      "[Batch 430/501] Loss: 1.8373, Batch Acc: 0.1250\n",
      "[Batch 440/501] Loss: 1.7416, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.7991, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.5007, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.7830, Batch Acc: 0.2500\n",
      "[Batch 480/501] Loss: 1.7966, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.4658, Batch Acc: 0.3750\n",
      "[Batch 500/501] Loss: 1.6070, Batch Acc: 0.2500\n",
      "Epoch 2 Summary - Loss: 830.4673, Train Accuracy: 0.2350\n",
      "[Batch 10/501] Loss: 1.9679, Batch Acc: 0.2500\n",
      "[Batch 20/501] Loss: 1.6523, Batch Acc: 0.3750\n",
      "[Batch 30/501] Loss: 1.9776, Batch Acc: 0.0000\n",
      "[Batch 40/501] Loss: 1.7235, Batch Acc: 0.1250\n",
      "[Batch 50/501] Loss: 1.7027, Batch Acc: 0.1250\n",
      "[Batch 60/501] Loss: 1.6656, Batch Acc: 0.1250\n",
      "[Batch 70/501] Loss: 1.8500, Batch Acc: 0.0000\n",
      "[Batch 80/501] Loss: 1.5537, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.5595, Batch Acc: 0.2500\n",
      "[Batch 100/501] Loss: 1.8164, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 1.7274, Batch Acc: 0.2500\n",
      "[Batch 120/501] Loss: 1.8991, Batch Acc: 0.2500\n",
      "[Batch 130/501] Loss: 1.6317, Batch Acc: 0.2500\n",
      "[Batch 140/501] Loss: 1.8756, Batch Acc: 0.0000\n",
      "[Batch 150/501] Loss: 1.7477, Batch Acc: 0.2500\n",
      "[Batch 160/501] Loss: 1.5938, Batch Acc: 0.3750\n",
      "[Batch 170/501] Loss: 1.6068, Batch Acc: 0.5000\n",
      "[Batch 180/501] Loss: 1.7593, Batch Acc: 0.2500\n",
      "[Batch 190/501] Loss: 1.6330, Batch Acc: 0.3750\n",
      "[Batch 200/501] Loss: 1.7511, Batch Acc: 0.2500\n",
      "[Batch 210/501] Loss: 1.4845, Batch Acc: 0.1250\n",
      "[Batch 220/501] Loss: 1.6379, Batch Acc: 0.2500\n",
      "[Batch 230/501] Loss: 1.4137, Batch Acc: 0.5000\n",
      "[Batch 240/501] Loss: 1.3307, Batch Acc: 0.6250\n",
      "[Batch 250/501] Loss: 1.5805, Batch Acc: 0.3750\n",
      "[Batch 260/501] Loss: 1.6445, Batch Acc: 0.2500\n",
      "[Batch 270/501] Loss: 1.6140, Batch Acc: 0.1250\n",
      "[Batch 280/501] Loss: 1.6967, Batch Acc: 0.2500\n",
      "[Batch 290/501] Loss: 1.7836, Batch Acc: 0.2500\n",
      "[Batch 300/501] Loss: 1.5001, Batch Acc: 0.1250\n",
      "[Batch 310/501] Loss: 1.6146, Batch Acc: 0.0000\n",
      "[Batch 320/501] Loss: 1.6820, Batch Acc: 0.5000\n",
      "[Batch 330/501] Loss: 1.6344, Batch Acc: 0.1250\n",
      "[Batch 340/501] Loss: 1.6983, Batch Acc: 0.0000\n",
      "[Batch 350/501] Loss: 1.6436, Batch Acc: 0.0000\n",
      "[Batch 360/501] Loss: 1.7404, Batch Acc: 0.1250\n",
      "[Batch 370/501] Loss: 1.4849, Batch Acc: 0.5000\n",
      "[Batch 380/501] Loss: 1.7850, Batch Acc: 0.0000\n",
      "[Batch 390/501] Loss: 1.5463, Batch Acc: 0.1250\n",
      "[Batch 400/501] Loss: 2.0690, Batch Acc: 0.2500\n",
      "[Batch 410/501] Loss: 1.6314, Batch Acc: 0.1250\n",
      "[Batch 420/501] Loss: 1.6115, Batch Acc: 0.1250\n",
      "[Batch 430/501] Loss: 1.6957, Batch Acc: 0.1250\n",
      "[Batch 440/501] Loss: 1.7221, Batch Acc: 0.1250\n",
      "[Batch 450/501] Loss: 1.4827, Batch Acc: 0.3750\n",
      "[Batch 460/501] Loss: 1.4649, Batch Acc: 0.3750\n",
      "[Batch 470/501] Loss: 1.5691, Batch Acc: 0.2500\n",
      "[Batch 480/501] Loss: 1.3915, Batch Acc: 0.7500\n",
      "[Batch 490/501] Loss: 1.9103, Batch Acc: 0.0000\n",
      "[Batch 500/501] Loss: 1.6547, Batch Acc: 0.0000\n",
      "Epoch 3 Summary - Loss: 833.4212, Train Accuracy: 0.2293\n",
      "[Batch 10/501] Loss: 1.8852, Batch Acc: 0.1250\n",
      "[Batch 20/501] Loss: 1.6853, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.7822, Batch Acc: 0.1250\n",
      "[Batch 40/501] Loss: 1.6911, Batch Acc: 0.2500\n",
      "[Batch 50/501] Loss: 1.6442, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.5958, Batch Acc: 0.2500\n",
      "[Batch 70/501] Loss: 1.5754, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.7462, Batch Acc: 0.0000\n",
      "[Batch 90/501] Loss: 1.8602, Batch Acc: 0.0000\n",
      "[Batch 100/501] Loss: 1.5378, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 1.5588, Batch Acc: 0.3750\n",
      "[Batch 120/501] Loss: 1.7995, Batch Acc: 0.1250\n",
      "[Batch 130/501] Loss: 1.3348, Batch Acc: 0.5000\n",
      "[Batch 140/501] Loss: 1.6127, Batch Acc: 0.2500\n",
      "[Batch 150/501] Loss: 1.5064, Batch Acc: 0.5000\n",
      "[Batch 160/501] Loss: 1.6414, Batch Acc: 0.1250\n",
      "[Batch 170/501] Loss: 1.7827, Batch Acc: 0.2500\n",
      "[Batch 180/501] Loss: 1.6299, Batch Acc: 0.2500\n",
      "[Batch 190/501] Loss: 1.8792, Batch Acc: 0.1250\n",
      "[Batch 200/501] Loss: 1.7379, Batch Acc: 0.1250\n",
      "[Batch 210/501] Loss: 1.4647, Batch Acc: 0.3750\n",
      "[Batch 220/501] Loss: 1.4683, Batch Acc: 0.2500\n",
      "[Batch 230/501] Loss: 1.5959, Batch Acc: 0.1250\n",
      "[Batch 240/501] Loss: 1.6738, Batch Acc: 0.3750\n",
      "[Batch 250/501] Loss: 1.5671, Batch Acc: 0.5000\n",
      "[Batch 260/501] Loss: 1.4830, Batch Acc: 0.3750\n",
      "[Batch 270/501] Loss: 1.5439, Batch Acc: 0.2500\n",
      "[Batch 280/501] Loss: 1.6682, Batch Acc: 0.2500\n",
      "[Batch 290/501] Loss: 1.4965, Batch Acc: 0.2500\n",
      "[Batch 300/501] Loss: 1.6789, Batch Acc: 0.1250\n",
      "[Batch 310/501] Loss: 1.5563, Batch Acc: 0.2500\n",
      "[Batch 320/501] Loss: 1.6436, Batch Acc: 0.3750\n",
      "[Batch 330/501] Loss: 1.5826, Batch Acc: 0.1250\n",
      "[Batch 340/501] Loss: 1.9036, Batch Acc: 0.0000\n",
      "[Batch 350/501] Loss: 1.6657, Batch Acc: 0.2500\n",
      "[Batch 360/501] Loss: 1.6048, Batch Acc: 0.3750\n",
      "[Batch 370/501] Loss: 1.8756, Batch Acc: 0.0000\n",
      "[Batch 380/501] Loss: 1.7656, Batch Acc: 0.1250\n",
      "[Batch 390/501] Loss: 1.8779, Batch Acc: 0.2500\n",
      "[Batch 400/501] Loss: 1.7616, Batch Acc: 0.2500\n",
      "[Batch 410/501] Loss: 1.6267, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.6789, Batch Acc: 0.1250\n",
      "[Batch 430/501] Loss: 1.5648, Batch Acc: 0.3750\n",
      "[Batch 440/501] Loss: 1.6670, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.6226, Batch Acc: 0.1250\n",
      "[Batch 460/501] Loss: 1.7375, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.4485, Batch Acc: 0.3750\n",
      "[Batch 480/501] Loss: 1.7792, Batch Acc: 0.1250\n",
      "[Batch 490/501] Loss: 1.8613, Batch Acc: 0.1250\n",
      "[Batch 500/501] Loss: 1.6377, Batch Acc: 0.1250\n",
      "Epoch 4 Summary - Loss: 832.8711, Train Accuracy: 0.2181\n",
      "[Batch 10/501] Loss: 1.4396, Batch Acc: 0.3750\n",
      "[Batch 20/501] Loss: 1.5104, Batch Acc: 0.3750\n",
      "[Batch 30/501] Loss: 1.6855, Batch Acc: 0.0000\n",
      "[Batch 40/501] Loss: 1.6906, Batch Acc: 0.2500\n",
      "[Batch 50/501] Loss: 1.7232, Batch Acc: 0.1250\n",
      "[Batch 60/501] Loss: 1.8084, Batch Acc: 0.1250\n",
      "[Batch 70/501] Loss: 1.5714, Batch Acc: 0.1250\n",
      "[Batch 80/501] Loss: 1.5140, Batch Acc: 0.1250\n",
      "[Batch 90/501] Loss: 1.7694, Batch Acc: 0.1250\n",
      "[Batch 100/501] Loss: 1.7737, Batch Acc: 0.1250\n",
      "[Batch 110/501] Loss: 1.6078, Batch Acc: 0.5000\n",
      "[Batch 120/501] Loss: 1.7330, Batch Acc: 0.1250\n",
      "[Batch 130/501] Loss: 1.8075, Batch Acc: 0.1250\n",
      "[Batch 140/501] Loss: 1.6884, Batch Acc: 0.0000\n",
      "[Batch 150/501] Loss: 1.7938, Batch Acc: 0.1250\n",
      "[Batch 160/501] Loss: 1.6383, Batch Acc: 0.1250\n",
      "[Batch 170/501] Loss: 1.6253, Batch Acc: 0.3750\n",
      "[Batch 180/501] Loss: 1.6700, Batch Acc: 0.1250\n",
      "[Batch 190/501] Loss: 1.7833, Batch Acc: 0.2500\n",
      "[Batch 200/501] Loss: 1.7962, Batch Acc: 0.1250\n",
      "[Batch 210/501] Loss: 1.6490, Batch Acc: 0.3750\n",
      "[Batch 220/501] Loss: 1.6405, Batch Acc: 0.3750\n",
      "[Batch 230/501] Loss: 1.7244, Batch Acc: 0.1250\n",
      "[Batch 240/501] Loss: 1.5746, Batch Acc: 0.2500\n",
      "[Batch 250/501] Loss: 1.6547, Batch Acc: 0.3750\n",
      "[Batch 260/501] Loss: 1.7741, Batch Acc: 0.1250\n",
      "[Batch 270/501] Loss: 1.5861, Batch Acc: 0.3750\n",
      "[Batch 280/501] Loss: 1.7546, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 1.6285, Batch Acc: 0.2500\n",
      "[Batch 300/501] Loss: 1.6272, Batch Acc: 0.1250\n",
      "[Batch 310/501] Loss: 1.5164, Batch Acc: 0.1250\n",
      "[Batch 320/501] Loss: 1.8329, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 1.6654, Batch Acc: 0.3750\n",
      "[Batch 340/501] Loss: 1.5246, Batch Acc: 0.0000\n",
      "[Batch 350/501] Loss: 1.7351, Batch Acc: 0.2500\n",
      "[Batch 360/501] Loss: 1.5894, Batch Acc: 0.5000\n",
      "[Batch 370/501] Loss: 1.4738, Batch Acc: 0.3750\n",
      "[Batch 380/501] Loss: 1.8348, Batch Acc: 0.0000\n",
      "[Batch 390/501] Loss: 1.7729, Batch Acc: 0.0000\n",
      "[Batch 400/501] Loss: 1.4042, Batch Acc: 0.3750\n",
      "[Batch 410/501] Loss: 1.7863, Batch Acc: 0.1250\n",
      "[Batch 420/501] Loss: 1.5014, Batch Acc: 0.5000\n",
      "[Batch 430/501] Loss: 1.6123, Batch Acc: 0.3750\n",
      "[Batch 440/501] Loss: 1.4650, Batch Acc: 0.5000\n",
      "[Batch 450/501] Loss: 1.6252, Batch Acc: 0.3750\n",
      "[Batch 460/501] Loss: 1.9122, Batch Acc: 0.1250\n",
      "[Batch 470/501] Loss: 1.8094, Batch Acc: 0.1250\n",
      "[Batch 480/501] Loss: 1.5756, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.5464, Batch Acc: 0.3750\n",
      "[Batch 500/501] Loss: 1.9880, Batch Acc: 0.0000\n",
      "Epoch 5 Summary - Loss: 831.5678, Train Accuracy: 0.2193\n",
      "[Batch 10/501] Loss: 1.5340, Batch Acc: 0.2500\n",
      "[Batch 20/501] Loss: 1.5204, Batch Acc: 0.3750\n",
      "[Batch 30/501] Loss: 1.4040, Batch Acc: 0.5000\n",
      "[Batch 40/501] Loss: 1.5129, Batch Acc: 0.3750\n",
      "[Batch 50/501] Loss: 1.7527, Batch Acc: 0.1250\n",
      "[Batch 60/501] Loss: 1.5888, Batch Acc: 0.2500\n",
      "[Batch 70/501] Loss: 1.5713, Batch Acc: 0.3750\n",
      "[Batch 80/501] Loss: 1.6490, Batch Acc: 0.1250\n",
      "[Batch 90/501] Loss: 1.6219, Batch Acc: 0.2500\n",
      "[Batch 100/501] Loss: 1.8440, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 1.5411, Batch Acc: 0.3750\n",
      "[Batch 120/501] Loss: 1.5840, Batch Acc: 0.2500\n",
      "[Batch 130/501] Loss: 1.9110, Batch Acc: 0.2500\n",
      "[Batch 140/501] Loss: 1.5978, Batch Acc: 0.2500\n",
      "[Batch 150/501] Loss: 1.6459, Batch Acc: 0.2500\n",
      "[Batch 160/501] Loss: 1.6439, Batch Acc: 0.2500\n",
      "[Batch 170/501] Loss: 1.8441, Batch Acc: 0.1250\n",
      "[Batch 180/501] Loss: 1.8417, Batch Acc: 0.0000\n",
      "[Batch 190/501] Loss: 1.5265, Batch Acc: 0.5000\n",
      "[Batch 200/501] Loss: 1.6381, Batch Acc: 0.2500\n",
      "[Batch 210/501] Loss: 1.6268, Batch Acc: 0.2500\n",
      "[Batch 220/501] Loss: 1.6621, Batch Acc: 0.0000\n",
      "[Batch 230/501] Loss: 1.6264, Batch Acc: 0.3750\n",
      "[Batch 240/501] Loss: 1.8562, Batch Acc: 0.2500\n",
      "[Batch 250/501] Loss: 1.7257, Batch Acc: 0.3750\n",
      "[Batch 260/501] Loss: 1.7692, Batch Acc: 0.1250\n",
      "[Batch 270/501] Loss: 1.3202, Batch Acc: 0.5000\n",
      "[Batch 280/501] Loss: 1.7437, Batch Acc: 0.2500\n",
      "[Batch 290/501] Loss: 1.4186, Batch Acc: 0.5000\n",
      "[Batch 300/501] Loss: 1.4697, Batch Acc: 0.1250\n",
      "[Batch 310/501] Loss: 1.7151, Batch Acc: 0.1250\n",
      "[Batch 320/501] Loss: 1.5843, Batch Acc: 0.3750\n",
      "[Batch 330/501] Loss: 1.7261, Batch Acc: 0.1250\n",
      "[Batch 340/501] Loss: 1.3886, Batch Acc: 0.6250\n",
      "[Batch 350/501] Loss: 1.7069, Batch Acc: 0.1250\n",
      "[Batch 360/501] Loss: 1.6151, Batch Acc: 0.1250\n",
      "[Batch 370/501] Loss: 1.7442, Batch Acc: 0.0000\n",
      "[Batch 380/501] Loss: 1.7815, Batch Acc: 0.2500\n",
      "[Batch 390/501] Loss: 1.4106, Batch Acc: 0.3750\n",
      "[Batch 400/501] Loss: 1.8287, Batch Acc: 0.1250\n",
      "[Batch 410/501] Loss: 1.6686, Batch Acc: 0.1250\n",
      "[Batch 420/501] Loss: 1.6981, Batch Acc: 0.1250\n",
      "[Batch 430/501] Loss: 1.6174, Batch Acc: 0.2500\n",
      "[Batch 440/501] Loss: 1.5536, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.8155, Batch Acc: 0.1250\n",
      "[Batch 460/501] Loss: 1.9268, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.6281, Batch Acc: 0.3750\n",
      "[Batch 480/501] Loss: 1.8270, Batch Acc: 0.3750\n",
      "[Batch 490/501] Loss: 1.7019, Batch Acc: 0.1250\n",
      "[Batch 500/501] Loss: 1.5342, Batch Acc: 0.2500\n",
      "Epoch 6 Summary - Loss: 830.8062, Train Accuracy: 0.2280\n",
      "[Batch 10/501] Loss: 1.5080, Batch Acc: 0.3750\n",
      "[Batch 20/501] Loss: 1.6065, Batch Acc: 0.3750\n",
      "[Batch 30/501] Loss: 1.7488, Batch Acc: 0.0000\n",
      "[Batch 40/501] Loss: 1.5711, Batch Acc: 0.2500\n",
      "[Batch 50/501] Loss: 1.7548, Batch Acc: 0.1250\n",
      "[Batch 60/501] Loss: 1.6997, Batch Acc: 0.1250\n",
      "[Batch 70/501] Loss: 1.5729, Batch Acc: 0.3750\n",
      "[Batch 80/501] Loss: 1.6187, Batch Acc: 0.3750\n",
      "[Batch 90/501] Loss: 1.7355, Batch Acc: 0.3750\n",
      "[Batch 100/501] Loss: 1.8641, Batch Acc: 0.1250\n",
      "[Batch 110/501] Loss: 1.2929, Batch Acc: 0.5000\n",
      "[Batch 120/501] Loss: 1.6562, Batch Acc: 0.2500\n",
      "[Batch 130/501] Loss: 1.8379, Batch Acc: 0.1250\n",
      "[Batch 140/501] Loss: 1.6229, Batch Acc: 0.2500\n",
      "[Batch 150/501] Loss: 1.6159, Batch Acc: 0.5000\n",
      "[Batch 160/501] Loss: 1.7683, Batch Acc: 0.1250\n",
      "[Batch 170/501] Loss: 1.4262, Batch Acc: 0.2500\n",
      "[Batch 180/501] Loss: 1.5777, Batch Acc: 0.5000\n",
      "[Batch 190/501] Loss: 1.4935, Batch Acc: 0.5000\n",
      "[Batch 200/501] Loss: 1.7376, Batch Acc: 0.2500\n",
      "[Batch 210/501] Loss: 1.5490, Batch Acc: 0.3750\n",
      "[Batch 220/501] Loss: 1.4812, Batch Acc: 0.2500\n",
      "[Batch 230/501] Loss: 1.4892, Batch Acc: 0.5000\n",
      "[Batch 240/501] Loss: 1.4793, Batch Acc: 0.2500\n",
      "[Batch 250/501] Loss: 1.7171, Batch Acc: 0.2500\n",
      "[Batch 260/501] Loss: 1.8093, Batch Acc: 0.1250\n",
      "[Batch 270/501] Loss: 1.6556, Batch Acc: 0.3750\n",
      "[Batch 280/501] Loss: 1.5129, Batch Acc: 0.2500\n",
      "[Batch 290/501] Loss: 1.9465, Batch Acc: 0.1250\n",
      "[Batch 300/501] Loss: 1.4811, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 1.4478, Batch Acc: 0.3750\n",
      "[Batch 320/501] Loss: 1.6808, Batch Acc: 0.3750\n",
      "[Batch 330/501] Loss: 1.6609, Batch Acc: 0.2500\n",
      "[Batch 340/501] Loss: 1.6735, Batch Acc: 0.0000\n",
      "[Batch 350/501] Loss: 1.4743, Batch Acc: 0.5000\n",
      "[Batch 360/501] Loss: 1.7051, Batch Acc: 0.0000\n",
      "[Batch 370/501] Loss: 1.6523, Batch Acc: 0.2500\n",
      "[Batch 380/501] Loss: 1.6428, Batch Acc: 0.2500\n",
      "[Batch 390/501] Loss: 1.5091, Batch Acc: 0.5000\n",
      "[Batch 400/501] Loss: 1.6628, Batch Acc: 0.1250\n",
      "[Batch 410/501] Loss: 1.9218, Batch Acc: 0.1250\n",
      "[Batch 420/501] Loss: 1.3924, Batch Acc: 0.3750\n",
      "[Batch 430/501] Loss: 1.8207, Batch Acc: 0.1250\n",
      "[Batch 440/501] Loss: 2.1066, Batch Acc: 0.0000\n",
      "[Batch 450/501] Loss: 1.4910, Batch Acc: 0.3750\n",
      "[Batch 460/501] Loss: 1.3863, Batch Acc: 0.3750\n",
      "[Batch 470/501] Loss: 1.9143, Batch Acc: 0.0000\n",
      "[Batch 480/501] Loss: 1.8600, Batch Acc: 0.0000\n",
      "[Batch 490/501] Loss: 1.8657, Batch Acc: 0.1250\n",
      "[Batch 500/501] Loss: 1.9600, Batch Acc: 0.2500\n",
      "Epoch 7 Summary - Loss: 829.4954, Train Accuracy: 0.2298\n",
      "[Batch 10/501] Loss: 1.4839, Batch Acc: 0.6250\n",
      "[Batch 20/501] Loss: 1.7345, Batch Acc: 0.1250\n",
      "[Batch 30/501] Loss: 1.6247, Batch Acc: 0.3750\n",
      "[Batch 40/501] Loss: 1.4453, Batch Acc: 0.3750\n",
      "[Batch 50/501] Loss: 1.2889, Batch Acc: 0.7500\n",
      "[Batch 60/501] Loss: 1.7565, Batch Acc: 0.2500\n",
      "[Batch 70/501] Loss: 1.7083, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.5381, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.6670, Batch Acc: 0.1250\n",
      "[Batch 100/501] Loss: 1.8228, Batch Acc: 0.3750\n",
      "[Batch 110/501] Loss: 1.7207, Batch Acc: 0.0000\n",
      "[Batch 120/501] Loss: 1.6318, Batch Acc: 0.2500\n",
      "[Batch 130/501] Loss: 1.8501, Batch Acc: 0.0000\n",
      "[Batch 140/501] Loss: 1.5018, Batch Acc: 0.3750\n",
      "[Batch 150/501] Loss: 1.8999, Batch Acc: 0.1250\n",
      "[Batch 160/501] Loss: 1.7705, Batch Acc: 0.1250\n",
      "[Batch 170/501] Loss: 1.5124, Batch Acc: 0.1250\n",
      "[Batch 180/501] Loss: 1.5308, Batch Acc: 0.1250\n",
      "[Batch 190/501] Loss: 1.6182, Batch Acc: 0.2500\n",
      "[Batch 200/501] Loss: 1.6104, Batch Acc: 0.2500\n",
      "[Batch 210/501] Loss: 1.7072, Batch Acc: 0.2500\n",
      "[Batch 220/501] Loss: 1.5918, Batch Acc: 0.3750\n",
      "[Batch 230/501] Loss: 1.8170, Batch Acc: 0.1250\n",
      "[Batch 240/501] Loss: 1.2436, Batch Acc: 0.7500\n",
      "[Batch 250/501] Loss: 1.4621, Batch Acc: 0.3750\n",
      "[Batch 260/501] Loss: 1.4250, Batch Acc: 0.5000\n",
      "[Batch 270/501] Loss: 1.3796, Batch Acc: 0.5000\n",
      "[Batch 280/501] Loss: 1.5601, Batch Acc: 0.5000\n",
      "[Batch 290/501] Loss: 1.8325, Batch Acc: 0.2500\n",
      "[Batch 300/501] Loss: 1.7664, Batch Acc: 0.3750\n",
      "[Batch 310/501] Loss: 1.6549, Batch Acc: 0.2500\n",
      "[Batch 320/501] Loss: 1.6259, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 1.5711, Batch Acc: 0.3750\n",
      "[Batch 340/501] Loss: 1.7339, Batch Acc: 0.1250\n",
      "[Batch 350/501] Loss: 1.5022, Batch Acc: 0.1250\n",
      "[Batch 360/501] Loss: 1.7966, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 1.9815, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.6338, Batch Acc: 0.2500\n",
      "[Batch 390/501] Loss: 1.3546, Batch Acc: 0.6250\n",
      "[Batch 400/501] Loss: 1.6080, Batch Acc: 0.2500\n",
      "[Batch 410/501] Loss: 1.5729, Batch Acc: 0.1250\n",
      "[Batch 420/501] Loss: 1.7515, Batch Acc: 0.2500\n",
      "[Batch 430/501] Loss: 1.4834, Batch Acc: 0.5000\n",
      "[Batch 440/501] Loss: 1.7269, Batch Acc: 0.1250\n",
      "[Batch 450/501] Loss: 1.4811, Batch Acc: 0.1250\n",
      "[Batch 460/501] Loss: 1.5865, Batch Acc: 0.3750\n",
      "[Batch 470/501] Loss: 1.8660, Batch Acc: 0.2500\n",
      "[Batch 480/501] Loss: 1.6698, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.6821, Batch Acc: 0.3750\n",
      "[Batch 500/501] Loss: 1.6552, Batch Acc: 0.1250\n",
      "Epoch 8 Summary - Loss: 829.7141, Train Accuracy: 0.2305\n",
      "[Batch 10/501] Loss: 1.7522, Batch Acc: 0.3750\n",
      "[Batch 20/501] Loss: 1.5635, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.8183, Batch Acc: 0.0000\n",
      "[Batch 40/501] Loss: 1.5776, Batch Acc: 0.2500\n",
      "[Batch 50/501] Loss: 1.5221, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.6002, Batch Acc: 0.2500\n",
      "[Batch 70/501] Loss: 1.7255, Batch Acc: 0.1250\n",
      "[Batch 80/501] Loss: 1.6934, Batch Acc: 0.0000\n",
      "[Batch 90/501] Loss: 1.6210, Batch Acc: 0.3750\n",
      "[Batch 100/501] Loss: 1.8532, Batch Acc: 0.1250\n",
      "[Batch 110/501] Loss: 1.5624, Batch Acc: 0.2500\n",
      "[Batch 120/501] Loss: 1.9517, Batch Acc: 0.0000\n",
      "[Batch 130/501] Loss: 1.4536, Batch Acc: 0.5000\n",
      "[Batch 140/501] Loss: 1.5639, Batch Acc: 0.3750\n",
      "[Batch 150/501] Loss: 1.5801, Batch Acc: 0.1250\n",
      "[Batch 160/501] Loss: 1.5522, Batch Acc: 0.2500\n",
      "[Batch 170/501] Loss: 1.7938, Batch Acc: 0.1250\n",
      "[Batch 180/501] Loss: 1.4626, Batch Acc: 0.2500\n",
      "[Batch 190/501] Loss: 1.5583, Batch Acc: 0.3750\n",
      "[Batch 200/501] Loss: 1.5185, Batch Acc: 0.3750\n",
      "[Batch 210/501] Loss: 1.6092, Batch Acc: 0.2500\n",
      "[Batch 220/501] Loss: 1.6122, Batch Acc: 0.2500\n",
      "[Batch 230/501] Loss: 1.3959, Batch Acc: 0.3750\n",
      "[Batch 240/501] Loss: 1.6107, Batch Acc: 0.2500\n",
      "[Batch 250/501] Loss: 1.7679, Batch Acc: 0.2500\n",
      "[Batch 260/501] Loss: 1.7144, Batch Acc: 0.2500\n",
      "[Batch 270/501] Loss: 1.5659, Batch Acc: 0.3750\n",
      "[Batch 280/501] Loss: 1.7462, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 1.6303, Batch Acc: 0.2500\n",
      "[Batch 300/501] Loss: 1.6884, Batch Acc: 0.5000\n",
      "[Batch 310/501] Loss: 1.7966, Batch Acc: 0.1250\n",
      "[Batch 320/501] Loss: 1.7157, Batch Acc: 0.0000\n",
      "[Batch 330/501] Loss: 1.7425, Batch Acc: 0.2500\n",
      "[Batch 340/501] Loss: 1.7708, Batch Acc: 0.1250\n",
      "[Batch 350/501] Loss: 1.5907, Batch Acc: 0.2500\n",
      "[Batch 360/501] Loss: 1.8484, Batch Acc: 0.1250\n",
      "[Batch 370/501] Loss: 1.6847, Batch Acc: 0.0000\n",
      "[Batch 380/501] Loss: 1.5941, Batch Acc: 0.3750\n",
      "[Batch 390/501] Loss: 1.6123, Batch Acc: 0.1250\n",
      "[Batch 400/501] Loss: 1.7129, Batch Acc: 0.0000\n",
      "[Batch 410/501] Loss: 1.6279, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.7695, Batch Acc: 0.0000\n",
      "[Batch 430/501] Loss: 1.7893, Batch Acc: 0.1250\n",
      "[Batch 440/501] Loss: 1.7140, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.6492, Batch Acc: 0.1250\n",
      "[Batch 460/501] Loss: 1.7465, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.5302, Batch Acc: 0.3750\n",
      "[Batch 480/501] Loss: 1.7900, Batch Acc: 0.0000\n",
      "[Batch 490/501] Loss: 1.6016, Batch Acc: 0.1250\n",
      "[Batch 500/501] Loss: 1.3601, Batch Acc: 0.3750\n",
      "Epoch 9 Summary - Loss: 828.5169, Train Accuracy: 0.2325\n",
      "[Batch 10/501] Loss: 1.5404, Batch Acc: 0.3750\n",
      "[Batch 20/501] Loss: 1.5362, Batch Acc: 0.5000\n",
      "[Batch 30/501] Loss: 1.7658, Batch Acc: 0.1250\n",
      "[Batch 40/501] Loss: 1.7220, Batch Acc: 0.1250\n",
      "[Batch 50/501] Loss: 1.5602, Batch Acc: 0.3750\n",
      "[Batch 60/501] Loss: 1.6754, Batch Acc: 0.1250\n",
      "[Batch 70/501] Loss: 1.7607, Batch Acc: 0.3750\n",
      "[Batch 80/501] Loss: 1.5814, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.8089, Batch Acc: 0.3750\n",
      "[Batch 100/501] Loss: 1.6731, Batch Acc: 0.1250\n",
      "[Batch 110/501] Loss: 1.4862, Batch Acc: 0.2500\n",
      "[Batch 120/501] Loss: 1.7492, Batch Acc: 0.1250\n",
      "[Batch 130/501] Loss: 1.5536, Batch Acc: 0.2500\n",
      "[Batch 140/501] Loss: 1.6264, Batch Acc: 0.2500\n",
      "[Batch 150/501] Loss: 1.5899, Batch Acc: 0.3750\n",
      "[Batch 160/501] Loss: 1.8318, Batch Acc: 0.2500\n",
      "[Batch 170/501] Loss: 1.7228, Batch Acc: 0.1250\n",
      "[Batch 180/501] Loss: 1.7702, Batch Acc: 0.2500\n",
      "[Batch 190/501] Loss: 1.5676, Batch Acc: 0.3750\n",
      "[Batch 200/501] Loss: 1.5608, Batch Acc: 0.0000\n",
      "[Batch 210/501] Loss: 1.7190, Batch Acc: 0.3750\n",
      "[Batch 220/501] Loss: 1.6181, Batch Acc: 0.2500\n",
      "[Batch 230/501] Loss: 1.8052, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.5837, Batch Acc: 0.2500\n",
      "[Batch 250/501] Loss: 1.7310, Batch Acc: 0.0000\n",
      "[Batch 260/501] Loss: 1.6047, Batch Acc: 0.2500\n",
      "[Batch 270/501] Loss: 1.5371, Batch Acc: 0.2500\n",
      "[Batch 280/501] Loss: 1.7199, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 1.6458, Batch Acc: 0.1250\n",
      "[Batch 300/501] Loss: 1.5238, Batch Acc: 0.5000\n",
      "[Batch 310/501] Loss: 1.5936, Batch Acc: 0.1250\n",
      "[Batch 320/501] Loss: 1.7610, Batch Acc: 0.0000\n",
      "[Batch 330/501] Loss: 1.6714, Batch Acc: 0.1250\n",
      "[Batch 340/501] Loss: 1.7706, Batch Acc: 0.1250\n",
      "[Batch 350/501] Loss: 1.7887, Batch Acc: 0.1250\n",
      "[Batch 360/501] Loss: 1.5317, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 1.8261, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.6456, Batch Acc: 0.2500\n",
      "[Batch 390/501] Loss: 1.5805, Batch Acc: 0.3750\n",
      "[Batch 400/501] Loss: 1.7607, Batch Acc: 0.1250\n",
      "[Batch 410/501] Loss: 1.6979, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.8432, Batch Acc: 0.0000\n",
      "[Batch 430/501] Loss: 1.7095, Batch Acc: 0.1250\n",
      "[Batch 440/501] Loss: 1.4475, Batch Acc: 0.5000\n",
      "[Batch 450/501] Loss: 1.4951, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.2270, Batch Acc: 0.6250\n",
      "[Batch 470/501] Loss: 1.8735, Batch Acc: 0.1250\n",
      "[Batch 480/501] Loss: 1.7523, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.4881, Batch Acc: 0.2500\n",
      "[Batch 500/501] Loss: 1.4860, Batch Acc: 0.3750\n",
      "Epoch 10 Summary - Loss: 830.7742, Train Accuracy: 0.2218\n",
      "Test Accuracy: 0.2572\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_accuracy</td><td>▃█▆▁▂▅▆▆▇▃</td></tr><tr><td>train_loss</td><td>▃▄█▇▅▄▂▃▁▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>test_accuracy</td><td>0.25717</td></tr><tr><td>test_loss</td><td>207.90975</td></tr><tr><td>train_accuracy</td><td>0.22181</td></tr><tr><td>train_loss</td><td>830.77417</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dry-sweep-14</strong> at: <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/0zok0vlp' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/0zok0vlp</a><br> View project at: <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250615_143424-0zok0vlp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: me0abq6w with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_nodes: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ajit.kumar4@happiestminds.com/Documents/Drackula/ComputerVisionFromScratch/wandb/run-20250615_143713-me0abq6w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/me0abq6w' target=\"_blank\">gentle-sweep-15</a></strong> to <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/me0abq6w' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/me0abq6w</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 10/501] Loss: 1.6168, Batch Acc: 0.2500\n",
      "[Batch 20/501] Loss: 1.7021, Batch Acc: 0.1250\n",
      "[Batch 30/501] Loss: 1.9702, Batch Acc: 0.1250\n",
      "[Batch 40/501] Loss: 1.6498, Batch Acc: 0.2500\n",
      "[Batch 50/501] Loss: 1.6484, Batch Acc: 0.1250\n",
      "[Batch 60/501] Loss: 1.6230, Batch Acc: 0.3750\n",
      "[Batch 70/501] Loss: 1.5341, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.4797, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.9273, Batch Acc: 0.1250\n",
      "[Batch 100/501] Loss: 1.6365, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 1.7114, Batch Acc: 0.1250\n",
      "[Batch 120/501] Loss: 1.6898, Batch Acc: 0.0000\n",
      "[Batch 130/501] Loss: 1.5901, Batch Acc: 0.3750\n",
      "[Batch 140/501] Loss: 1.6344, Batch Acc: 0.1250\n",
      "[Batch 150/501] Loss: 1.6985, Batch Acc: 0.2500\n",
      "[Batch 160/501] Loss: 1.4024, Batch Acc: 0.3750\n",
      "[Batch 170/501] Loss: 1.7867, Batch Acc: 0.1250\n",
      "[Batch 180/501] Loss: 1.6811, Batch Acc: 0.3750\n",
      "[Batch 190/501] Loss: 1.6177, Batch Acc: 0.2500\n",
      "[Batch 200/501] Loss: 1.7295, Batch Acc: 0.0000\n",
      "[Batch 210/501] Loss: 1.6452, Batch Acc: 0.1250\n",
      "[Batch 220/501] Loss: 1.3157, Batch Acc: 0.5000\n",
      "[Batch 230/501] Loss: 1.7950, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.9180, Batch Acc: 0.0000\n",
      "[Batch 250/501] Loss: 1.5982, Batch Acc: 0.3750\n",
      "[Batch 260/501] Loss: 1.8295, Batch Acc: 0.1250\n",
      "[Batch 270/501] Loss: 1.8702, Batch Acc: 0.2500\n",
      "[Batch 280/501] Loss: 1.4482, Batch Acc: 0.2500\n",
      "[Batch 290/501] Loss: 1.6646, Batch Acc: 0.1250\n",
      "[Batch 300/501] Loss: 1.6740, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 1.6156, Batch Acc: 0.1250\n",
      "[Batch 320/501] Loss: 1.7065, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 1.6735, Batch Acc: 0.2500\n",
      "[Batch 340/501] Loss: 1.7451, Batch Acc: 0.1250\n",
      "[Batch 350/501] Loss: 1.6118, Batch Acc: 0.3750\n",
      "[Batch 360/501] Loss: 1.7648, Batch Acc: 0.0000\n",
      "[Batch 370/501] Loss: 1.6495, Batch Acc: 0.3750\n",
      "[Batch 380/501] Loss: 1.5796, Batch Acc: 0.2500\n",
      "[Batch 390/501] Loss: 1.4744, Batch Acc: 0.2500\n",
      "[Batch 400/501] Loss: 1.5186, Batch Acc: 0.1250\n",
      "[Batch 410/501] Loss: 1.8112, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.4944, Batch Acc: 0.5000\n",
      "[Batch 430/501] Loss: 1.5707, Batch Acc: 0.3750\n",
      "[Batch 440/501] Loss: 1.7477, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.6113, Batch Acc: 0.3750\n",
      "[Batch 460/501] Loss: 1.6570, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.5545, Batch Acc: 0.3750\n",
      "[Batch 480/501] Loss: 1.5301, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.5510, Batch Acc: 0.2500\n",
      "[Batch 500/501] Loss: 1.6391, Batch Acc: 0.2500\n",
      "Epoch 1 Summary - Loss: 819.2279, Train Accuracy: 0.2617\n",
      "[Batch 10/501] Loss: 1.7272, Batch Acc: 0.5000\n",
      "[Batch 20/501] Loss: 1.6035, Batch Acc: 0.1250\n",
      "[Batch 30/501] Loss: 1.8481, Batch Acc: 0.1250\n",
      "[Batch 40/501] Loss: 1.6198, Batch Acc: 0.2500\n",
      "[Batch 50/501] Loss: 1.4484, Batch Acc: 0.5000\n",
      "[Batch 60/501] Loss: 1.5497, Batch Acc: 0.2500\n",
      "[Batch 70/501] Loss: 1.6722, Batch Acc: 0.1250\n",
      "[Batch 80/501] Loss: 1.6030, Batch Acc: 0.5000\n",
      "[Batch 90/501] Loss: 1.5630, Batch Acc: 0.2500\n",
      "[Batch 100/501] Loss: 1.7267, Batch Acc: 0.3750\n",
      "[Batch 110/501] Loss: 1.4602, Batch Acc: 0.3750\n",
      "[Batch 120/501] Loss: 1.6562, Batch Acc: 0.3750\n",
      "[Batch 130/501] Loss: 1.9129, Batch Acc: 0.0000\n",
      "[Batch 140/501] Loss: 1.1545, Batch Acc: 0.6250\n",
      "[Batch 150/501] Loss: 1.7991, Batch Acc: 0.0000\n",
      "[Batch 160/501] Loss: 1.4312, Batch Acc: 0.3750\n",
      "[Batch 170/501] Loss: 1.5852, Batch Acc: 0.3750\n",
      "[Batch 180/501] Loss: 1.6037, Batch Acc: 0.3750\n",
      "[Batch 190/501] Loss: 1.7378, Batch Acc: 0.1250\n",
      "[Batch 200/501] Loss: 1.2164, Batch Acc: 0.6250\n",
      "[Batch 210/501] Loss: 1.7413, Batch Acc: 0.2500\n",
      "[Batch 220/501] Loss: 1.9081, Batch Acc: 0.0000\n",
      "[Batch 230/501] Loss: 1.7503, Batch Acc: 0.1250\n",
      "[Batch 240/501] Loss: 1.6943, Batch Acc: 0.1250\n",
      "[Batch 250/501] Loss: 1.3881, Batch Acc: 0.5000\n",
      "[Batch 260/501] Loss: 1.4155, Batch Acc: 0.3750\n",
      "[Batch 270/501] Loss: 1.8306, Batch Acc: 0.2500\n",
      "[Batch 280/501] Loss: 1.6465, Batch Acc: 0.2500\n",
      "[Batch 290/501] Loss: 1.6745, Batch Acc: 0.2500\n",
      "[Batch 300/501] Loss: 1.8832, Batch Acc: 0.0000\n",
      "[Batch 310/501] Loss: 1.6773, Batch Acc: 0.1250\n",
      "[Batch 320/501] Loss: 1.8853, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 1.5084, Batch Acc: 0.3750\n",
      "[Batch 340/501] Loss: 1.4041, Batch Acc: 0.5000\n",
      "[Batch 350/501] Loss: 1.6680, Batch Acc: 0.1250\n",
      "[Batch 360/501] Loss: 1.7316, Batch Acc: 0.1250\n",
      "[Batch 370/501] Loss: 1.8545, Batch Acc: 0.2500\n",
      "[Batch 380/501] Loss: 1.8657, Batch Acc: 0.1250\n",
      "[Batch 390/501] Loss: 1.5422, Batch Acc: 0.2500\n",
      "[Batch 400/501] Loss: 1.3779, Batch Acc: 0.5000\n",
      "[Batch 410/501] Loss: 1.6703, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.8492, Batch Acc: 0.0000\n",
      "[Batch 430/501] Loss: 1.5621, Batch Acc: 0.3750\n",
      "[Batch 440/501] Loss: 1.6870, Batch Acc: 0.1250\n",
      "[Batch 450/501] Loss: 1.4402, Batch Acc: 0.3750\n",
      "[Batch 460/501] Loss: 1.7735, Batch Acc: 0.1250\n",
      "[Batch 470/501] Loss: 1.5571, Batch Acc: 0.5000\n",
      "[Batch 480/501] Loss: 1.5391, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.4393, Batch Acc: 0.5000\n",
      "[Batch 500/501] Loss: 1.8530, Batch Acc: 0.0000\n",
      "Epoch 2 Summary - Loss: 814.4028, Train Accuracy: 0.2687\n",
      "[Batch 10/501] Loss: 1.6821, Batch Acc: 0.1250\n",
      "[Batch 20/501] Loss: 1.3815, Batch Acc: 0.5000\n",
      "[Batch 30/501] Loss: 1.5683, Batch Acc: 0.3750\n",
      "[Batch 40/501] Loss: 1.8391, Batch Acc: 0.1250\n",
      "[Batch 50/501] Loss: 1.5092, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.7252, Batch Acc: 0.1250\n",
      "[Batch 70/501] Loss: 1.7289, Batch Acc: 0.1250\n",
      "[Batch 80/501] Loss: 1.4045, Batch Acc: 0.5000\n",
      "[Batch 90/501] Loss: 1.5268, Batch Acc: 0.1250\n",
      "[Batch 100/501] Loss: 1.7666, Batch Acc: 0.1250\n",
      "[Batch 110/501] Loss: 1.4587, Batch Acc: 0.2500\n",
      "[Batch 120/501] Loss: 1.8177, Batch Acc: 0.0000\n",
      "[Batch 130/501] Loss: 1.7103, Batch Acc: 0.1250\n",
      "[Batch 140/501] Loss: 1.3771, Batch Acc: 0.5000\n",
      "[Batch 150/501] Loss: 1.6410, Batch Acc: 0.2500\n",
      "[Batch 160/501] Loss: 1.6491, Batch Acc: 0.3750\n",
      "[Batch 170/501] Loss: 1.8495, Batch Acc: 0.0000\n",
      "[Batch 180/501] Loss: 1.8252, Batch Acc: 0.0000\n",
      "[Batch 190/501] Loss: 1.6253, Batch Acc: 0.2500\n",
      "[Batch 200/501] Loss: 1.6045, Batch Acc: 0.3750\n",
      "[Batch 210/501] Loss: 1.6821, Batch Acc: 0.3750\n",
      "[Batch 220/501] Loss: 1.4853, Batch Acc: 0.2500\n",
      "[Batch 230/501] Loss: 1.4558, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.7327, Batch Acc: 0.2500\n",
      "[Batch 250/501] Loss: 1.7750, Batch Acc: 0.2500\n",
      "[Batch 260/501] Loss: 1.6068, Batch Acc: 0.1250\n",
      "[Batch 270/501] Loss: 1.7704, Batch Acc: 0.1250\n",
      "[Batch 280/501] Loss: 1.6318, Batch Acc: 0.2500\n",
      "[Batch 290/501] Loss: 1.5291, Batch Acc: 0.3750\n",
      "[Batch 300/501] Loss: 1.6492, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 1.6862, Batch Acc: 0.2500\n",
      "[Batch 320/501] Loss: 1.4846, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 1.6525, Batch Acc: 0.2500\n",
      "[Batch 340/501] Loss: 1.3032, Batch Acc: 0.7500\n",
      "[Batch 350/501] Loss: 1.2938, Batch Acc: 0.6250\n",
      "[Batch 360/501] Loss: 1.7571, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 1.5723, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.2573, Batch Acc: 0.7500\n",
      "[Batch 390/501] Loss: 1.8630, Batch Acc: 0.1250\n",
      "[Batch 400/501] Loss: 1.5147, Batch Acc: 0.2500\n",
      "[Batch 410/501] Loss: 1.7278, Batch Acc: 0.1250\n",
      "[Batch 420/501] Loss: 1.4258, Batch Acc: 0.3750\n",
      "[Batch 430/501] Loss: 1.8217, Batch Acc: 0.2500\n",
      "[Batch 440/501] Loss: 1.6389, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.5719, Batch Acc: 0.1250\n",
      "[Batch 460/501] Loss: 1.6668, Batch Acc: 0.1250\n",
      "[Batch 470/501] Loss: 1.6302, Batch Acc: 0.2500\n",
      "[Batch 480/501] Loss: 1.8375, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.8150, Batch Acc: 0.0000\n",
      "[Batch 500/501] Loss: 1.7427, Batch Acc: 0.1250\n",
      "Epoch 3 Summary - Loss: 824.3151, Train Accuracy: 0.2540\n",
      "[Batch 10/501] Loss: 1.6972, Batch Acc: 0.2500\n",
      "[Batch 20/501] Loss: 1.6520, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.3883, Batch Acc: 0.2500\n",
      "[Batch 40/501] Loss: 1.5829, Batch Acc: 0.3750\n",
      "[Batch 50/501] Loss: 1.5167, Batch Acc: 0.3750\n",
      "[Batch 60/501] Loss: 1.3477, Batch Acc: 0.6250\n",
      "[Batch 70/501] Loss: 1.5058, Batch Acc: 0.5000\n",
      "[Batch 80/501] Loss: 1.6004, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.3108, Batch Acc: 0.6250\n",
      "[Batch 100/501] Loss: 1.7760, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 1.7936, Batch Acc: 0.0000\n",
      "[Batch 120/501] Loss: 1.7943, Batch Acc: 0.3750\n",
      "[Batch 130/501] Loss: 1.6562, Batch Acc: 0.1250\n",
      "[Batch 140/501] Loss: 1.6299, Batch Acc: 0.1250\n",
      "[Batch 150/501] Loss: 1.5601, Batch Acc: 0.2500\n",
      "[Batch 160/501] Loss: 1.4663, Batch Acc: 0.3750\n",
      "[Batch 170/501] Loss: 1.7490, Batch Acc: 0.2500\n",
      "[Batch 180/501] Loss: 1.4205, Batch Acc: 0.3750\n",
      "[Batch 190/501] Loss: 1.6546, Batch Acc: 0.3750\n",
      "[Batch 200/501] Loss: 1.6941, Batch Acc: 0.1250\n",
      "[Batch 210/501] Loss: 1.7672, Batch Acc: 0.1250\n",
      "[Batch 220/501] Loss: 1.5765, Batch Acc: 0.2500\n",
      "[Batch 230/501] Loss: 1.4972, Batch Acc: 0.1250\n",
      "[Batch 240/501] Loss: 1.5365, Batch Acc: 0.3750\n",
      "[Batch 250/501] Loss: 1.5671, Batch Acc: 0.1250\n",
      "[Batch 260/501] Loss: 1.7379, Batch Acc: 0.2500\n",
      "[Batch 270/501] Loss: 1.7496, Batch Acc: 0.2500\n",
      "[Batch 280/501] Loss: 1.3506, Batch Acc: 0.3750\n",
      "[Batch 290/501] Loss: 1.5022, Batch Acc: 0.2500\n",
      "[Batch 300/501] Loss: 1.5708, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 1.5572, Batch Acc: 0.2500\n",
      "[Batch 320/501] Loss: 1.3157, Batch Acc: 0.5000\n",
      "[Batch 330/501] Loss: 1.8462, Batch Acc: 0.0000\n",
      "[Batch 340/501] Loss: 1.6380, Batch Acc: 0.1250\n",
      "[Batch 350/501] Loss: 1.5665, Batch Acc: 0.2500\n",
      "[Batch 360/501] Loss: 1.7502, Batch Acc: 0.1250\n",
      "[Batch 370/501] Loss: 1.7064, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.5787, Batch Acc: 0.2500\n",
      "[Batch 390/501] Loss: 1.5866, Batch Acc: 0.2500\n",
      "[Batch 400/501] Loss: 1.7547, Batch Acc: 0.2500\n",
      "[Batch 410/501] Loss: 1.9139, Batch Acc: 0.0000\n",
      "[Batch 420/501] Loss: 1.8981, Batch Acc: 0.1250\n",
      "[Batch 430/501] Loss: 1.5406, Batch Acc: 0.3750\n",
      "[Batch 440/501] Loss: 1.7731, Batch Acc: 0.1250\n",
      "[Batch 450/501] Loss: 1.8101, Batch Acc: 0.1250\n",
      "[Batch 460/501] Loss: 1.6535, Batch Acc: 0.1250\n",
      "[Batch 470/501] Loss: 1.5965, Batch Acc: 0.2500\n",
      "[Batch 480/501] Loss: 1.6961, Batch Acc: 0.1250\n",
      "[Batch 490/501] Loss: 1.6337, Batch Acc: 0.2500\n",
      "[Batch 500/501] Loss: 1.4984, Batch Acc: 0.5000\n",
      "Epoch 4 Summary - Loss: 816.4599, Train Accuracy: 0.2580\n",
      "[Batch 10/501] Loss: 1.7231, Batch Acc: 0.1250\n",
      "[Batch 20/501] Loss: 1.8195, Batch Acc: 0.1250\n",
      "[Batch 30/501] Loss: 1.7224, Batch Acc: 0.2500\n",
      "[Batch 40/501] Loss: 1.4318, Batch Acc: 0.3750\n",
      "[Batch 50/501] Loss: 1.5662, Batch Acc: 0.3750\n",
      "[Batch 60/501] Loss: 1.2772, Batch Acc: 0.3750\n",
      "[Batch 70/501] Loss: 1.8604, Batch Acc: 0.0000\n",
      "[Batch 80/501] Loss: 1.2371, Batch Acc: 0.5000\n",
      "[Batch 90/501] Loss: 1.5813, Batch Acc: 0.2500\n",
      "[Batch 100/501] Loss: 1.4927, Batch Acc: 0.1250\n",
      "[Batch 110/501] Loss: 1.3835, Batch Acc: 0.3750\n",
      "[Batch 120/501] Loss: 1.6086, Batch Acc: 0.1250\n",
      "[Batch 130/501] Loss: 1.6303, Batch Acc: 0.2500\n",
      "[Batch 140/501] Loss: 1.5325, Batch Acc: 0.5000\n",
      "[Batch 150/501] Loss: 1.7937, Batch Acc: 0.1250\n",
      "[Batch 160/501] Loss: 1.7052, Batch Acc: 0.2500\n",
      "[Batch 170/501] Loss: 1.8776, Batch Acc: 0.0000\n",
      "[Batch 180/501] Loss: 1.5862, Batch Acc: 0.2500\n",
      "[Batch 190/501] Loss: 1.5587, Batch Acc: 0.3750\n",
      "[Batch 200/501] Loss: 1.6310, Batch Acc: 0.5000\n",
      "[Batch 210/501] Loss: 1.6872, Batch Acc: 0.6250\n",
      "[Batch 220/501] Loss: 1.8478, Batch Acc: 0.1250\n",
      "[Batch 230/501] Loss: 1.5161, Batch Acc: 0.3750\n",
      "[Batch 240/501] Loss: 1.4359, Batch Acc: 0.6250\n",
      "[Batch 250/501] Loss: 1.8271, Batch Acc: 0.1250\n",
      "[Batch 260/501] Loss: 1.5615, Batch Acc: 0.3750\n",
      "[Batch 270/501] Loss: 1.7206, Batch Acc: 0.1250\n",
      "[Batch 280/501] Loss: 1.3036, Batch Acc: 0.6250\n",
      "[Batch 290/501] Loss: 1.6414, Batch Acc: 0.1250\n",
      "[Batch 300/501] Loss: 1.4931, Batch Acc: 0.3750\n",
      "[Batch 310/501] Loss: 1.6915, Batch Acc: 0.1250\n",
      "[Batch 320/501] Loss: 1.6176, Batch Acc: 0.1250\n",
      "[Batch 330/501] Loss: 1.8502, Batch Acc: 0.1250\n",
      "[Batch 340/501] Loss: 1.3842, Batch Acc: 0.2500\n",
      "[Batch 350/501] Loss: 1.6263, Batch Acc: 0.2500\n",
      "[Batch 360/501] Loss: 1.6862, Batch Acc: 0.1250\n",
      "[Batch 370/501] Loss: 1.8838, Batch Acc: 0.0000\n",
      "[Batch 380/501] Loss: 1.4818, Batch Acc: 0.3750\n",
      "[Batch 390/501] Loss: 1.4721, Batch Acc: 0.3750\n",
      "[Batch 400/501] Loss: 1.3139, Batch Acc: 0.3750\n",
      "[Batch 410/501] Loss: 1.5124, Batch Acc: 0.3750\n",
      "[Batch 420/501] Loss: 1.4478, Batch Acc: 0.3750\n",
      "[Batch 430/501] Loss: 1.9216, Batch Acc: 0.0000\n",
      "[Batch 440/501] Loss: 1.6576, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.5461, Batch Acc: 0.3750\n",
      "[Batch 460/501] Loss: 1.7362, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.7846, Batch Acc: 0.2500\n",
      "[Batch 480/501] Loss: 1.5572, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.4935, Batch Acc: 0.5000\n",
      "[Batch 500/501] Loss: 1.6201, Batch Acc: 0.2500\n",
      "Epoch 5 Summary - Loss: 815.5814, Train Accuracy: 0.2620\n",
      "[Batch 10/501] Loss: 1.9517, Batch Acc: 0.0000\n",
      "[Batch 20/501] Loss: 1.4858, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.5268, Batch Acc: 0.3750\n",
      "[Batch 40/501] Loss: 1.6503, Batch Acc: 0.3750\n",
      "[Batch 50/501] Loss: 1.9004, Batch Acc: 0.1250\n",
      "[Batch 60/501] Loss: 1.6291, Batch Acc: 0.2500\n",
      "[Batch 70/501] Loss: 1.5037, Batch Acc: 0.3750\n",
      "[Batch 80/501] Loss: 1.5302, Batch Acc: 0.3750\n",
      "[Batch 90/501] Loss: 1.5599, Batch Acc: 0.2500\n",
      "[Batch 100/501] Loss: 1.5847, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 1.5500, Batch Acc: 0.2500\n",
      "[Batch 120/501] Loss: 1.3451, Batch Acc: 0.5000\n",
      "[Batch 130/501] Loss: 1.5442, Batch Acc: 0.3750\n",
      "[Batch 140/501] Loss: 1.9080, Batch Acc: 0.2500\n",
      "[Batch 150/501] Loss: 1.5676, Batch Acc: 0.3750\n",
      "[Batch 160/501] Loss: 1.3816, Batch Acc: 0.5000\n",
      "[Batch 170/501] Loss: 1.4388, Batch Acc: 0.3750\n",
      "[Batch 180/501] Loss: 1.6718, Batch Acc: 0.2500\n",
      "[Batch 190/501] Loss: 1.5804, Batch Acc: 0.2500\n",
      "[Batch 200/501] Loss: 1.3410, Batch Acc: 0.5000\n",
      "[Batch 210/501] Loss: 1.6877, Batch Acc: 0.3750\n",
      "[Batch 220/501] Loss: 1.6926, Batch Acc: 0.3750\n",
      "[Batch 230/501] Loss: 1.5818, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.6836, Batch Acc: 0.1250\n",
      "[Batch 250/501] Loss: 1.4342, Batch Acc: 0.3750\n",
      "[Batch 260/501] Loss: 1.3852, Batch Acc: 0.3750\n",
      "[Batch 270/501] Loss: 1.5725, Batch Acc: 0.2500\n",
      "[Batch 280/501] Loss: 1.6060, Batch Acc: 0.3750\n",
      "[Batch 290/501] Loss: 1.5999, Batch Acc: 0.3750\n",
      "[Batch 300/501] Loss: 1.6606, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 1.5693, Batch Acc: 0.3750\n",
      "[Batch 320/501] Loss: 1.5345, Batch Acc: 0.3750\n",
      "[Batch 330/501] Loss: 1.7600, Batch Acc: 0.2500\n",
      "[Batch 340/501] Loss: 1.6787, Batch Acc: 0.2500\n",
      "[Batch 350/501] Loss: 1.4812, Batch Acc: 0.2500\n",
      "[Batch 360/501] Loss: 1.4038, Batch Acc: 0.5000\n",
      "[Batch 370/501] Loss: 1.5121, Batch Acc: 0.3750\n",
      "[Batch 380/501] Loss: 1.7044, Batch Acc: 0.1250\n",
      "[Batch 390/501] Loss: 1.5461, Batch Acc: 0.2500\n",
      "[Batch 400/501] Loss: 1.6661, Batch Acc: 0.1250\n",
      "[Batch 410/501] Loss: 1.8206, Batch Acc: 0.1250\n",
      "[Batch 420/501] Loss: 1.6284, Batch Acc: 0.2500\n",
      "[Batch 430/501] Loss: 1.4687, Batch Acc: 0.5000\n",
      "[Batch 440/501] Loss: 1.8497, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.5942, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.7451, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.2851, Batch Acc: 0.7500\n",
      "[Batch 480/501] Loss: 1.7991, Batch Acc: 0.0000\n",
      "[Batch 490/501] Loss: 1.7917, Batch Acc: 0.1250\n",
      "[Batch 500/501] Loss: 1.6402, Batch Acc: 0.2500\n",
      "Epoch 6 Summary - Loss: 820.3223, Train Accuracy: 0.2645\n",
      "[Batch 10/501] Loss: 1.5088, Batch Acc: 0.5000\n",
      "[Batch 20/501] Loss: 1.6346, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.4951, Batch Acc: 0.5000\n",
      "[Batch 40/501] Loss: 1.5864, Batch Acc: 0.2500\n",
      "[Batch 50/501] Loss: 1.4180, Batch Acc: 0.6250\n",
      "[Batch 60/501] Loss: 1.4951, Batch Acc: 0.3750\n",
      "[Batch 70/501] Loss: 1.5939, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.7469, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.4867, Batch Acc: 0.5000\n",
      "[Batch 100/501] Loss: 1.7839, Batch Acc: 0.3750\n",
      "[Batch 110/501] Loss: 1.5452, Batch Acc: 0.2500\n",
      "[Batch 120/501] Loss: 1.9539, Batch Acc: 0.0000\n",
      "[Batch 130/501] Loss: 1.8197, Batch Acc: 0.1250\n",
      "[Batch 140/501] Loss: 1.3636, Batch Acc: 0.5000\n",
      "[Batch 150/501] Loss: 1.4825, Batch Acc: 0.5000\n",
      "[Batch 160/501] Loss: 1.6244, Batch Acc: 0.1250\n",
      "[Batch 170/501] Loss: 1.5429, Batch Acc: 0.2500\n",
      "[Batch 180/501] Loss: 1.6258, Batch Acc: 0.3750\n",
      "[Batch 190/501] Loss: 1.5135, Batch Acc: 0.3750\n",
      "[Batch 200/501] Loss: 1.4401, Batch Acc: 0.3750\n",
      "[Batch 210/501] Loss: 1.4623, Batch Acc: 0.3750\n",
      "[Batch 220/501] Loss: 1.6700, Batch Acc: 0.2500\n",
      "[Batch 230/501] Loss: 1.6010, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.2466, Batch Acc: 0.5000\n",
      "[Batch 250/501] Loss: 1.7181, Batch Acc: 0.1250\n",
      "[Batch 260/501] Loss: 1.4829, Batch Acc: 0.3750\n",
      "[Batch 270/501] Loss: 1.5951, Batch Acc: 0.1250\n",
      "[Batch 280/501] Loss: 1.7466, Batch Acc: 0.0000\n",
      "[Batch 290/501] Loss: 1.3611, Batch Acc: 0.5000\n",
      "[Batch 300/501] Loss: 1.4406, Batch Acc: 0.3750\n",
      "[Batch 310/501] Loss: 1.6703, Batch Acc: 0.2500\n",
      "[Batch 320/501] Loss: 1.7506, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 1.5954, Batch Acc: 0.3750\n",
      "[Batch 340/501] Loss: 1.5251, Batch Acc: 0.3750\n",
      "[Batch 350/501] Loss: 1.5944, Batch Acc: 0.2500\n",
      "[Batch 360/501] Loss: 1.8553, Batch Acc: 0.1250\n",
      "[Batch 370/501] Loss: 1.9791, Batch Acc: 0.0000\n",
      "[Batch 380/501] Loss: 1.6510, Batch Acc: 0.1250\n",
      "[Batch 390/501] Loss: 1.4427, Batch Acc: 0.6250\n",
      "[Batch 400/501] Loss: 1.7019, Batch Acc: 0.1250\n",
      "[Batch 410/501] Loss: 1.5171, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.6288, Batch Acc: 0.1250\n",
      "[Batch 430/501] Loss: 1.7412, Batch Acc: 0.1250\n",
      "[Batch 440/501] Loss: 1.6439, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.5909, Batch Acc: 0.3750\n",
      "[Batch 460/501] Loss: 1.7652, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.4517, Batch Acc: 0.3750\n",
      "[Batch 480/501] Loss: 1.5712, Batch Acc: 0.1250\n",
      "[Batch 490/501] Loss: 1.7842, Batch Acc: 0.0000\n",
      "[Batch 500/501] Loss: 1.6518, Batch Acc: 0.2500\n",
      "Epoch 7 Summary - Loss: 820.6994, Train Accuracy: 0.2597\n",
      "[Batch 10/501] Loss: 1.7440, Batch Acc: 0.2500\n",
      "[Batch 20/501] Loss: 1.9135, Batch Acc: 0.1250\n",
      "[Batch 30/501] Loss: 1.6111, Batch Acc: 0.2500\n",
      "[Batch 40/501] Loss: 1.8059, Batch Acc: 0.1250\n",
      "[Batch 50/501] Loss: 1.5535, Batch Acc: 0.3750\n",
      "[Batch 60/501] Loss: 1.5917, Batch Acc: 0.2500\n",
      "[Batch 70/501] Loss: 1.6148, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.5157, Batch Acc: 0.3750\n",
      "[Batch 90/501] Loss: 1.5349, Batch Acc: 0.3750\n",
      "[Batch 100/501] Loss: 1.5386, Batch Acc: 0.1250\n",
      "[Batch 110/501] Loss: 1.7360, Batch Acc: 0.1250\n",
      "[Batch 120/501] Loss: 1.3934, Batch Acc: 0.5000\n",
      "[Batch 130/501] Loss: 1.7368, Batch Acc: 0.2500\n",
      "[Batch 140/501] Loss: 1.7657, Batch Acc: 0.1250\n",
      "[Batch 150/501] Loss: 1.7175, Batch Acc: 0.2500\n",
      "[Batch 160/501] Loss: 1.5738, Batch Acc: 0.2500\n",
      "[Batch 170/501] Loss: 1.8319, Batch Acc: 0.0000\n",
      "[Batch 180/501] Loss: 1.9300, Batch Acc: 0.0000\n",
      "[Batch 190/501] Loss: 1.5700, Batch Acc: 0.2500\n",
      "[Batch 200/501] Loss: 1.7675, Batch Acc: 0.0000\n",
      "[Batch 210/501] Loss: 1.9050, Batch Acc: 0.1250\n",
      "[Batch 220/501] Loss: 1.5795, Batch Acc: 0.3750\n",
      "[Batch 230/501] Loss: 1.6752, Batch Acc: 0.1250\n",
      "[Batch 240/501] Loss: 1.9535, Batch Acc: 0.1250\n",
      "[Batch 250/501] Loss: 1.5022, Batch Acc: 0.2500\n",
      "[Batch 260/501] Loss: 1.4757, Batch Acc: 0.5000\n",
      "[Batch 270/501] Loss: 1.5075, Batch Acc: 0.3750\n",
      "[Batch 280/501] Loss: 1.5785, Batch Acc: 0.2500\n",
      "[Batch 290/501] Loss: 1.6890, Batch Acc: 0.2500\n",
      "[Batch 300/501] Loss: 1.6870, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 1.7054, Batch Acc: 0.2500\n",
      "[Batch 320/501] Loss: 1.5821, Batch Acc: 0.3750\n",
      "[Batch 330/501] Loss: 1.4369, Batch Acc: 0.3750\n",
      "[Batch 340/501] Loss: 1.7179, Batch Acc: 0.2500\n",
      "[Batch 350/501] Loss: 1.4792, Batch Acc: 0.2500\n",
      "[Batch 360/501] Loss: 1.3972, Batch Acc: 0.5000\n",
      "[Batch 370/501] Loss: 1.9380, Batch Acc: 0.0000\n",
      "[Batch 380/501] Loss: 1.3831, Batch Acc: 0.3750\n",
      "[Batch 390/501] Loss: 1.6803, Batch Acc: 0.2500\n",
      "[Batch 400/501] Loss: 1.4560, Batch Acc: 0.3750\n",
      "[Batch 410/501] Loss: 1.6498, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.6453, Batch Acc: 0.2500\n",
      "[Batch 430/501] Loss: 1.6500, Batch Acc: 0.2500\n",
      "[Batch 440/501] Loss: 1.5860, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.8137, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.6855, Batch Acc: 0.3750\n",
      "[Batch 470/501] Loss: 1.4551, Batch Acc: 0.3750\n",
      "[Batch 480/501] Loss: 1.6015, Batch Acc: 0.1250\n",
      "[Batch 490/501] Loss: 1.4506, Batch Acc: 0.3750\n",
      "[Batch 500/501] Loss: 1.5562, Batch Acc: 0.2500\n",
      "Epoch 8 Summary - Loss: 819.7715, Train Accuracy: 0.2622\n",
      "[Batch 10/501] Loss: 1.5906, Batch Acc: 0.1250\n",
      "[Batch 20/501] Loss: 1.3363, Batch Acc: 0.5000\n",
      "[Batch 30/501] Loss: 1.5912, Batch Acc: 0.2500\n",
      "[Batch 40/501] Loss: 1.6877, Batch Acc: 0.2500\n",
      "[Batch 50/501] Loss: 1.9105, Batch Acc: 0.1250\n",
      "[Batch 60/501] Loss: 1.7553, Batch Acc: 0.2500\n",
      "[Batch 70/501] Loss: 1.6781, Batch Acc: 0.3750\n",
      "[Batch 80/501] Loss: 1.5517, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.4847, Batch Acc: 0.2500\n",
      "[Batch 100/501] Loss: 1.6521, Batch Acc: 0.1250\n",
      "[Batch 110/501] Loss: 1.4867, Batch Acc: 0.2500\n",
      "[Batch 120/501] Loss: 1.6833, Batch Acc: 0.0000\n",
      "[Batch 130/501] Loss: 1.5744, Batch Acc: 0.3750\n",
      "[Batch 140/501] Loss: 1.4222, Batch Acc: 0.3750\n",
      "[Batch 150/501] Loss: 1.9244, Batch Acc: 0.0000\n",
      "[Batch 160/501] Loss: 1.8580, Batch Acc: 0.1250\n",
      "[Batch 170/501] Loss: 1.9004, Batch Acc: 0.0000\n",
      "[Batch 180/501] Loss: 1.3444, Batch Acc: 0.3750\n",
      "[Batch 190/501] Loss: 1.6070, Batch Acc: 0.3750\n",
      "[Batch 200/501] Loss: 1.9886, Batch Acc: 0.1250\n",
      "[Batch 210/501] Loss: 1.6849, Batch Acc: 0.1250\n",
      "[Batch 220/501] Loss: 1.6574, Batch Acc: 0.2500\n",
      "[Batch 230/501] Loss: 1.8440, Batch Acc: 0.1250\n",
      "[Batch 240/501] Loss: 1.5252, Batch Acc: 0.1250\n",
      "[Batch 250/501] Loss: 1.7412, Batch Acc: 0.2500\n",
      "[Batch 260/501] Loss: 1.8004, Batch Acc: 0.1250\n",
      "[Batch 270/501] Loss: 1.5209, Batch Acc: 0.3750\n",
      "[Batch 280/501] Loss: 1.5839, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 1.9151, Batch Acc: 0.1250\n",
      "[Batch 300/501] Loss: 1.4925, Batch Acc: 0.3750\n",
      "[Batch 310/501] Loss: 1.6035, Batch Acc: 0.2500\n",
      "[Batch 320/501] Loss: 1.8528, Batch Acc: 0.0000\n",
      "[Batch 330/501] Loss: 1.4967, Batch Acc: 0.3750\n",
      "[Batch 340/501] Loss: 1.8484, Batch Acc: 0.1250\n",
      "[Batch 350/501] Loss: 1.6176, Batch Acc: 0.3750\n",
      "[Batch 360/501] Loss: 1.3849, Batch Acc: 0.6250\n",
      "[Batch 370/501] Loss: 1.5211, Batch Acc: 0.2500\n",
      "[Batch 380/501] Loss: 1.6650, Batch Acc: 0.2500\n",
      "[Batch 390/501] Loss: 1.6005, Batch Acc: 0.2500\n",
      "[Batch 400/501] Loss: 1.8443, Batch Acc: 0.1250\n",
      "[Batch 410/501] Loss: 1.7883, Batch Acc: 0.1250\n",
      "[Batch 420/501] Loss: 1.3594, Batch Acc: 0.5000\n",
      "[Batch 430/501] Loss: 1.8200, Batch Acc: 0.0000\n",
      "[Batch 440/501] Loss: 2.0266, Batch Acc: 0.1250\n",
      "[Batch 450/501] Loss: 1.8680, Batch Acc: 0.1250\n",
      "[Batch 460/501] Loss: 1.5973, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.3042, Batch Acc: 0.5000\n",
      "[Batch 480/501] Loss: 1.4268, Batch Acc: 0.5000\n",
      "[Batch 490/501] Loss: 1.7189, Batch Acc: 0.3750\n",
      "[Batch 500/501] Loss: 1.4395, Batch Acc: 0.2500\n",
      "Epoch 9 Summary - Loss: 817.3940, Train Accuracy: 0.2632\n",
      "[Batch 10/501] Loss: 1.6173, Batch Acc: 0.1250\n",
      "[Batch 20/501] Loss: 1.6633, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.6408, Batch Acc: 0.2500\n",
      "[Batch 40/501] Loss: 1.2917, Batch Acc: 0.6250\n",
      "[Batch 50/501] Loss: 1.5652, Batch Acc: 0.3750\n",
      "[Batch 60/501] Loss: 1.5660, Batch Acc: 0.3750\n",
      "[Batch 70/501] Loss: 1.4279, Batch Acc: 0.5000\n",
      "[Batch 80/501] Loss: 1.5506, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.6525, Batch Acc: 0.1250\n",
      "[Batch 100/501] Loss: 1.6906, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 1.7481, Batch Acc: 0.0000\n",
      "[Batch 120/501] Loss: 1.6451, Batch Acc: 0.2500\n",
      "[Batch 130/501] Loss: 1.2632, Batch Acc: 0.5000\n",
      "[Batch 140/501] Loss: 1.4463, Batch Acc: 0.3750\n",
      "[Batch 150/501] Loss: 1.5557, Batch Acc: 0.2500\n",
      "[Batch 160/501] Loss: 1.4423, Batch Acc: 0.3750\n",
      "[Batch 170/501] Loss: 1.7432, Batch Acc: 0.0000\n",
      "[Batch 180/501] Loss: 1.7878, Batch Acc: 0.2500\n",
      "[Batch 190/501] Loss: 1.4324, Batch Acc: 0.3750\n",
      "[Batch 200/501] Loss: 1.5672, Batch Acc: 0.2500\n",
      "[Batch 210/501] Loss: 1.4915, Batch Acc: 0.3750\n",
      "[Batch 220/501] Loss: 1.6869, Batch Acc: 0.1250\n",
      "[Batch 230/501] Loss: 1.8744, Batch Acc: 0.0000\n",
      "[Batch 240/501] Loss: 1.7358, Batch Acc: 0.2500\n",
      "[Batch 250/501] Loss: 1.8172, Batch Acc: 0.1250\n",
      "[Batch 260/501] Loss: 1.8349, Batch Acc: 0.1250\n",
      "[Batch 270/501] Loss: 1.7011, Batch Acc: 0.2500\n",
      "[Batch 280/501] Loss: 1.9730, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 1.6998, Batch Acc: 0.1250\n",
      "[Batch 300/501] Loss: 1.6858, Batch Acc: 0.1250\n",
      "[Batch 310/501] Loss: 1.6667, Batch Acc: 0.2500\n",
      "[Batch 320/501] Loss: 1.6794, Batch Acc: 0.3750\n",
      "[Batch 330/501] Loss: 1.2338, Batch Acc: 0.7500\n",
      "[Batch 340/501] Loss: 1.5815, Batch Acc: 0.3750\n",
      "[Batch 350/501] Loss: 1.4357, Batch Acc: 0.3750\n",
      "[Batch 360/501] Loss: 1.9441, Batch Acc: 0.1250\n",
      "[Batch 370/501] Loss: 1.6941, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.7410, Batch Acc: 0.1250\n",
      "[Batch 390/501] Loss: 1.6995, Batch Acc: 0.2500\n",
      "[Batch 400/501] Loss: 1.9801, Batch Acc: 0.0000\n",
      "[Batch 410/501] Loss: 1.6054, Batch Acc: 0.3750\n",
      "[Batch 420/501] Loss: 1.7931, Batch Acc: 0.0000\n",
      "[Batch 430/501] Loss: 1.6374, Batch Acc: 0.5000\n",
      "[Batch 440/501] Loss: 1.7664, Batch Acc: 0.0000\n",
      "[Batch 450/501] Loss: 1.7982, Batch Acc: 0.1250\n",
      "[Batch 460/501] Loss: 1.4869, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.6552, Batch Acc: 0.2500\n",
      "[Batch 480/501] Loss: 1.6958, Batch Acc: 0.1250\n",
      "[Batch 490/501] Loss: 1.6083, Batch Acc: 0.1250\n",
      "[Batch 500/501] Loss: 1.3528, Batch Acc: 0.6250\n",
      "Epoch 10 Summary - Loss: 816.8152, Train Accuracy: 0.2670\n",
      "Test Accuracy: 0.2809\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_accuracy</td><td>▅█▁▃▅▆▄▅▅▇</td></tr><tr><td>train_loss</td><td>▄▁█▂▂▅▅▅▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>test_accuracy</td><td>0.28091</td></tr><tr><td>test_loss</td><td>206.23727</td></tr><tr><td>train_accuracy</td><td>0.26697</td></tr><tr><td>train_loss</td><td>816.81519</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gentle-sweep-15</strong> at: <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/me0abq6w' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/me0abq6w</a><br> View project at: <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250615_143713-me0abq6w/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: grmgdkal with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_nodes: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ajit.kumar4@happiestminds.com/Documents/Drackula/ComputerVisionFromScratch/wandb/run-20250615_144054-grmgdkal</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/grmgdkal' target=\"_blank\">vivid-sweep-16</a></strong> to <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/grmgdkal' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/grmgdkal</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 10/501] Loss: 1.6625, Batch Acc: 0.1250\n",
      "[Batch 20/501] Loss: 1.9010, Batch Acc: 0.1250\n",
      "[Batch 30/501] Loss: 1.7465, Batch Acc: 0.1250\n",
      "[Batch 40/501] Loss: 1.7834, Batch Acc: 0.1250\n",
      "[Batch 50/501] Loss: 1.4060, Batch Acc: 0.5000\n",
      "[Batch 60/501] Loss: 1.6924, Batch Acc: 0.2500\n",
      "[Batch 70/501] Loss: 1.6963, Batch Acc: 0.3750\n",
      "[Batch 80/501] Loss: 1.3248, Batch Acc: 0.3750\n",
      "[Batch 90/501] Loss: 1.8852, Batch Acc: 0.2500\n",
      "[Batch 100/501] Loss: 1.7041, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 1.5929, Batch Acc: 0.0000\n",
      "[Batch 120/501] Loss: 1.5271, Batch Acc: 0.2500\n",
      "[Batch 130/501] Loss: 1.8167, Batch Acc: 0.3750\n",
      "[Batch 140/501] Loss: 1.4179, Batch Acc: 0.5000\n",
      "[Batch 150/501] Loss: 1.7196, Batch Acc: 0.1250\n",
      "[Batch 160/501] Loss: 1.7613, Batch Acc: 0.3750\n",
      "[Batch 170/501] Loss: 1.6134, Batch Acc: 0.1250\n",
      "[Batch 180/501] Loss: 1.7449, Batch Acc: 0.1250\n",
      "[Batch 190/501] Loss: 1.5810, Batch Acc: 0.3750\n",
      "[Batch 200/501] Loss: 1.7712, Batch Acc: 0.2500\n",
      "[Batch 210/501] Loss: 1.5792, Batch Acc: 0.1250\n",
      "[Batch 220/501] Loss: 1.9846, Batch Acc: 0.1250\n",
      "[Batch 230/501] Loss: 1.4029, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 2.1323, Batch Acc: 0.2500\n",
      "[Batch 250/501] Loss: 1.5877, Batch Acc: 0.2500\n",
      "[Batch 260/501] Loss: 1.4251, Batch Acc: 0.5000\n",
      "[Batch 270/501] Loss: 1.5016, Batch Acc: 0.3750\n",
      "[Batch 280/501] Loss: 1.7265, Batch Acc: 0.5000\n",
      "[Batch 290/501] Loss: 1.8490, Batch Acc: 0.1250\n",
      "[Batch 300/501] Loss: 1.6328, Batch Acc: 0.0000\n",
      "[Batch 310/501] Loss: 1.7562, Batch Acc: 0.2500\n",
      "[Batch 320/501] Loss: 1.8937, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 1.9881, Batch Acc: 0.0000\n",
      "[Batch 340/501] Loss: 1.3337, Batch Acc: 0.5000\n",
      "[Batch 350/501] Loss: 1.5983, Batch Acc: 0.3750\n",
      "[Batch 360/501] Loss: 1.6653, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 1.6514, Batch Acc: 0.2500\n",
      "[Batch 380/501] Loss: 1.7065, Batch Acc: 0.2500\n",
      "[Batch 390/501] Loss: 1.5388, Batch Acc: 0.3750\n",
      "[Batch 400/501] Loss: 1.4509, Batch Acc: 0.2500\n",
      "[Batch 410/501] Loss: 1.5611, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.6406, Batch Acc: 0.5000\n",
      "[Batch 430/501] Loss: 1.3278, Batch Acc: 0.6250\n",
      "[Batch 440/501] Loss: 1.6067, Batch Acc: 0.3750\n",
      "[Batch 450/501] Loss: 1.3436, Batch Acc: 0.3750\n",
      "[Batch 460/501] Loss: 1.7915, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.6290, Batch Acc: 0.2500\n",
      "[Batch 480/501] Loss: 1.4511, Batch Acc: 0.5000\n",
      "[Batch 490/501] Loss: 1.4901, Batch Acc: 0.5000\n",
      "[Batch 500/501] Loss: 1.4552, Batch Acc: 0.3750\n",
      "Epoch 1 Summary - Loss: 825.5134, Train Accuracy: 0.2817\n",
      "[Batch 10/501] Loss: 1.5644, Batch Acc: 0.3750\n",
      "[Batch 20/501] Loss: 1.8634, Batch Acc: 0.1250\n",
      "[Batch 30/501] Loss: 1.5499, Batch Acc: 0.3750\n",
      "[Batch 40/501] Loss: 1.5036, Batch Acc: 0.1250\n",
      "[Batch 50/501] Loss: 1.6218, Batch Acc: 0.0000\n",
      "[Batch 60/501] Loss: 1.3640, Batch Acc: 0.3750\n",
      "[Batch 70/501] Loss: 1.9598, Batch Acc: 0.1250\n",
      "[Batch 80/501] Loss: 1.9447, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.6866, Batch Acc: 0.3750\n",
      "[Batch 100/501] Loss: 1.5237, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 2.0171, Batch Acc: 0.2500\n",
      "[Batch 120/501] Loss: 1.5833, Batch Acc: 0.3750\n",
      "[Batch 130/501] Loss: 1.4745, Batch Acc: 0.3750\n",
      "[Batch 140/501] Loss: 1.9491, Batch Acc: 0.2500\n",
      "[Batch 150/501] Loss: 1.6967, Batch Acc: 0.3750\n",
      "[Batch 160/501] Loss: 1.7078, Batch Acc: 0.2500\n",
      "[Batch 170/501] Loss: 1.6607, Batch Acc: 0.2500\n",
      "[Batch 180/501] Loss: 1.4816, Batch Acc: 0.3750\n",
      "[Batch 190/501] Loss: 2.0121, Batch Acc: 0.0000\n",
      "[Batch 200/501] Loss: 1.5519, Batch Acc: 0.5000\n",
      "[Batch 210/501] Loss: 1.4857, Batch Acc: 0.5000\n",
      "[Batch 220/501] Loss: 1.7410, Batch Acc: 0.2500\n",
      "[Batch 230/501] Loss: 1.4834, Batch Acc: 0.3750\n",
      "[Batch 240/501] Loss: 1.6177, Batch Acc: 0.3750\n",
      "[Batch 250/501] Loss: 1.4725, Batch Acc: 0.1250\n",
      "[Batch 260/501] Loss: 1.4012, Batch Acc: 0.3750\n",
      "[Batch 270/501] Loss: 1.4671, Batch Acc: 0.5000\n",
      "[Batch 280/501] Loss: 1.8041, Batch Acc: 0.3750\n",
      "[Batch 290/501] Loss: 1.6822, Batch Acc: 0.3750\n",
      "[Batch 300/501] Loss: 1.6473, Batch Acc: 0.1250\n",
      "[Batch 310/501] Loss: 1.7952, Batch Acc: 0.2500\n",
      "[Batch 320/501] Loss: 1.5291, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 1.3667, Batch Acc: 0.3750\n",
      "[Batch 340/501] Loss: 1.4824, Batch Acc: 0.2500\n",
      "[Batch 350/501] Loss: 1.5246, Batch Acc: 0.5000\n",
      "[Batch 360/501] Loss: 1.9361, Batch Acc: 0.0000\n",
      "[Batch 370/501] Loss: 1.5757, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.6930, Batch Acc: 0.3750\n",
      "[Batch 390/501] Loss: 1.5885, Batch Acc: 0.3750\n",
      "[Batch 400/501] Loss: 1.8227, Batch Acc: 0.1250\n",
      "[Batch 410/501] Loss: 1.9239, Batch Acc: 0.3750\n",
      "[Batch 420/501] Loss: 1.5413, Batch Acc: 0.5000\n",
      "[Batch 430/501] Loss: 1.8658, Batch Acc: 0.2500\n",
      "[Batch 440/501] Loss: 1.8137, Batch Acc: 0.3750\n",
      "[Batch 450/501] Loss: 2.0413, Batch Acc: 0.1250\n",
      "[Batch 460/501] Loss: 1.9044, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.6526, Batch Acc: 0.2500\n",
      "[Batch 480/501] Loss: 1.5659, Batch Acc: 0.3750\n",
      "[Batch 490/501] Loss: 1.3808, Batch Acc: 0.3750\n",
      "[Batch 500/501] Loss: 1.5025, Batch Acc: 0.3750\n",
      "Epoch 2 Summary - Loss: 818.8756, Train Accuracy: 0.2957\n",
      "[Batch 10/501] Loss: 1.9098, Batch Acc: 0.1250\n",
      "[Batch 20/501] Loss: 1.6462, Batch Acc: 0.6250\n",
      "[Batch 30/501] Loss: 2.0935, Batch Acc: 0.0000\n",
      "[Batch 40/501] Loss: 1.3207, Batch Acc: 0.2500\n",
      "[Batch 50/501] Loss: 1.6143, Batch Acc: 0.1250\n",
      "[Batch 60/501] Loss: 1.7487, Batch Acc: 0.2500\n",
      "[Batch 70/501] Loss: 1.5498, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.3694, Batch Acc: 0.6250\n",
      "[Batch 90/501] Loss: 1.8366, Batch Acc: 0.3750\n",
      "[Batch 100/501] Loss: 1.9219, Batch Acc: 0.1250\n",
      "[Batch 110/501] Loss: 1.7863, Batch Acc: 0.0000\n",
      "[Batch 120/501] Loss: 1.8799, Batch Acc: 0.1250\n",
      "[Batch 130/501] Loss: 1.8271, Batch Acc: 0.1250\n",
      "[Batch 140/501] Loss: 1.5772, Batch Acc: 0.2500\n",
      "[Batch 150/501] Loss: 1.3606, Batch Acc: 0.5000\n",
      "[Batch 160/501] Loss: 1.4603, Batch Acc: 0.3750\n",
      "[Batch 170/501] Loss: 1.6632, Batch Acc: 0.3750\n",
      "[Batch 180/501] Loss: 1.6088, Batch Acc: 0.1250\n",
      "[Batch 190/501] Loss: 1.4135, Batch Acc: 0.2500\n",
      "[Batch 200/501] Loss: 1.9398, Batch Acc: 0.3750\n",
      "[Batch 210/501] Loss: 1.9751, Batch Acc: 0.1250\n",
      "[Batch 220/501] Loss: 1.6361, Batch Acc: 0.2500\n",
      "[Batch 230/501] Loss: 1.7280, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.6000, Batch Acc: 0.5000\n",
      "[Batch 250/501] Loss: 2.1771, Batch Acc: 0.1250\n",
      "[Batch 260/501] Loss: 1.7222, Batch Acc: 0.2500\n",
      "[Batch 270/501] Loss: 2.0461, Batch Acc: 0.1250\n",
      "[Batch 280/501] Loss: 2.0444, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 1.9575, Batch Acc: 0.0000\n",
      "[Batch 300/501] Loss: 1.9242, Batch Acc: 0.1250\n",
      "[Batch 310/501] Loss: 1.4902, Batch Acc: 0.2500\n",
      "[Batch 320/501] Loss: 1.5861, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 2.0600, Batch Acc: 0.2500\n",
      "[Batch 340/501] Loss: 1.8614, Batch Acc: 0.2500\n",
      "[Batch 350/501] Loss: 1.5961, Batch Acc: 0.1250\n",
      "[Batch 360/501] Loss: 1.8682, Batch Acc: 0.1250\n",
      "[Batch 370/501] Loss: 1.8976, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.7198, Batch Acc: 0.1250\n",
      "[Batch 390/501] Loss: 1.4501, Batch Acc: 0.6250\n",
      "[Batch 400/501] Loss: 1.5012, Batch Acc: 0.2500\n",
      "[Batch 410/501] Loss: 1.6629, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.4142, Batch Acc: 0.2500\n",
      "[Batch 430/501] Loss: 1.8493, Batch Acc: 0.0000\n",
      "[Batch 440/501] Loss: 1.5454, Batch Acc: 0.3750\n",
      "[Batch 450/501] Loss: 1.8320, Batch Acc: 0.3750\n",
      "[Batch 460/501] Loss: 1.7204, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.4908, Batch Acc: 0.3750\n",
      "[Batch 480/501] Loss: 1.5395, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.6337, Batch Acc: 0.2500\n",
      "[Batch 500/501] Loss: 1.3055, Batch Acc: 0.3750\n",
      "Epoch 3 Summary - Loss: 828.0876, Train Accuracy: 0.2809\n",
      "[Batch 10/501] Loss: 1.6373, Batch Acc: 0.1250\n",
      "[Batch 20/501] Loss: 1.6421, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.7582, Batch Acc: 0.2500\n",
      "[Batch 40/501] Loss: 1.8295, Batch Acc: 0.3750\n",
      "[Batch 50/501] Loss: 1.6603, Batch Acc: 0.1250\n",
      "[Batch 60/501] Loss: 1.7316, Batch Acc: 0.3750\n",
      "[Batch 70/501] Loss: 1.4872, Batch Acc: 0.3750\n",
      "[Batch 80/501] Loss: 1.8623, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.7369, Batch Acc: 0.0000\n",
      "[Batch 100/501] Loss: 1.6654, Batch Acc: 0.1250\n",
      "[Batch 110/501] Loss: 1.4401, Batch Acc: 0.3750\n",
      "[Batch 120/501] Loss: 1.5818, Batch Acc: 0.2500\n",
      "[Batch 130/501] Loss: 1.7363, Batch Acc: 0.2500\n",
      "[Batch 140/501] Loss: 1.7900, Batch Acc: 0.1250\n",
      "[Batch 150/501] Loss: 1.4199, Batch Acc: 0.2500\n",
      "[Batch 160/501] Loss: 1.5666, Batch Acc: 0.3750\n",
      "[Batch 170/501] Loss: 1.2210, Batch Acc: 0.7500\n",
      "[Batch 180/501] Loss: 1.6446, Batch Acc: 0.5000\n",
      "[Batch 190/501] Loss: 1.6307, Batch Acc: 0.1250\n",
      "[Batch 200/501] Loss: 1.6425, Batch Acc: 0.2500\n",
      "[Batch 210/501] Loss: 1.6237, Batch Acc: 0.3750\n",
      "[Batch 220/501] Loss: 1.5061, Batch Acc: 0.3750\n",
      "[Batch 230/501] Loss: 1.9409, Batch Acc: 0.1250\n",
      "[Batch 240/501] Loss: 1.6955, Batch Acc: 0.0000\n",
      "[Batch 250/501] Loss: 1.8441, Batch Acc: 0.1250\n",
      "[Batch 260/501] Loss: 1.4406, Batch Acc: 0.1250\n",
      "[Batch 270/501] Loss: 1.4896, Batch Acc: 0.2500\n",
      "[Batch 280/501] Loss: 1.3884, Batch Acc: 0.5000\n",
      "[Batch 290/501] Loss: 1.6122, Batch Acc: 0.3750\n",
      "[Batch 300/501] Loss: 1.5807, Batch Acc: 0.1250\n",
      "[Batch 310/501] Loss: 1.8624, Batch Acc: 0.2500\n",
      "[Batch 320/501] Loss: 2.1695, Batch Acc: 0.0000\n",
      "[Batch 330/501] Loss: 1.5887, Batch Acc: 0.2500\n",
      "[Batch 340/501] Loss: 1.7924, Batch Acc: 0.2500\n",
      "[Batch 350/501] Loss: 1.4419, Batch Acc: 0.3750\n",
      "[Batch 360/501] Loss: 1.6869, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 1.6609, Batch Acc: 0.2500\n",
      "[Batch 380/501] Loss: 1.9788, Batch Acc: 0.2500\n",
      "[Batch 390/501] Loss: 1.8878, Batch Acc: 0.2500\n",
      "[Batch 400/501] Loss: 1.2669, Batch Acc: 0.6250\n",
      "[Batch 410/501] Loss: 1.7464, Batch Acc: 0.3750\n",
      "[Batch 420/501] Loss: 1.5969, Batch Acc: 0.3750\n",
      "[Batch 430/501] Loss: 1.7006, Batch Acc: 0.3750\n",
      "[Batch 440/501] Loss: 1.8365, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.6749, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.4927, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.4077, Batch Acc: 0.3750\n",
      "[Batch 480/501] Loss: 1.4818, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.5817, Batch Acc: 0.1250\n",
      "[Batch 500/501] Loss: 1.6966, Batch Acc: 0.1250\n",
      "Epoch 4 Summary - Loss: 822.0949, Train Accuracy: 0.2752\n",
      "[Batch 10/501] Loss: 1.7170, Batch Acc: 0.1250\n",
      "[Batch 20/501] Loss: 1.6968, Batch Acc: 0.0000\n",
      "[Batch 30/501] Loss: 2.0340, Batch Acc: 0.1250\n",
      "[Batch 40/501] Loss: 1.7583, Batch Acc: 0.2500\n",
      "[Batch 50/501] Loss: 1.9380, Batch Acc: 0.3750\n",
      "[Batch 60/501] Loss: 2.1689, Batch Acc: 0.0000\n",
      "[Batch 70/501] Loss: 1.5255, Batch Acc: 0.5000\n",
      "[Batch 80/501] Loss: 1.8747, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.9097, Batch Acc: 0.3750\n",
      "[Batch 100/501] Loss: 1.9189, Batch Acc: 0.1250\n",
      "[Batch 110/501] Loss: 1.6737, Batch Acc: 0.2500\n",
      "[Batch 120/501] Loss: 1.8592, Batch Acc: 0.2500\n",
      "[Batch 130/501] Loss: 1.5211, Batch Acc: 0.3750\n",
      "[Batch 140/501] Loss: 1.7193, Batch Acc: 0.2500\n",
      "[Batch 150/501] Loss: 1.6341, Batch Acc: 0.2500\n",
      "[Batch 160/501] Loss: 1.4569, Batch Acc: 0.2500\n",
      "[Batch 170/501] Loss: 1.4867, Batch Acc: 0.1250\n",
      "[Batch 180/501] Loss: 1.7160, Batch Acc: 0.2500\n",
      "[Batch 190/501] Loss: 1.3628, Batch Acc: 0.2500\n",
      "[Batch 200/501] Loss: 1.5776, Batch Acc: 0.2500\n",
      "[Batch 210/501] Loss: 1.6730, Batch Acc: 0.3750\n",
      "[Batch 220/501] Loss: 2.0779, Batch Acc: 0.3750\n",
      "[Batch 230/501] Loss: 1.4202, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.5988, Batch Acc: 0.3750\n",
      "[Batch 250/501] Loss: 1.2823, Batch Acc: 0.5000\n",
      "[Batch 260/501] Loss: 1.7087, Batch Acc: 0.2500\n",
      "[Batch 270/501] Loss: 2.0570, Batch Acc: 0.1250\n",
      "[Batch 280/501] Loss: 1.6687, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 1.5317, Batch Acc: 0.3750\n",
      "[Batch 300/501] Loss: 1.8108, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 1.7831, Batch Acc: 0.3750\n",
      "[Batch 320/501] Loss: 1.5122, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 1.8804, Batch Acc: 0.1250\n",
      "[Batch 340/501] Loss: 1.6245, Batch Acc: 0.2500\n",
      "[Batch 350/501] Loss: 2.1681, Batch Acc: 0.2500\n",
      "[Batch 360/501] Loss: 1.7242, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 1.8729, Batch Acc: 0.2500\n",
      "[Batch 380/501] Loss: 2.0360, Batch Acc: 0.1250\n",
      "[Batch 390/501] Loss: 1.5819, Batch Acc: 0.5000\n",
      "[Batch 400/501] Loss: 1.7195, Batch Acc: 0.1250\n",
      "[Batch 410/501] Loss: 1.5630, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.6902, Batch Acc: 0.1250\n",
      "[Batch 430/501] Loss: 1.5976, Batch Acc: 0.2500\n",
      "[Batch 440/501] Loss: 1.5653, Batch Acc: 0.1250\n",
      "[Batch 450/501] Loss: 2.0009, Batch Acc: 0.0000\n",
      "[Batch 460/501] Loss: 1.2770, Batch Acc: 0.3750\n",
      "[Batch 470/501] Loss: 1.5369, Batch Acc: 0.5000\n",
      "[Batch 480/501] Loss: 1.5251, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.5200, Batch Acc: 0.0000\n",
      "[Batch 500/501] Loss: 2.2667, Batch Acc: 0.0000\n",
      "Epoch 5 Summary - Loss: 823.3967, Train Accuracy: 0.2779\n",
      "[Batch 10/501] Loss: 1.4044, Batch Acc: 0.3750\n",
      "[Batch 20/501] Loss: 1.6446, Batch Acc: 0.3750\n",
      "[Batch 30/501] Loss: 1.4142, Batch Acc: 0.5000\n",
      "[Batch 40/501] Loss: 1.7392, Batch Acc: 0.2500\n",
      "[Batch 50/501] Loss: 1.6362, Batch Acc: 0.3750\n",
      "[Batch 60/501] Loss: 1.6854, Batch Acc: 0.1250\n",
      "[Batch 70/501] Loss: 1.5022, Batch Acc: 0.3750\n",
      "[Batch 80/501] Loss: 1.6192, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.2632, Batch Acc: 0.6250\n",
      "[Batch 100/501] Loss: 1.4703, Batch Acc: 0.3750\n",
      "[Batch 110/501] Loss: 1.6397, Batch Acc: 0.5000\n",
      "[Batch 120/501] Loss: 1.7744, Batch Acc: 0.2500\n",
      "[Batch 130/501] Loss: 1.7927, Batch Acc: 0.2500\n",
      "[Batch 140/501] Loss: 1.8090, Batch Acc: 0.1250\n",
      "[Batch 150/501] Loss: 1.8362, Batch Acc: 0.0000\n",
      "[Batch 160/501] Loss: 1.7424, Batch Acc: 0.2500\n",
      "[Batch 170/501] Loss: 1.6004, Batch Acc: 0.1250\n",
      "[Batch 180/501] Loss: 1.7907, Batch Acc: 0.3750\n",
      "[Batch 190/501] Loss: 1.7509, Batch Acc: 0.2500\n",
      "[Batch 200/501] Loss: 1.7957, Batch Acc: 0.1250\n",
      "[Batch 210/501] Loss: 1.5428, Batch Acc: 0.3750\n",
      "[Batch 220/501] Loss: 1.5897, Batch Acc: 0.3750\n",
      "[Batch 230/501] Loss: 1.5399, Batch Acc: 0.3750\n",
      "[Batch 240/501] Loss: 1.3938, Batch Acc: 0.1250\n",
      "[Batch 250/501] Loss: 1.4147, Batch Acc: 0.5000\n",
      "[Batch 260/501] Loss: 1.7033, Batch Acc: 0.1250\n",
      "[Batch 270/501] Loss: 1.6533, Batch Acc: 0.1250\n",
      "[Batch 280/501] Loss: 1.4581, Batch Acc: 0.2500\n",
      "[Batch 290/501] Loss: 1.7863, Batch Acc: 0.1250\n",
      "[Batch 300/501] Loss: 1.7437, Batch Acc: 0.3750\n",
      "[Batch 310/501] Loss: 1.8194, Batch Acc: 0.1250\n",
      "[Batch 320/501] Loss: 1.7157, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 1.6045, Batch Acc: 0.3750\n",
      "[Batch 340/501] Loss: 2.0805, Batch Acc: 0.1250\n",
      "[Batch 350/501] Loss: 1.9878, Batch Acc: 0.1250\n",
      "[Batch 360/501] Loss: 1.9477, Batch Acc: 0.0000\n",
      "[Batch 370/501] Loss: 2.0411, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.5329, Batch Acc: 0.3750\n",
      "[Batch 390/501] Loss: 1.7653, Batch Acc: 0.1250\n",
      "[Batch 400/501] Loss: 1.7506, Batch Acc: 0.2500\n",
      "[Batch 410/501] Loss: 1.6318, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 2.0047, Batch Acc: 0.2500\n",
      "[Batch 430/501] Loss: 1.4701, Batch Acc: 0.1250\n",
      "[Batch 440/501] Loss: 1.7176, Batch Acc: 0.1250\n",
      "[Batch 450/501] Loss: 1.4653, Batch Acc: 0.3750\n",
      "[Batch 460/501] Loss: 1.6632, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.4396, Batch Acc: 0.5000\n",
      "[Batch 480/501] Loss: 1.5403, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.8109, Batch Acc: 0.1250\n",
      "[Batch 500/501] Loss: 1.6379, Batch Acc: 0.2500\n",
      "Epoch 6 Summary - Loss: 822.8337, Train Accuracy: 0.2782\n",
      "[Batch 10/501] Loss: 1.5719, Batch Acc: 0.1250\n",
      "[Batch 20/501] Loss: 1.5181, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.7974, Batch Acc: 0.2500\n",
      "[Batch 40/501] Loss: 1.5131, Batch Acc: 0.6250\n",
      "[Batch 50/501] Loss: 1.3898, Batch Acc: 0.6250\n",
      "[Batch 60/501] Loss: 1.9152, Batch Acc: 0.1250\n",
      "[Batch 70/501] Loss: 1.7095, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.7826, Batch Acc: 0.2500\n",
      "[Batch 90/501] Loss: 1.3819, Batch Acc: 0.3750\n",
      "[Batch 100/501] Loss: 1.5369, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 1.5531, Batch Acc: 0.0000\n",
      "[Batch 120/501] Loss: 1.6640, Batch Acc: 0.1250\n",
      "[Batch 130/501] Loss: 1.6154, Batch Acc: 0.1250\n",
      "[Batch 140/501] Loss: 1.5767, Batch Acc: 0.3750\n",
      "[Batch 150/501] Loss: 1.7781, Batch Acc: 0.3750\n",
      "[Batch 160/501] Loss: 1.8187, Batch Acc: 0.0000\n",
      "[Batch 170/501] Loss: 1.8666, Batch Acc: 0.2500\n",
      "[Batch 180/501] Loss: 1.7707, Batch Acc: 0.3750\n",
      "[Batch 190/501] Loss: 1.8252, Batch Acc: 0.3750\n",
      "[Batch 200/501] Loss: 1.6469, Batch Acc: 0.2500\n",
      "[Batch 210/501] Loss: 1.5650, Batch Acc: 0.5000\n",
      "[Batch 220/501] Loss: 1.7005, Batch Acc: 0.1250\n",
      "[Batch 230/501] Loss: 1.5416, Batch Acc: 0.1250\n",
      "[Batch 240/501] Loss: 1.3205, Batch Acc: 0.5000\n",
      "[Batch 250/501] Loss: 1.5840, Batch Acc: 0.1250\n",
      "[Batch 260/501] Loss: 1.5839, Batch Acc: 0.2500\n",
      "[Batch 270/501] Loss: 1.5164, Batch Acc: 0.3750\n",
      "[Batch 280/501] Loss: 1.5224, Batch Acc: 0.2500\n",
      "[Batch 290/501] Loss: 1.7664, Batch Acc: 0.0000\n",
      "[Batch 300/501] Loss: 1.4635, Batch Acc: 0.3750\n",
      "[Batch 310/501] Loss: 1.7614, Batch Acc: 0.1250\n",
      "[Batch 320/501] Loss: 1.6202, Batch Acc: 0.2500\n",
      "[Batch 330/501] Loss: 1.5709, Batch Acc: 0.2500\n",
      "[Batch 340/501] Loss: 1.4960, Batch Acc: 0.5000\n",
      "[Batch 350/501] Loss: 1.3799, Batch Acc: 0.3750\n",
      "[Batch 360/501] Loss: 1.7465, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 2.0643, Batch Acc: 0.5000\n",
      "[Batch 380/501] Loss: 2.2844, Batch Acc: 0.1250\n",
      "[Batch 390/501] Loss: 1.3479, Batch Acc: 0.2500\n",
      "[Batch 400/501] Loss: 1.6263, Batch Acc: 0.1250\n",
      "[Batch 410/501] Loss: 1.6089, Batch Acc: 0.1250\n",
      "[Batch 420/501] Loss: 1.8046, Batch Acc: 0.2500\n",
      "[Batch 430/501] Loss: 1.9767, Batch Acc: 0.0000\n",
      "[Batch 440/501] Loss: 1.5510, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.6237, Batch Acc: 0.2500\n",
      "[Batch 460/501] Loss: 1.8588, Batch Acc: 0.2500\n",
      "[Batch 470/501] Loss: 1.8405, Batch Acc: 0.2500\n",
      "[Batch 480/501] Loss: 1.7716, Batch Acc: 0.1250\n",
      "[Batch 490/501] Loss: 2.0728, Batch Acc: 0.1250\n",
      "[Batch 500/501] Loss: 1.8757, Batch Acc: 0.2500\n",
      "Epoch 7 Summary - Loss: 824.1846, Train Accuracy: 0.2750\n",
      "[Batch 10/501] Loss: 1.7727, Batch Acc: 0.1250\n",
      "[Batch 20/501] Loss: 1.5357, Batch Acc: 0.5000\n",
      "[Batch 30/501] Loss: 1.7126, Batch Acc: 0.1250\n",
      "[Batch 40/501] Loss: 1.3156, Batch Acc: 0.3750\n",
      "[Batch 50/501] Loss: 1.5736, Batch Acc: 0.6250\n",
      "[Batch 60/501] Loss: 1.8318, Batch Acc: 0.1250\n",
      "[Batch 70/501] Loss: 1.5120, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.9516, Batch Acc: 0.1250\n",
      "[Batch 90/501] Loss: 1.6415, Batch Acc: 0.2500\n",
      "[Batch 100/501] Loss: 1.5032, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 1.6070, Batch Acc: 0.2500\n",
      "[Batch 120/501] Loss: 1.9428, Batch Acc: 0.1250\n",
      "[Batch 130/501] Loss: 1.3406, Batch Acc: 0.2500\n",
      "[Batch 140/501] Loss: 1.8709, Batch Acc: 0.2500\n",
      "[Batch 150/501] Loss: 1.5823, Batch Acc: 0.5000\n",
      "[Batch 160/501] Loss: 1.6414, Batch Acc: 0.5000\n",
      "[Batch 170/501] Loss: 1.8639, Batch Acc: 0.1250\n",
      "[Batch 180/501] Loss: 1.6048, Batch Acc: 0.1250\n",
      "[Batch 190/501] Loss: 1.6119, Batch Acc: 0.1250\n",
      "[Batch 200/501] Loss: 1.5665, Batch Acc: 0.2500\n",
      "[Batch 210/501] Loss: 1.4018, Batch Acc: 0.3750\n",
      "[Batch 220/501] Loss: 1.4819, Batch Acc: 0.3750\n",
      "[Batch 230/501] Loss: 1.7490, Batch Acc: 0.3750\n",
      "[Batch 240/501] Loss: 1.5563, Batch Acc: 0.2500\n",
      "[Batch 250/501] Loss: 1.5729, Batch Acc: 0.2500\n",
      "[Batch 260/501] Loss: 1.6272, Batch Acc: 0.3750\n",
      "[Batch 270/501] Loss: 1.3465, Batch Acc: 0.3750\n",
      "[Batch 280/501] Loss: 1.5021, Batch Acc: 0.5000\n",
      "[Batch 290/501] Loss: 1.3463, Batch Acc: 0.5000\n",
      "[Batch 300/501] Loss: 1.6640, Batch Acc: 0.1250\n",
      "[Batch 310/501] Loss: 1.3974, Batch Acc: 0.5000\n",
      "[Batch 320/501] Loss: 1.3414, Batch Acc: 0.3750\n",
      "[Batch 330/501] Loss: 1.3441, Batch Acc: 0.5000\n",
      "[Batch 340/501] Loss: 1.2451, Batch Acc: 0.7500\n",
      "[Batch 350/501] Loss: 1.6411, Batch Acc: 0.2500\n",
      "[Batch 360/501] Loss: 1.5184, Batch Acc: 0.3750\n",
      "[Batch 370/501] Loss: 1.6051, Batch Acc: 0.2500\n",
      "[Batch 380/501] Loss: 2.0488, Batch Acc: 0.0000\n",
      "[Batch 390/501] Loss: 1.5451, Batch Acc: 0.2500\n",
      "[Batch 400/501] Loss: 1.5780, Batch Acc: 0.2500\n",
      "[Batch 410/501] Loss: 1.7198, Batch Acc: 0.1250\n",
      "[Batch 420/501] Loss: 2.1489, Batch Acc: 0.1250\n",
      "[Batch 430/501] Loss: 1.6576, Batch Acc: 0.5000\n",
      "[Batch 440/501] Loss: 1.4892, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 2.1263, Batch Acc: 0.1250\n",
      "[Batch 460/501] Loss: 2.1635, Batch Acc: 0.1250\n",
      "[Batch 470/501] Loss: 1.5927, Batch Acc: 0.2500\n",
      "[Batch 480/501] Loss: 1.7772, Batch Acc: 0.3750\n",
      "[Batch 490/501] Loss: 1.8636, Batch Acc: 0.1250\n",
      "[Batch 500/501] Loss: 2.0292, Batch Acc: 0.0000\n",
      "Epoch 8 Summary - Loss: 821.4235, Train Accuracy: 0.2807\n",
      "[Batch 10/501] Loss: 2.2632, Batch Acc: 0.0000\n",
      "[Batch 20/501] Loss: 1.3678, Batch Acc: 0.5000\n",
      "[Batch 30/501] Loss: 1.6545, Batch Acc: 0.3750\n",
      "[Batch 40/501] Loss: 1.4431, Batch Acc: 0.2500\n",
      "[Batch 50/501] Loss: 1.8803, Batch Acc: 0.0000\n",
      "[Batch 60/501] Loss: 1.6355, Batch Acc: 0.2500\n",
      "[Batch 70/501] Loss: 1.5180, Batch Acc: 0.2500\n",
      "[Batch 80/501] Loss: 1.6098, Batch Acc: 0.3750\n",
      "[Batch 90/501] Loss: 1.6070, Batch Acc: 0.3750\n",
      "[Batch 100/501] Loss: 1.9683, Batch Acc: 0.2500\n",
      "[Batch 110/501] Loss: 1.5950, Batch Acc: 0.1250\n",
      "[Batch 120/501] Loss: 1.7892, Batch Acc: 0.5000\n",
      "[Batch 130/501] Loss: 1.7035, Batch Acc: 0.2500\n",
      "[Batch 140/501] Loss: 1.6134, Batch Acc: 0.2500\n",
      "[Batch 150/501] Loss: 1.8229, Batch Acc: 0.1250\n",
      "[Batch 160/501] Loss: 1.9415, Batch Acc: 0.3750\n",
      "[Batch 170/501] Loss: 1.8731, Batch Acc: 0.2500\n",
      "[Batch 180/501] Loss: 1.3384, Batch Acc: 0.5000\n",
      "[Batch 190/501] Loss: 1.4444, Batch Acc: 0.3750\n",
      "[Batch 200/501] Loss: 1.6295, Batch Acc: 0.3750\n",
      "[Batch 210/501] Loss: 1.8845, Batch Acc: 0.3750\n",
      "[Batch 220/501] Loss: 1.2419, Batch Acc: 0.5000\n",
      "[Batch 230/501] Loss: 1.4778, Batch Acc: 0.5000\n",
      "[Batch 240/501] Loss: 1.2320, Batch Acc: 0.7500\n",
      "[Batch 250/501] Loss: 1.6023, Batch Acc: 0.2500\n",
      "[Batch 260/501] Loss: 1.6037, Batch Acc: 0.6250\n",
      "[Batch 270/501] Loss: 1.6945, Batch Acc: 0.2500\n",
      "[Batch 280/501] Loss: 1.7631, Batch Acc: 0.0000\n",
      "[Batch 290/501] Loss: 1.7548, Batch Acc: 0.1250\n",
      "[Batch 300/501] Loss: 1.8763, Batch Acc: 0.1250\n",
      "[Batch 310/501] Loss: 1.8649, Batch Acc: 0.2500\n",
      "[Batch 320/501] Loss: 1.4386, Batch Acc: 0.5000\n",
      "[Batch 330/501] Loss: 1.3878, Batch Acc: 0.3750\n",
      "[Batch 340/501] Loss: 1.8237, Batch Acc: 0.1250\n",
      "[Batch 350/501] Loss: 1.4307, Batch Acc: 0.3750\n",
      "[Batch 360/501] Loss: 1.7036, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 2.0300, Batch Acc: 0.1250\n",
      "[Batch 380/501] Loss: 1.7960, Batch Acc: 0.1250\n",
      "[Batch 390/501] Loss: 1.8167, Batch Acc: 0.0000\n",
      "[Batch 400/501] Loss: 1.5149, Batch Acc: 0.2500\n",
      "[Batch 410/501] Loss: 1.5396, Batch Acc: 0.3750\n",
      "[Batch 420/501] Loss: 1.9172, Batch Acc: 0.2500\n",
      "[Batch 430/501] Loss: 1.3052, Batch Acc: 0.5000\n",
      "[Batch 440/501] Loss: 1.6363, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.4537, Batch Acc: 0.5000\n",
      "[Batch 460/501] Loss: 1.5817, Batch Acc: 0.3750\n",
      "[Batch 470/501] Loss: 1.4070, Batch Acc: 0.6250\n",
      "[Batch 480/501] Loss: 1.4071, Batch Acc: 0.3750\n",
      "[Batch 490/501] Loss: 1.5965, Batch Acc: 0.2500\n",
      "[Batch 500/501] Loss: 1.7893, Batch Acc: 0.1250\n",
      "Epoch 9 Summary - Loss: 820.2334, Train Accuracy: 0.2884\n",
      "[Batch 10/501] Loss: 1.7832, Batch Acc: 0.3750\n",
      "[Batch 20/501] Loss: 1.7296, Batch Acc: 0.2500\n",
      "[Batch 30/501] Loss: 1.8934, Batch Acc: 0.0000\n",
      "[Batch 40/501] Loss: 1.0638, Batch Acc: 0.7500\n",
      "[Batch 50/501] Loss: 1.6547, Batch Acc: 0.2500\n",
      "[Batch 60/501] Loss: 1.7227, Batch Acc: 0.5000\n",
      "[Batch 70/501] Loss: 1.4188, Batch Acc: 0.1250\n",
      "[Batch 80/501] Loss: 1.7482, Batch Acc: 0.3750\n",
      "[Batch 90/501] Loss: 1.4334, Batch Acc: 0.1250\n",
      "[Batch 100/501] Loss: 1.8900, Batch Acc: 0.0000\n",
      "[Batch 110/501] Loss: 1.5965, Batch Acc: 0.3750\n",
      "[Batch 120/501] Loss: 1.7757, Batch Acc: 0.3750\n",
      "[Batch 130/501] Loss: 1.3163, Batch Acc: 0.5000\n",
      "[Batch 140/501] Loss: 2.0099, Batch Acc: 0.1250\n",
      "[Batch 150/501] Loss: 1.6637, Batch Acc: 0.2500\n",
      "[Batch 160/501] Loss: 1.5325, Batch Acc: 0.5000\n",
      "[Batch 170/501] Loss: 1.5681, Batch Acc: 0.5000\n",
      "[Batch 180/501] Loss: 1.6219, Batch Acc: 0.3750\n",
      "[Batch 190/501] Loss: 1.6953, Batch Acc: 0.2500\n",
      "[Batch 200/501] Loss: 1.6937, Batch Acc: 0.5000\n",
      "[Batch 210/501] Loss: 1.2945, Batch Acc: 0.5000\n",
      "[Batch 220/501] Loss: 1.6545, Batch Acc: 0.2500\n",
      "[Batch 230/501] Loss: 1.6551, Batch Acc: 0.2500\n",
      "[Batch 240/501] Loss: 1.5270, Batch Acc: 0.3750\n",
      "[Batch 250/501] Loss: 1.5240, Batch Acc: 0.2500\n",
      "[Batch 260/501] Loss: 1.6051, Batch Acc: 0.1250\n",
      "[Batch 270/501] Loss: 1.9162, Batch Acc: 0.3750\n",
      "[Batch 280/501] Loss: 1.9169, Batch Acc: 0.1250\n",
      "[Batch 290/501] Loss: 1.7676, Batch Acc: 0.3750\n",
      "[Batch 300/501] Loss: 1.7002, Batch Acc: 0.2500\n",
      "[Batch 310/501] Loss: 1.7054, Batch Acc: 0.1250\n",
      "[Batch 320/501] Loss: 2.0465, Batch Acc: 0.1250\n",
      "[Batch 330/501] Loss: 2.1284, Batch Acc: 0.1250\n",
      "[Batch 340/501] Loss: 1.7499, Batch Acc: 0.2500\n",
      "[Batch 350/501] Loss: 1.6889, Batch Acc: 0.1250\n",
      "[Batch 360/501] Loss: 1.5676, Batch Acc: 0.2500\n",
      "[Batch 370/501] Loss: 1.5715, Batch Acc: 0.5000\n",
      "[Batch 380/501] Loss: 1.6219, Batch Acc: 0.1250\n",
      "[Batch 390/501] Loss: 1.7219, Batch Acc: 0.1250\n",
      "[Batch 400/501] Loss: 1.6410, Batch Acc: 0.3750\n",
      "[Batch 410/501] Loss: 1.6511, Batch Acc: 0.2500\n",
      "[Batch 420/501] Loss: 1.4548, Batch Acc: 0.5000\n",
      "[Batch 430/501] Loss: 1.5511, Batch Acc: 0.1250\n",
      "[Batch 440/501] Loss: 1.6132, Batch Acc: 0.2500\n",
      "[Batch 450/501] Loss: 1.4329, Batch Acc: 0.5000\n",
      "[Batch 460/501] Loss: 1.8773, Batch Acc: 0.1250\n",
      "[Batch 470/501] Loss: 1.5106, Batch Acc: 0.2500\n",
      "[Batch 480/501] Loss: 1.3698, Batch Acc: 0.2500\n",
      "[Batch 490/501] Loss: 1.7685, Batch Acc: 0.5000\n",
      "[Batch 500/501] Loss: 2.1253, Batch Acc: 0.1250\n",
      "Epoch 10 Summary - Loss: 817.2786, Train Accuracy: 0.2959\n",
      "Test Accuracy: 0.3056\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_accuracy</td><td>▃█▃▁▂▂▁▃▆█</td></tr><tr><td>train_loss</td><td>▆▂█▄▅▅▅▄▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>test_accuracy</td><td>0.30564</td></tr><tr><td>test_loss</td><td>201.63809</td></tr><tr><td>train_accuracy</td><td>0.29591</td></tr><tr><td>train_loss</td><td>817.27861</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vivid-sweep-16</strong> at: <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/grmgdkal' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/grmgdkal</a><br> View project at: <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250615_144054-grmgdkal/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ishonvie with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_nodes: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ajit.kumar4@happiestminds.com/Documents/Drackula/ComputerVisionFromScratch/wandb/run-20250615_144445-ishonvie</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/ishonvie' target=\"_blank\">legendary-sweep-17</a></strong> to <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/sweeps/7yx61jek</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/ishonvie' target=\"_blank\">https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset-3/runs/ishonvie</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 10/251] Loss: 1.6618, Batch Acc: 0.2500\n",
      "[Batch 20/251] Loss: 1.7765, Batch Acc: 0.2500\n",
      "[Batch 30/251] Loss: 1.7056, Batch Acc: 0.1250\n",
      "[Batch 40/251] Loss: 1.6525, Batch Acc: 0.2500\n",
      "[Batch 50/251] Loss: 1.7577, Batch Acc: 0.2500\n",
      "[Batch 60/251] Loss: 1.5491, Batch Acc: 0.3750\n",
      "[Batch 70/251] Loss: 1.8893, Batch Acc: 0.2500\n",
      "[Batch 80/251] Loss: 1.7818, Batch Acc: 0.2500\n",
      "[Batch 90/251] Loss: 1.6115, Batch Acc: 0.2500\n",
      "[Batch 100/251] Loss: 1.5596, Batch Acc: 0.2500\n",
      "[Batch 110/251] Loss: 1.6213, Batch Acc: 0.1250\n",
      "[Batch 120/251] Loss: 1.8042, Batch Acc: 0.1875\n",
      "[Batch 130/251] Loss: 1.6564, Batch Acc: 0.1875\n",
      "[Batch 140/251] Loss: 1.6303, Batch Acc: 0.3750\n",
      "[Batch 150/251] Loss: 1.8399, Batch Acc: 0.0625\n",
      "[Batch 160/251] Loss: 1.8478, Batch Acc: 0.1250\n",
      "[Batch 170/251] Loss: 1.5560, Batch Acc: 0.2500\n",
      "[Batch 180/251] Loss: 1.3204, Batch Acc: 0.4375\n",
      "[Batch 190/251] Loss: 1.4309, Batch Acc: 0.4375\n",
      "[Batch 200/251] Loss: 1.6626, Batch Acc: 0.3125\n",
      "[Batch 210/251] Loss: 1.6200, Batch Acc: 0.3125\n",
      "[Batch 220/251] Loss: 1.6229, Batch Acc: 0.3125\n",
      "[Batch 230/251] Loss: 1.3672, Batch Acc: 0.5625\n",
      "[Batch 240/251] Loss: 1.7361, Batch Acc: 0.1250\n",
      "[Batch 250/251] Loss: 1.6007, Batch Acc: 0.3125\n",
      "Epoch 1 Summary - Loss: 408.2301, Train Accuracy: 0.2485\n",
      "[Batch 10/251] Loss: 1.5574, Batch Acc: 0.3750\n",
      "[Batch 20/251] Loss: 1.9063, Batch Acc: 0.1250\n",
      "[Batch 30/251] Loss: 1.5805, Batch Acc: 0.2500\n",
      "[Batch 40/251] Loss: 1.5527, Batch Acc: 0.2500\n",
      "[Batch 50/251] Loss: 1.5938, Batch Acc: 0.3125\n",
      "[Batch 60/251] Loss: 1.4148, Batch Acc: 0.4375\n",
      "[Batch 70/251] Loss: 1.5769, Batch Acc: 0.2500\n",
      "[Batch 80/251] Loss: 1.6325, Batch Acc: 0.2500\n",
      "[Batch 90/251] Loss: 1.6452, Batch Acc: 0.3125\n",
      "[Batch 100/251] Loss: 1.7148, Batch Acc: 0.1250\n",
      "[Batch 110/251] Loss: 1.4999, Batch Acc: 0.3750\n",
      "[Batch 120/251] Loss: 1.5295, Batch Acc: 0.4375\n",
      "[Batch 130/251] Loss: 1.6071, Batch Acc: 0.2500\n",
      "[Batch 140/251] Loss: 1.6767, Batch Acc: 0.1875\n",
      "[Batch 150/251] Loss: 1.7079, Batch Acc: 0.1875\n",
      "[Batch 160/251] Loss: 1.7695, Batch Acc: 0.1250\n",
      "[Batch 170/251] Loss: 1.8476, Batch Acc: 0.1250\n",
      "[Batch 180/251] Loss: 1.6202, Batch Acc: 0.1875\n",
      "[Batch 190/251] Loss: 1.8933, Batch Acc: 0.1875\n",
      "[Batch 200/251] Loss: 1.5413, Batch Acc: 0.2500\n",
      "[Batch 210/251] Loss: 1.5231, Batch Acc: 0.3125\n",
      "[Batch 220/251] Loss: 1.5591, Batch Acc: 0.2500\n",
      "[Batch 230/251] Loss: 1.4440, Batch Acc: 0.4375\n",
      "[Batch 240/251] Loss: 1.6063, Batch Acc: 0.3125\n",
      "[Batch 250/251] Loss: 1.5873, Batch Acc: 0.2500\n",
      "Epoch 2 Summary - Loss: 407.4666, Train Accuracy: 0.2502\n",
      "[Batch 10/251] Loss: 1.7672, Batch Acc: 0.1250\n",
      "[Batch 20/251] Loss: 1.4915, Batch Acc: 0.3125\n",
      "[Batch 30/251] Loss: 1.8915, Batch Acc: 0.1250\n",
      "[Batch 40/251] Loss: 1.6561, Batch Acc: 0.2500\n",
      "[Batch 50/251] Loss: 1.6049, Batch Acc: 0.1875\n",
      "[Batch 60/251] Loss: 1.7428, Batch Acc: 0.0625\n",
      "[Batch 70/251] Loss: 1.7908, Batch Acc: 0.1250\n",
      "[Batch 80/251] Loss: 1.6615, Batch Acc: 0.1875\n",
      "[Batch 90/251] Loss: 1.4898, Batch Acc: 0.3750\n",
      "[Batch 100/251] Loss: 1.6065, Batch Acc: 0.2500\n",
      "[Batch 110/251] Loss: 1.7417, Batch Acc: 0.0625\n",
      "[Batch 120/251] Loss: 1.5430, Batch Acc: 0.2500\n",
      "[Batch 130/251] Loss: 1.5958, Batch Acc: 0.1875\n",
      "[Batch 140/251] Loss: 1.8628, Batch Acc: 0.1250\n",
      "[Batch 150/251] Loss: 1.6138, Batch Acc: 0.0625\n",
      "[Batch 160/251] Loss: 1.7662, Batch Acc: 0.0625\n",
      "[Batch 170/251] Loss: 1.6094, Batch Acc: 0.3125\n",
      "[Batch 180/251] Loss: 1.4623, Batch Acc: 0.3750\n",
      "[Batch 190/251] Loss: 1.5333, Batch Acc: 0.3750\n",
      "[Batch 200/251] Loss: 1.5925, Batch Acc: 0.3125\n",
      "[Batch 210/251] Loss: 1.7243, Batch Acc: 0.2500\n",
      "[Batch 220/251] Loss: 1.7380, Batch Acc: 0.1875\n",
      "[Batch 230/251] Loss: 1.4816, Batch Acc: 0.4375\n",
      "[Batch 240/251] Loss: 1.6692, Batch Acc: 0.1250\n",
      "[Batch 250/251] Loss: 1.5826, Batch Acc: 0.1875\n",
      "Epoch 3 Summary - Loss: 408.4993, Train Accuracy: 0.2470\n",
      "[Batch 10/251] Loss: 1.6246, Batch Acc: 0.3125\n",
      "[Batch 20/251] Loss: 1.5797, Batch Acc: 0.3750\n",
      "[Batch 30/251] Loss: 1.7572, Batch Acc: 0.2500\n",
      "[Batch 40/251] Loss: 1.6611, Batch Acc: 0.3125\n",
      "[Batch 50/251] Loss: 1.6268, Batch Acc: 0.3125\n",
      "[Batch 60/251] Loss: 1.5185, Batch Acc: 0.1875\n",
      "[Batch 70/251] Loss: 1.6302, Batch Acc: 0.4375\n",
      "[Batch 80/251] Loss: 1.6440, Batch Acc: 0.1875\n",
      "[Batch 90/251] Loss: 1.5479, Batch Acc: 0.1875\n",
      "[Batch 100/251] Loss: 1.5687, Batch Acc: 0.3125\n",
      "[Batch 110/251] Loss: 1.5233, Batch Acc: 0.3750\n",
      "[Batch 120/251] Loss: 1.7667, Batch Acc: 0.1875\n",
      "[Batch 130/251] Loss: 1.4734, Batch Acc: 0.2500\n",
      "[Batch 140/251] Loss: 1.6587, Batch Acc: 0.2500\n",
      "[Batch 150/251] Loss: 1.6913, Batch Acc: 0.1875\n",
      "[Batch 160/251] Loss: 1.6148, Batch Acc: 0.3750\n",
      "[Batch 170/251] Loss: 1.5861, Batch Acc: 0.3750\n",
      "[Batch 180/251] Loss: 1.4007, Batch Acc: 0.4375\n",
      "[Batch 190/251] Loss: 1.8589, Batch Acc: 0.1250\n",
      "[Batch 200/251] Loss: 1.6844, Batch Acc: 0.3125\n",
      "[Batch 210/251] Loss: 1.7342, Batch Acc: 0.1875\n",
      "[Batch 220/251] Loss: 1.6146, Batch Acc: 0.1875\n",
      "[Batch 230/251] Loss: 1.5224, Batch Acc: 0.3125\n",
      "[Batch 240/251] Loss: 1.7018, Batch Acc: 0.1875\n",
      "[Batch 250/251] Loss: 1.4987, Batch Acc: 0.1875\n",
      "Epoch 4 Summary - Loss: 407.9370, Train Accuracy: 0.2493\n",
      "[Batch 10/251] Loss: 1.5376, Batch Acc: 0.3125\n",
      "[Batch 20/251] Loss: 1.8088, Batch Acc: 0.1250\n",
      "[Batch 30/251] Loss: 1.7063, Batch Acc: 0.3125\n",
      "[Batch 40/251] Loss: 1.5609, Batch Acc: 0.1875\n",
      "[Batch 50/251] Loss: 1.5554, Batch Acc: 0.3125\n",
      "[Batch 60/251] Loss: 1.4271, Batch Acc: 0.4375\n",
      "[Batch 70/251] Loss: 1.8698, Batch Acc: 0.0000\n",
      "[Batch 80/251] Loss: 1.6402, Batch Acc: 0.2500\n",
      "[Batch 90/251] Loss: 1.8123, Batch Acc: 0.1875\n",
      "[Batch 100/251] Loss: 1.3412, Batch Acc: 0.6250\n",
      "[Batch 110/251] Loss: 1.5748, Batch Acc: 0.2500\n",
      "[Batch 120/251] Loss: 1.5826, Batch Acc: 0.2500\n",
      "[Batch 130/251] Loss: 1.5941, Batch Acc: 0.2500\n",
      "[Batch 140/251] Loss: 1.3686, Batch Acc: 0.3750\n",
      "[Batch 150/251] Loss: 1.5672, Batch Acc: 0.3125\n",
      "[Batch 160/251] Loss: 1.5582, Batch Acc: 0.2500\n",
      "[Batch 170/251] Loss: 1.5695, Batch Acc: 0.3750\n",
      "[Batch 180/251] Loss: 1.6632, Batch Acc: 0.0000\n",
      "[Batch 190/251] Loss: 1.4781, Batch Acc: 0.3125\n",
      "[Batch 200/251] Loss: 1.4464, Batch Acc: 0.3125\n",
      "[Batch 210/251] Loss: 1.6594, Batch Acc: 0.1875\n",
      "[Batch 220/251] Loss: 1.5264, Batch Acc: 0.3125\n",
      "[Batch 230/251] Loss: 1.7397, Batch Acc: 0.1875\n",
      "[Batch 240/251] Loss: 1.7656, Batch Acc: 0.1875\n",
      "[Batch 250/251] Loss: 1.5360, Batch Acc: 0.3125\n",
      "Epoch 5 Summary - Loss: 407.4963, Train Accuracy: 0.2465\n",
      "[Batch 10/251] Loss: 1.5878, Batch Acc: 0.2500\n",
      "[Batch 20/251] Loss: 1.4394, Batch Acc: 0.2500\n",
      "[Batch 30/251] Loss: 1.6928, Batch Acc: 0.1250\n",
      "[Batch 40/251] Loss: 1.6162, Batch Acc: 0.3125\n",
      "[Batch 50/251] Loss: 1.6197, Batch Acc: 0.3125\n",
      "[Batch 60/251] Loss: 1.4999, Batch Acc: 0.3125\n",
      "[Batch 70/251] Loss: 1.5062, Batch Acc: 0.4375\n",
      "[Batch 80/251] Loss: 1.7029, Batch Acc: 0.1875\n",
      "[Batch 90/251] Loss: 1.4396, Batch Acc: 0.4375\n",
      "[Batch 100/251] Loss: 1.7896, Batch Acc: 0.0625\n",
      "[Batch 110/251] Loss: 1.5663, Batch Acc: 0.2500\n",
      "[Batch 120/251] Loss: 1.2886, Batch Acc: 0.5625\n",
      "[Batch 130/251] Loss: 1.6827, Batch Acc: 0.1250\n",
      "[Batch 140/251] Loss: 1.6366, Batch Acc: 0.2500\n",
      "[Batch 150/251] Loss: 1.7215, Batch Acc: 0.0625\n",
      "[Batch 160/251] Loss: 1.7019, Batch Acc: 0.1875\n",
      "[Batch 170/251] Loss: 1.6427, Batch Acc: 0.1250\n",
      "[Batch 180/251] Loss: 1.8648, Batch Acc: 0.0625\n",
      "[Batch 190/251] Loss: 1.6834, Batch Acc: 0.2500\n",
      "[Batch 200/251] Loss: 1.9223, Batch Acc: 0.1875\n",
      "[Batch 210/251] Loss: 1.7837, Batch Acc: 0.1875\n",
      "[Batch 220/251] Loss: 1.6523, Batch Acc: 0.1250\n",
      "[Batch 230/251] Loss: 1.4597, Batch Acc: 0.3125\n",
      "[Batch 240/251] Loss: 1.6888, Batch Acc: 0.2500\n",
      "[Batch 250/251] Loss: 1.5325, Batch Acc: 0.3125\n",
      "Epoch 6 Summary - Loss: 406.6708, Train Accuracy: 0.2512\n",
      "[Batch 10/251] Loss: 1.3347, Batch Acc: 0.3750\n",
      "[Batch 20/251] Loss: 1.6390, Batch Acc: 0.1875\n",
      "[Batch 30/251] Loss: 1.6057, Batch Acc: 0.3125\n",
      "[Batch 40/251] Loss: 1.6015, Batch Acc: 0.1875\n",
      "[Batch 50/251] Loss: 1.3886, Batch Acc: 0.5000\n",
      "[Batch 60/251] Loss: 1.8477, Batch Acc: 0.2500\n",
      "[Batch 70/251] Loss: 1.4819, Batch Acc: 0.2500\n",
      "[Batch 80/251] Loss: 1.7249, Batch Acc: 0.1250\n",
      "[Batch 90/251] Loss: 1.8306, Batch Acc: 0.1875\n",
      "[Batch 100/251] Loss: 1.4397, Batch Acc: 0.4375\n",
      "[Batch 110/251] Loss: 1.6276, Batch Acc: 0.3750\n",
      "[Batch 120/251] Loss: 1.7258, Batch Acc: 0.1250\n",
      "[Batch 130/251] Loss: 1.7131, Batch Acc: 0.1875\n",
      "[Batch 140/251] Loss: 1.6395, Batch Acc: 0.3750\n",
      "[Batch 150/251] Loss: 1.6002, Batch Acc: 0.2500\n",
      "[Batch 160/251] Loss: 1.5388, Batch Acc: 0.3750\n",
      "[Batch 170/251] Loss: 1.5415, Batch Acc: 0.3750\n",
      "[Batch 180/251] Loss: 1.7611, Batch Acc: 0.2500\n",
      "[Batch 190/251] Loss: 1.6711, Batch Acc: 0.1875\n",
      "[Batch 200/251] Loss: 1.4664, Batch Acc: 0.2500\n",
      "[Batch 210/251] Loss: 1.3842, Batch Acc: 0.3125\n",
      "[Batch 220/251] Loss: 1.6788, Batch Acc: 0.1875\n",
      "[Batch 230/251] Loss: 1.8521, Batch Acc: 0.1250\n",
      "[Batch 240/251] Loss: 1.7282, Batch Acc: 0.0625\n",
      "[Batch 250/251] Loss: 1.5526, Batch Acc: 0.2500\n",
      "Epoch 7 Summary - Loss: 406.7558, Train Accuracy: 0.2512\n",
      "[Batch 10/251] Loss: 1.5901, Batch Acc: 0.3125\n",
      "[Batch 20/251] Loss: 1.6533, Batch Acc: 0.2500\n"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id, function=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672d66a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4008 1011\n",
      "251 64\n",
      "torch.Size([3, 128, 128]) 0\n",
      "torch.Size([3, 128, 128]) 0\n",
      "torch.Size([16, 3, 128, 128]) tensor([2, 3, 4, 1, 2, 1, 1, 0, 0, 3, 1, 1, 4, 3, 0, 2])\n",
      "torch.Size([16, 3, 128, 128]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# print(len(train_dataset), len(test_dataset))\n",
    "# print(len(train_loader), len(test_loader))\n",
    "# for img, label in train_dataset:\n",
    "#     print(img.shape, label)\n",
    "#     break\n",
    "# for img, label in test_dataset:\n",
    "#     print(img.shape, label)\n",
    "#     break\n",
    "# for img, label in train_loader:\n",
    "#     print(img.shape, label)\n",
    "#     break   \n",
    "\n",
    "# for img, label in test_loader:\n",
    "#     print(img.shape, label)\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbe8724",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, function=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ed95a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f393049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 10/251] Loss: 24.2275, Batch Acc: 3.4375\n",
      "[Batch 20/251] Loss: 11.9568, Batch Acc: 6.3750\n",
      "[Batch 30/251] Loss: 22.4465, Batch Acc: 9.9375\n",
      "[Batch 40/251] Loss: 9.7260, Batch Acc: 13.3750\n",
      "[Batch 50/251] Loss: 11.5228, Batch Acc: 17.3750\n",
      "[Batch 60/251] Loss: 11.9649, Batch Acc: 21.7500\n",
      "[Batch 70/251] Loss: 6.5767, Batch Acc: 25.5625\n",
      "[Batch 80/251] Loss: 17.2534, Batch Acc: 29.6250\n",
      "[Batch 90/251] Loss: 16.7764, Batch Acc: 34.6250\n",
      "[Batch 100/251] Loss: 15.6186, Batch Acc: 38.3750\n",
      "[Batch 110/251] Loss: 18.6942, Batch Acc: 42.0000\n",
      "[Batch 120/251] Loss: 7.5438, Batch Acc: 46.1250\n",
      "[Batch 130/251] Loss: 9.0678, Batch Acc: 50.6875\n",
      "[Batch 140/251] Loss: 20.6536, Batch Acc: 54.2500\n",
      "[Batch 150/251] Loss: 16.8641, Batch Acc: 58.2500\n",
      "[Batch 160/251] Loss: 20.7025, Batch Acc: 62.2500\n",
      "[Batch 170/251] Loss: 10.8953, Batch Acc: 66.8750\n",
      "[Batch 180/251] Loss: 13.8928, Batch Acc: 70.9375\n",
      "[Batch 190/251] Loss: 20.1441, Batch Acc: 73.8750\n",
      "[Batch 200/251] Loss: 10.8839, Batch Acc: 77.8125\n",
      "[Batch 210/251] Loss: 10.6670, Batch Acc: 82.5625\n",
      "[Batch 220/251] Loss: 14.8816, Batch Acc: 87.6250\n",
      "[Batch 230/251] Loss: 34.2260, Batch Acc: 91.3125\n",
      "[Batch 240/251] Loss: 19.9491, Batch Acc: 95.5625\n",
      "[Batch 250/251] Loss: 10.3256, Batch Acc: 99.5625\n",
      "Epoch 1 Summary - Loss: 4124.7066, Train Accuracy: 0.3977\n",
      "[Batch 10/251] Loss: 9.9975, Batch Acc: 4.8125\n",
      "[Batch 20/251] Loss: 12.4347, Batch Acc: 10.0625\n",
      "[Batch 30/251] Loss: 10.4504, Batch Acc: 14.3125\n",
      "[Batch 40/251] Loss: 30.3483, Batch Acc: 19.0625\n",
      "[Batch 50/251] Loss: 23.2150, Batch Acc: 23.5000\n",
      "[Batch 60/251] Loss: 16.2255, Batch Acc: 27.8125\n",
      "[Batch 70/251] Loss: 6.6245, Batch Acc: 32.8125\n",
      "[Batch 80/251] Loss: 15.3283, Batch Acc: 37.6250\n",
      "[Batch 90/251] Loss: 15.1848, Batch Acc: 42.2500\n",
      "[Batch 100/251] Loss: 9.4917, Batch Acc: 47.0625\n",
      "[Batch 110/251] Loss: 13.3028, Batch Acc: 52.3125\n",
      "[Batch 120/251] Loss: 14.5167, Batch Acc: 57.5625\n",
      "[Batch 130/251] Loss: 14.9532, Batch Acc: 62.3125\n",
      "[Batch 140/251] Loss: 10.0651, Batch Acc: 66.9375\n",
      "[Batch 150/251] Loss: 19.9431, Batch Acc: 72.4375\n",
      "[Batch 160/251] Loss: 11.8927, Batch Acc: 77.1250\n",
      "[Batch 170/251] Loss: 18.6462, Batch Acc: 82.2500\n",
      "[Batch 180/251] Loss: 7.4815, Batch Acc: 87.3750\n",
      "[Batch 190/251] Loss: 8.2462, Batch Acc: 93.3750\n",
      "[Batch 200/251] Loss: 15.7739, Batch Acc: 97.8750\n",
      "[Batch 210/251] Loss: 27.9945, Batch Acc: 102.7500\n",
      "[Batch 220/251] Loss: 5.7838, Batch Acc: 107.3125\n",
      "[Batch 230/251] Loss: 12.6110, Batch Acc: 112.8125\n",
      "[Batch 240/251] Loss: 17.8336, Batch Acc: 118.1875\n",
      "[Batch 250/251] Loss: 5.5585, Batch Acc: 123.1875\n",
      "Epoch 2 Summary - Loss: 3267.1953, Train Accuracy: 0.4935\n",
      "[Batch 10/251] Loss: 8.4954, Batch Acc: 5.1875\n",
      "[Batch 20/251] Loss: 7.1724, Batch Acc: 10.5625\n",
      "[Batch 30/251] Loss: 2.3570, Batch Acc: 16.2500\n",
      "[Batch 40/251] Loss: 13.4836, Batch Acc: 22.1875\n",
      "[Batch 50/251] Loss: 6.5590, Batch Acc: 27.9375\n",
      "[Batch 60/251] Loss: 6.0798, Batch Acc: 33.9375\n",
      "[Batch 70/251] Loss: 2.3757, Batch Acc: 40.0000\n",
      "[Batch 80/251] Loss: 15.6019, Batch Acc: 45.5000\n",
      "[Batch 90/251] Loss: 11.6363, Batch Acc: 50.3750\n",
      "[Batch 100/251] Loss: 9.6211, Batch Acc: 56.1875\n",
      "[Batch 110/251] Loss: 14.6174, Batch Acc: 62.1250\n",
      "[Batch 120/251] Loss: 14.6291, Batch Acc: 68.0625\n",
      "[Batch 130/251] Loss: 3.3162, Batch Acc: 74.3125\n",
      "[Batch 140/251] Loss: 7.7884, Batch Acc: 80.0000\n",
      "[Batch 150/251] Loss: 8.8537, Batch Acc: 86.2500\n",
      "[Batch 160/251] Loss: 8.5427, Batch Acc: 91.6250\n",
      "[Batch 170/251] Loss: 1.9073, Batch Acc: 97.4375\n",
      "[Batch 180/251] Loss: 10.5865, Batch Acc: 103.0000\n",
      "[Batch 190/251] Loss: 8.8551, Batch Acc: 108.6875\n",
      "[Batch 200/251] Loss: 6.2410, Batch Acc: 115.0000\n",
      "[Batch 210/251] Loss: 3.8660, Batch Acc: 120.4375\n",
      "[Batch 220/251] Loss: 9.9736, Batch Acc: 125.5625\n",
      "[Batch 230/251] Loss: 12.7819, Batch Acc: 130.5000\n",
      "[Batch 240/251] Loss: 5.2970, Batch Acc: 136.1250\n",
      "[Batch 250/251] Loss: 3.2224, Batch Acc: 141.2500\n",
      "Epoch 3 Summary - Loss: 2239.0659, Train Accuracy: 0.5646\n",
      "[Batch 10/251] Loss: 17.8665, Batch Acc: 5.2500\n",
      "[Batch 20/251] Loss: 7.6846, Batch Acc: 10.1875\n",
      "[Batch 30/251] Loss: 22.4781, Batch Acc: 15.1250\n",
      "[Batch 40/251] Loss: 7.3877, Batch Acc: 20.9375\n",
      "[Batch 50/251] Loss: 7.5966, Batch Acc: 26.8750\n",
      "[Batch 60/251] Loss: 2.0347, Batch Acc: 33.4375\n",
      "[Batch 70/251] Loss: 2.2490, Batch Acc: 40.3750\n",
      "[Batch 80/251] Loss: 9.6734, Batch Acc: 46.1250\n",
      "[Batch 90/251] Loss: 14.1890, Batch Acc: 51.3750\n",
      "[Batch 100/251] Loss: 10.3623, Batch Acc: 57.1250\n",
      "[Batch 110/251] Loss: 6.9813, Batch Acc: 63.0000\n",
      "[Batch 120/251] Loss: 21.5776, Batch Acc: 68.6875\n",
      "[Batch 130/251] Loss: 8.2503, Batch Acc: 74.2500\n",
      "[Batch 140/251] Loss: 8.7674, Batch Acc: 80.0000\n",
      "[Batch 150/251] Loss: 4.3864, Batch Acc: 85.9375\n",
      "[Batch 160/251] Loss: 9.7011, Batch Acc: 92.6875\n",
      "[Batch 170/251] Loss: 9.1780, Batch Acc: 99.0625\n",
      "[Batch 180/251] Loss: 15.2434, Batch Acc: 104.5000\n",
      "[Batch 190/251] Loss: 10.0704, Batch Acc: 110.8750\n",
      "[Batch 200/251] Loss: 7.7173, Batch Acc: 117.0625\n",
      "[Batch 210/251] Loss: 1.1996, Batch Acc: 123.5000\n",
      "[Batch 220/251] Loss: 12.7020, Batch Acc: 130.3125\n",
      "[Batch 230/251] Loss: 11.6404, Batch Acc: 136.6250\n",
      "[Batch 240/251] Loss: 12.9991, Batch Acc: 142.3125\n",
      "[Batch 250/251] Loss: 6.8691, Batch Acc: 148.1250\n",
      "Epoch 4 Summary - Loss: 2198.2653, Train Accuracy: 0.5928\n",
      "[Batch 10/251] Loss: 3.9108, Batch Acc: 6.5625\n",
      "[Batch 20/251] Loss: 1.9245, Batch Acc: 13.8750\n",
      "[Batch 30/251] Loss: 3.4423, Batch Acc: 20.5625\n",
      "[Batch 40/251] Loss: 7.0914, Batch Acc: 27.3750\n",
      "[Batch 50/251] Loss: 4.1435, Batch Acc: 34.0625\n",
      "[Batch 60/251] Loss: 12.3970, Batch Acc: 39.8750\n",
      "[Batch 70/251] Loss: 7.4826, Batch Acc: 45.1875\n",
      "[Batch 80/251] Loss: 16.4459, Batch Acc: 50.9375\n",
      "[Batch 90/251] Loss: 11.4257, Batch Acc: 57.2500\n",
      "[Batch 100/251] Loss: 10.8302, Batch Acc: 64.0000\n",
      "[Batch 110/251] Loss: 3.3588, Batch Acc: 70.6250\n",
      "[Batch 120/251] Loss: 4.3914, Batch Acc: 77.1875\n",
      "[Batch 130/251] Loss: 5.1679, Batch Acc: 84.0625\n",
      "[Batch 140/251] Loss: 5.1329, Batch Acc: 90.8125\n",
      "[Batch 150/251] Loss: 7.3376, Batch Acc: 97.0000\n",
      "[Batch 160/251] Loss: 10.6213, Batch Acc: 102.0625\n",
      "[Batch 170/251] Loss: 7.3456, Batch Acc: 108.2500\n",
      "[Batch 180/251] Loss: 13.3650, Batch Acc: 114.7500\n",
      "[Batch 190/251] Loss: 4.9409, Batch Acc: 121.1875\n",
      "[Batch 200/251] Loss: 13.9421, Batch Acc: 127.5000\n",
      "[Batch 210/251] Loss: 5.0285, Batch Acc: 133.6875\n",
      "[Batch 220/251] Loss: 3.4980, Batch Acc: 140.1875\n",
      "[Batch 230/251] Loss: 9.5200, Batch Acc: 145.5625\n",
      "[Batch 240/251] Loss: 26.3491, Batch Acc: 151.0625\n",
      "[Batch 250/251] Loss: 13.4662, Batch Acc: 156.1875\n",
      "Epoch 5 Summary - Loss: 1976.4735, Train Accuracy: 0.6248\n",
      "[Batch 10/251] Loss: 8.9911, Batch Acc: 6.3750\n",
      "[Batch 20/251] Loss: 8.9870, Batch Acc: 13.2500\n",
      "[Batch 30/251] Loss: 18.3061, Batch Acc: 20.0625\n",
      "[Batch 40/251] Loss: 12.9819, Batch Acc: 26.3125\n",
      "[Batch 50/251] Loss: 10.4580, Batch Acc: 32.3750\n",
      "[Batch 60/251] Loss: 14.4191, Batch Acc: 39.0000\n",
      "[Batch 70/251] Loss: 19.4916, Batch Acc: 45.0000\n",
      "[Batch 80/251] Loss: 16.6882, Batch Acc: 50.1250\n",
      "[Batch 90/251] Loss: 3.4691, Batch Acc: 56.4375\n",
      "[Batch 100/251] Loss: 1.5328, Batch Acc: 62.3125\n",
      "[Batch 110/251] Loss: 9.7676, Batch Acc: 67.6875\n",
      "[Batch 120/251] Loss: 3.1653, Batch Acc: 74.5625\n",
      "[Batch 130/251] Loss: 14.0646, Batch Acc: 80.5625\n",
      "[Batch 140/251] Loss: 13.5920, Batch Acc: 86.1875\n",
      "[Batch 150/251] Loss: 4.3573, Batch Acc: 92.3750\n",
      "[Batch 160/251] Loss: 4.4894, Batch Acc: 98.5625\n",
      "[Batch 170/251] Loss: 10.4294, Batch Acc: 104.6875\n",
      "[Batch 180/251] Loss: 5.2754, Batch Acc: 110.6875\n",
      "[Batch 190/251] Loss: 7.1937, Batch Acc: 116.7500\n",
      "[Batch 200/251] Loss: 12.7919, Batch Acc: 123.1875\n",
      "[Batch 210/251] Loss: 3.9522, Batch Acc: 129.3750\n",
      "[Batch 220/251] Loss: 10.3038, Batch Acc: 135.0000\n",
      "[Batch 230/251] Loss: 17.7523, Batch Acc: 141.5000\n",
      "[Batch 240/251] Loss: 3.8384, Batch Acc: 147.0000\n",
      "[Batch 250/251] Loss: 6.3040, Batch Acc: 153.4375\n",
      "Epoch 6 Summary - Loss: 2530.5483, Train Accuracy: 0.6140\n",
      "[Batch 10/251] Loss: 22.6306, Batch Acc: 6.7500\n",
      "[Batch 20/251] Loss: 7.5056, Batch Acc: 12.9375\n",
      "[Batch 30/251] Loss: 12.8987, Batch Acc: 19.9375\n",
      "[Batch 40/251] Loss: 2.3973, Batch Acc: 26.8125\n",
      "[Batch 50/251] Loss: 4.2402, Batch Acc: 33.5000\n",
      "[Batch 60/251] Loss: 6.9440, Batch Acc: 40.1875\n",
      "[Batch 70/251] Loss: 24.5246, Batch Acc: 46.4375\n",
      "[Batch 80/251] Loss: 6.9814, Batch Acc: 53.1875\n",
      "[Batch 90/251] Loss: 3.6216, Batch Acc: 60.1875\n",
      "[Batch 100/251] Loss: 1.8384, Batch Acc: 67.0000\n",
      "[Batch 110/251] Loss: 6.1146, Batch Acc: 74.1250\n",
      "[Batch 120/251] Loss: 2.2422, Batch Acc: 81.7500\n",
      "[Batch 130/251] Loss: 3.5490, Batch Acc: 89.0000\n",
      "[Batch 140/251] Loss: 16.0966, Batch Acc: 95.8125\n",
      "[Batch 150/251] Loss: 5.4906, Batch Acc: 103.1875\n",
      "[Batch 160/251] Loss: 3.6855, Batch Acc: 110.0000\n",
      "[Batch 170/251] Loss: 19.7727, Batch Acc: 116.8125\n",
      "[Batch 180/251] Loss: 2.0984, Batch Acc: 123.5000\n",
      "[Batch 190/251] Loss: 16.2819, Batch Acc: 129.9375\n",
      "[Batch 200/251] Loss: 7.2012, Batch Acc: 137.1875\n",
      "[Batch 210/251] Loss: 4.7299, Batch Acc: 144.0625\n",
      "[Batch 220/251] Loss: 8.5716, Batch Acc: 150.9375\n",
      "[Batch 230/251] Loss: 11.9720, Batch Acc: 158.3125\n",
      "[Batch 240/251] Loss: 2.3687, Batch Acc: 165.3750\n",
      "[Batch 250/251] Loss: 13.5140, Batch Acc: 172.4375\n",
      "Epoch 7 Summary - Loss: 1629.1754, Train Accuracy: 0.6899\n",
      "[Batch 10/251] Loss: 3.6424, Batch Acc: 7.0000\n",
      "[Batch 20/251] Loss: 3.7022, Batch Acc: 14.5625\n",
      "[Batch 30/251] Loss: 4.0485, Batch Acc: 22.3125\n",
      "[Batch 40/251] Loss: 8.5071, Batch Acc: 30.0625\n",
      "[Batch 50/251] Loss: 7.2828, Batch Acc: 37.4375\n",
      "[Batch 60/251] Loss: 1.2900, Batch Acc: 45.3750\n",
      "[Batch 70/251] Loss: 10.7969, Batch Acc: 52.0625\n",
      "[Batch 80/251] Loss: 0.0115, Batch Acc: 58.8750\n",
      "[Batch 90/251] Loss: 10.2905, Batch Acc: 65.8750\n",
      "[Batch 100/251] Loss: 1.2750, Batch Acc: 72.8125\n",
      "[Batch 110/251] Loss: 9.9349, Batch Acc: 80.1250\n",
      "[Batch 120/251] Loss: 2.4687, Batch Acc: 87.3125\n",
      "[Batch 130/251] Loss: 10.9600, Batch Acc: 94.4375\n",
      "[Batch 140/251] Loss: 12.6585, Batch Acc: 101.8125\n",
      "[Batch 150/251] Loss: 2.0894, Batch Acc: 108.5625\n",
      "[Batch 160/251] Loss: 5.1194, Batch Acc: 115.6875\n",
      "[Batch 170/251] Loss: 4.4797, Batch Acc: 122.3750\n",
      "[Batch 180/251] Loss: 8.8323, Batch Acc: 129.4375\n",
      "[Batch 190/251] Loss: 2.2397, Batch Acc: 136.7500\n",
      "[Batch 200/251] Loss: 10.3403, Batch Acc: 142.9375\n",
      "[Batch 210/251] Loss: 3.2189, Batch Acc: 150.1875\n",
      "[Batch 220/251] Loss: 4.8618, Batch Acc: 157.1875\n",
      "[Batch 230/251] Loss: 8.9565, Batch Acc: 164.1875\n",
      "[Batch 240/251] Loss: 5.3029, Batch Acc: 171.3750\n",
      "[Batch 250/251] Loss: 18.0673, Batch Acc: 177.5000\n",
      "Epoch 8 Summary - Loss: 1544.9053, Train Accuracy: 0.7093\n",
      "[Batch 10/251] Loss: 4.3659, Batch Acc: 6.8125\n",
      "[Batch 20/251] Loss: 9.2238, Batch Acc: 13.2500\n",
      "[Batch 30/251] Loss: 8.1594, Batch Acc: 19.8125\n",
      "[Batch 40/251] Loss: 1.9533, Batch Acc: 27.0000\n",
      "[Batch 50/251] Loss: 18.2562, Batch Acc: 33.8125\n",
      "[Batch 60/251] Loss: 6.6769, Batch Acc: 40.3125\n",
      "[Batch 70/251] Loss: 3.6920, Batch Acc: 47.2500\n",
      "[Batch 80/251] Loss: 22.6400, Batch Acc: 53.5000\n",
      "[Batch 90/251] Loss: 10.1486, Batch Acc: 60.1250\n",
      "[Batch 100/251] Loss: 7.6723, Batch Acc: 67.5000\n",
      "[Batch 110/251] Loss: 4.0811, Batch Acc: 75.1250\n",
      "[Batch 120/251] Loss: 3.6030, Batch Acc: 81.6875\n",
      "[Batch 130/251] Loss: 1.2654, Batch Acc: 89.0625\n",
      "[Batch 140/251] Loss: 0.6302, Batch Acc: 96.8125\n",
      "[Batch 150/251] Loss: 5.4395, Batch Acc: 104.0000\n",
      "[Batch 160/251] Loss: 1.0509, Batch Acc: 112.1250\n",
      "[Batch 170/251] Loss: 2.4937, Batch Acc: 120.0000\n",
      "[Batch 180/251] Loss: 7.3728, Batch Acc: 127.8750\n",
      "[Batch 190/251] Loss: 8.1581, Batch Acc: 135.5000\n",
      "[Batch 200/251] Loss: 3.6442, Batch Acc: 142.5000\n",
      "[Batch 210/251] Loss: 15.4158, Batch Acc: 149.1250\n",
      "[Batch 220/251] Loss: 4.4652, Batch Acc: 155.1875\n",
      "[Batch 230/251] Loss: 20.6650, Batch Acc: 161.1875\n",
      "[Batch 240/251] Loss: 5.6123, Batch Acc: 168.5000\n",
      "[Batch 250/251] Loss: 7.0431, Batch Acc: 175.2500\n",
      "Epoch 9 Summary - Loss: 1770.4235, Train Accuracy: 0.7016\n",
      "[Batch 10/251] Loss: 3.9348, Batch Acc: 6.8125\n",
      "[Batch 20/251] Loss: 11.4696, Batch Acc: 14.2500\n",
      "[Batch 30/251] Loss: 3.8858, Batch Acc: 21.5625\n",
      "[Batch 40/251] Loss: 3.7481, Batch Acc: 28.8125\n",
      "[Batch 50/251] Loss: 3.9265, Batch Acc: 36.1250\n",
      "[Batch 60/251] Loss: 2.6243, Batch Acc: 43.8125\n",
      "[Batch 70/251] Loss: 4.4678, Batch Acc: 51.3750\n",
      "[Batch 80/251] Loss: 3.0397, Batch Acc: 59.3750\n",
      "[Batch 90/251] Loss: 1.5104, Batch Acc: 67.2500\n",
      "[Batch 100/251] Loss: 3.1753, Batch Acc: 74.3125\n",
      "[Batch 110/251] Loss: 0.7146, Batch Acc: 81.3125\n",
      "[Batch 120/251] Loss: 6.7441, Batch Acc: 88.5625\n",
      "[Batch 130/251] Loss: 1.8095, Batch Acc: 96.5000\n",
      "[Batch 140/251] Loss: 4.2600, Batch Acc: 104.3125\n",
      "[Batch 150/251] Loss: 13.7153, Batch Acc: 111.1875\n",
      "[Batch 160/251] Loss: 7.8645, Batch Acc: 118.7500\n",
      "[Batch 170/251] Loss: 3.1297, Batch Acc: 125.1875\n",
      "[Batch 180/251] Loss: 1.2311, Batch Acc: 132.4375\n",
      "[Batch 190/251] Loss: 4.0460, Batch Acc: 139.8125\n",
      "[Batch 200/251] Loss: 10.4367, Batch Acc: 147.3750\n",
      "[Batch 210/251] Loss: 0.5500, Batch Acc: 154.8125\n",
      "[Batch 220/251] Loss: 5.5207, Batch Acc: 162.3750\n",
      "[Batch 230/251] Loss: 5.2575, Batch Acc: 169.2500\n",
      "[Batch 240/251] Loss: 2.6460, Batch Acc: 176.6875\n",
      "[Batch 250/251] Loss: 3.0682, Batch Acc: 184.0000\n",
      "Epoch 10 Summary - Loss: 1249.6732, Train Accuracy: 0.7360\n",
      "Test Accuracy: 0.5490\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Training Loop\n",
    "# -----------------------------\n",
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        train_correct += (preds == labels).sum().item()\n",
    "        train_total += labels.size(0)\n",
    "        # Print every 10 batches\n",
    "        if(i + 1) % 10 == 0:\n",
    "            batch_acc = train_correct / labels.size(0)\n",
    "            print(f\"[Batch {i+1}/{len(train_loader)}] Loss: {loss.item():.4f}, Batch Acc: {batch_acc:.4f}\")\n",
    "\n",
    "\n",
    "    train_accuracy = train_correct / train_total\n",
    "    wandb.log({\"epoch\": epoch + 1, \"train_loss\": train_loss, \"train_accuracy\": train_accuracy})\n",
    "    print(f\"Epoch {epoch+1} Summary - Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# # Evaluation (Optional)\n",
    "# # -----------------------------\n",
    "model.eval()\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        test_correct += (preds == labels).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "\n",
    "print(f\"Test Accuracy: {test_correct / test_total:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2a3a31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
