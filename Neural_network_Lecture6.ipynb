{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBKB4e3P65J_"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "from wandb.integration.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "kQAIk0497BJX",
        "outputId": "0ed03689-099d-4a5c-e878-d19f84f2cc6a"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Create sweep with ID: wses5v86\n",
            "Sweep URL: https://wandb.ai/fangselection123-happiest-minds-technologies/5-Flower-Dataset/sweeps/wses5v86\n"
          ]
        }
      ],
      "source": [
        "sweep_config = {\n",
        "    'method' : 'grid',\n",
        "    'metric': {\n",
        "        'name' : 'val_accuracy',\n",
        "        'goal' : 'maximize'\n",
        "    },\n",
        "    'parameters' : {\n",
        "        'batch_size' : { 'values' : [8, 16, 32, 64, 128]},\n",
        "        'learning_rate' : { 'values' : [0.001, 0.0001, 0.00001]},\n",
        "        'hidden_nodes': {'values' : [32, 64, 128, 256]},\n",
        "        'img_size' : {'values' : [16, 64, 224]},\n",
        "        'epochs' : {'values': [5, 10]}\n",
        "    }\n",
        "}\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"5-Flower-Dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSJBotTi7CMx"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "  with wandb.init() as run:\n",
        "    config= wandb.config\n",
        "    import tensorflow as tf\n",
        "    IMG_HEIGHT= config.img_size\n",
        "    IMG_WIDTH= config.img_size\n",
        "    IMG_CHANNELS=3\n",
        "    CLASS_NAMES = [\"daisy\", \"dandelion\", \"roses\", \"sunflowers\", \"tulips\"]\n",
        "    def read_and_decode(filename, resize_dims):\n",
        "      # 1. Read the raw file\n",
        "      img_bytes= tf.io.read_file(filename)\n",
        "      # 2. Decode image data\n",
        "      img= tf.image.decode_jpeg(img_bytes, channels=IMG_CHANNELS)\n",
        "      # 3. Convert image to float values in [0, 1]\n",
        "      img= tf.image.convert_image_dtype(img, tf.float32)\n",
        "      # 4. Resize the image to the match the desire dimention\n",
        "      img= tf.image.resize(img, resize_dims)\n",
        "      return img\n",
        "\n",
        "    def parse_csvline(csv_line):\n",
        "      # print(\"csv line:\", csv_line)\n",
        "      # record_defaults specify the data types for each columns\n",
        "      record_default = [\"\", \"\"]\n",
        "      filename, label_string =tf.io.decode_csv(csv_line, record_defaults=record_default)\n",
        "\n",
        "\n",
        "      #load the image\n",
        "      img= read_and_decode(filename, [IMG_HEIGHT, IMG_WIDTH])\n",
        "      # print(\"Label String:\",label_string)\n",
        "      label = tf.argmax(tf.math.equal(CLASS_NAMES, label_string))\n",
        "      return img, label\n",
        "\n",
        "    #Define dataset\n",
        "    train_dataset = (\n",
        "        tf.data.TextLineDataset(\"gs://cloud-ml-data/img/flower_photos/train_set.csv\")\n",
        "        #.map(parse_csvline) # it will process one by one line to the map function which is slow\n",
        "        #.map(parse_csvline, num_parallel_calls=4) # it will process one by four line to the map function which is faster\n",
        "\n",
        "        .map(parse_csvline, num_parallel_calls=tf.data.AUTOTUNE) # It will adjust the number of line to the function depends upon the cpu\n",
        "        .batch(config.batch_size)\n",
        "        .prefetch(tf.data.AUTOTUNE)\n",
        "    )\n",
        "    test_dataset = (\n",
        "        tf.data.TextLineDataset(\"gs://cloud-ml-data/img/flower_photos/eval_set.csv\")\n",
        "        .map(parse_csvline, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        .batch(config.batch_size)\n",
        "        .prefetch(tf.data.AUTOTUNE) # When the model is training the current batch then it will prepare the next batch in the background.\n",
        "    )\n",
        "    regularizer=tf.keras.regularizers.l1_l2(l1=0.01, l2=0.01)\n",
        "    model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(IMG_WIDTH, IMG_HEIGHT, IMG_CHANNELS)),\n",
        "    keras.layers.Dense(config.hidden_nodes, kernel_regularizer=regularizer),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Activation('relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "\n",
        "    keras.layers.Dense(len(CLASS_NAMES), kernel_regularizer=regularizer),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Activation('softmax')\n",
        "\n",
        "\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss= keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics=['accuracy'])\n",
        "\n",
        "    model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=test_dataset,\n",
        "    epochs=config.epochs,\n",
        "    callbacks=[WandbMetricsLogger(),\n",
        "               tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True),\n",
        "               ]\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wandb.agent(sweep_id, function=train)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
